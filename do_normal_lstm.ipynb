{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# use only for tokenizer and padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1019):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# SEED = 1019\n",
    "# seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "class Config:\n",
    "    num_epochs = 15\n",
    "    batch_size = 512\n",
    "    test_batch_size = 512\n",
    "    vocab_size = 120000\n",
    "    max_length = 72\n",
    "    embedding_size = 300\n",
    "    hidden_size = 64\n",
    "    num_layers = 1\n",
    "    embedding_dropout = 0.3\n",
    "    layer_dropout = 0.1\n",
    "    dense_size = [hidden_size*2*4, int(hidden_size/4)] # depend on concat num\n",
    "    output_size = 1\n",
    "    num_cv_splits = 5\n",
    "    learning_rate = 0.001\n",
    "    clip_grad = 5.0\n",
    "    embeddings = ['glove', 'paragram', 'fasttext']\n",
    "    datadir = Path('./data/')\n",
    "    # datadir = Path('../input') # for kernel\n",
    "\n",
    "c = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [\n",
    "    ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '¬', '░', '¶', '↑', '±',  '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '₹', '´'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"'re\": \" are\",\n",
    "    \"tryin'\": \"trying\",\n",
    "    \"doesn'\": \"does not\",\n",
    "    'howdo': 'how do',\n",
    "    'whatare': 'what are',\n",
    "    'howcan': 'how can',\n",
    "    'howmuch': 'how much',\n",
    "    'howmany': 'how many',\n",
    "    'whydo': 'why do',\n",
    "    'doI': 'do I',\n",
    "    'theBest': 'the best',\n",
    "    'howdoes': 'how does',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spells = {\n",
    "    'colour': 'color',\n",
    "    'centre': 'center',\n",
    "    'favourite': 'favorite',\n",
    "    'travelling': 'traveling',\n",
    "    'counselling': 'counseling',\n",
    "    'theatre': 'theater',\n",
    "    'cancelled': 'canceled',\n",
    "    'labour': 'labor',\n",
    "    'organisation': 'organization',\n",
    "    'wwii': 'world war 2',\n",
    "    'citicise': 'criticize',\n",
    "    'youtu.be': 'youtube',\n",
    "    'youtu ': 'youtube ',\n",
    "    'qoura': 'quora',\n",
    "    'sallary': 'salary',\n",
    "    'Whta': 'what',\n",
    "    'whta': 'what',\n",
    "    'narcisist': 'narcissist',\n",
    "    'mastrubation': 'masturbation',\n",
    "    'mastrubate': 'masturbate',\n",
    "    \"mastrubating\": 'masturbating',\n",
    "    'pennis': 'penis',\n",
    "    'Etherium': 'ethereum',\n",
    "    'etherium': 'ethereum',\n",
    "    'narcissit': 'narcissist',\n",
    "    'bigdata': 'big data',\n",
    "    '2k17': '2017',\n",
    "    '2k18': '2018',\n",
    "    'qouta': 'quota',\n",
    "    'exboyfriend': 'ex boyfriend',\n",
    "    'exgirlfriend': 'ex girlfriend',\n",
    "    'airhostess': 'air hostess',\n",
    "    \"whst\": 'what',\n",
    "    'watsapp': 'whatsapp',\n",
    "    'demonitisation': 'demonetization',\n",
    "    'demonitization': 'demonetization',\n",
    "    'demonetisation': 'demonetization',\n",
    "    'quorans': 'quora user',\n",
    "    'quoran': 'quora user',\n",
    "    'pokémon': 'pokemon',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    train_df = pd.read_csv(datadir / 'train_local.csv')\n",
    "    test_df = pd.read_csv(datadir / 'test_local.csv')\n",
    "    print(\"Train shape : \", train_df.shape)\n",
    "    print(\"Test shape : \", test_df.shape)\n",
    "    return train_df, test_df\n",
    "\n",
    "def clean(df):\n",
    "    df = clean_lower(df)\n",
    "    df = clean_unicode(df)\n",
    "    df = clean_abbreviation(df, abbreviations)\n",
    "    df = clean_spells(df, spells)\n",
    "    df = clean_language(df)\n",
    "    df = clean_puncts(df, puncts)\n",
    "    df = clean_space(df)\n",
    "    return df\n",
    "\n",
    "def clean_unicode(df):\n",
    "    codes = ['\\x7f', '\\u200b', '\\xa0', '\\ufeff', '\\u200e', '\\u202a', '\\u202c', '\\u2060', '\\uf0d8', '\\ue019', '\\uf02d', '\\u200f', '\\u2061', '\\ue01b']\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: _clean_unicode(x, codes))\n",
    "    return df\n",
    "\n",
    "def _clean_unicode(x, codes):\n",
    "    for u in codes:\n",
    "        if u in x:\n",
    "            x = x.replace(u, '')\n",
    "    return x\n",
    "\n",
    "def clean_language(df):\n",
    "    langs1 = r'[\\p{Katakana}\\p{Hiragana}\\p{Han}]' # regex\n",
    "    langs2 = r'[ஆய்தஎழுத்துஆயுதஎழுத்துशुषछछशुषدوउसशुष북한내제តើបងប្អូនមានមធ្យបាយអ្វីខ្លះដើម្បីរកឃើញឯកសារអំពីប្រវត្តិស្ត្រនៃប្រាសាទអង្គរវट्टरौरआदસંઘરાજ્યपीतऊनअहএকটিবাড়িএকটিখামারএরঅধীনেপদেরবাছাইপরীক্ষাএরপ্রশ্নওউত্তরসহকোথায়পেতেপারিص、。Емелядуракلكلمقاممقال수능ί서로가를행복하게기乡국고등학교는몇시간업니《》싱관없어나이रचा키کپڤ」मिलगईकलेजेकोठंडकऋॠऌॡर]'\n",
    "    compiled_langs1 = regex.compile(langs1)\n",
    "    compiled_langs2 = re.compile(langs2)\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs1))\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs2))\n",
    "    return df\n",
    "\n",
    "def _clean_language(x, compiled_re):\n",
    "    return compiled_re.sub(' <lang> ', x)\n",
    "\n",
    "def clean_lower(df):\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "def clean_puncts(df, puncts):\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_puncts(x, puncts))\n",
    "    return df\n",
    "    \n",
    "def _clean_puncts(x, puncts):\n",
    "    x = str(x)\n",
    "    # added space around puncts after replace\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_spells(df, spells):\n",
    "    compiled_spells = re.compile('(%s)' % '|'.join(spells.keys()))\n",
    "    def replace(match):\n",
    "        return spells[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_spells(x, compiled_spells, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_spells(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_abbreviation(df, abbreviations):\n",
    "    compiled_abbreviation = re.compile('(%s)' % '|'.join(abbreviations.keys()))\n",
    "    def replace(match):\n",
    "        return abbreviations[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_abreviation(x, compiled_abbreviation, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_abreviation(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_space(df):\n",
    "    compiled_re = re.compile(r\"\\s+\")\n",
    "    df['question_text'] = df[\"question_text\"].apply(lambda x: _clean_space(x, compiled_re))\n",
    "    return df\n",
    "\n",
    "def _clean_space(x, compiled_re):\n",
    "    return compiled_re.sub(\" \", x)\n",
    "        \n",
    "def prepare_tokenizer(texts, max_words):\n",
    "    tokenizer = Tokenizer(num_words=max_words, filters='', oov_token='<unk>')\n",
    "    tokenizer.fit_on_texts(list(texts))\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_and_padding(texts, tokenizer, max_length):\n",
    "    texts = tokenizer.texts_to_sequences(texts)\n",
    "    texts = pad_sequences(texts, maxlen=max_length)\n",
    "    return texts\n",
    "\n",
    "def get_all_vocabs(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config, tokenizer, all_vocabs, embedding_weights = None):\n",
    "        super(Embeddings, self).__init__()\n",
    "        \n",
    "        self.embedding_map = {\n",
    "            'fasttext': self._load_fasttext,\n",
    "            'glove': self._load_glove,\n",
    "            'paragram': self._load_paragram\n",
    "        }\n",
    "        self.c = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.all_vocabs = all_vocabs\n",
    "        \n",
    "        if embedding_weights is None:\n",
    "            embedding_weights = self._load_embeddings(self.c.embeddings)\n",
    "            \n",
    "        self.original_embedding_weights = embedding_weights\n",
    "        self.embeddings = nn.Embedding(self.c.vocab_size + 1, self.c.embedding_size, padding_idx=0)\n",
    "        self.embeddings.weight = nn.Parameter(embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.embedding_dropout = nn.Dropout(self.c.embedding_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.embeddings(x)\n",
    "        return self.embedding_dropout(embedding)\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        self.embeddings.weight = nn.Parameter(self.original_embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "    \n",
    "    def _load_embeddings(self, embedding_list: list):\n",
    "        embedding_weights = np.zeros((self.c.vocab_size, self.c.embedding_size))\n",
    "        pool = Pool(num_cores)\n",
    "        embedding_weights = np.mean(pool.map(self._load_an_embedding, embedding_list), 0)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return torch.tensor(embedding_weights, dtype=torch.float32)\n",
    "\n",
    "    def _load_an_embedding(self, emb):\n",
    "        return self.embedding_map[emb](self.tokenizer.word_index)\n",
    "        \n",
    "    def _get_embeddings_pair(self, word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "        \n",
    "    def _make_embeddings(self, embeddings_index, word_index, emb_mean, emb_std):\n",
    "        nb_words = min(self.c.vocab_size, len(word_index))\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, self.c.embedding_size))\n",
    "        embedding_matrix[0] = np.zeros(self.c.embedding_size)\n",
    "        for word, i in word_index.items():\n",
    "            if i >= self.c.vocab_size:\n",
    "                continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return embedding_matrix\n",
    "    \n",
    "    def _load_glove(self, word_index):\n",
    "        print('loading glove')\n",
    "        filepath = self.c.datadir / 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "    \n",
    "    def _load_fasttext(self, word_index):    \n",
    "        print('loading fasttext')\n",
    "        filepath = self.c.datadir / 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0033469985, 0.109855495\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "\n",
    "    def _load_paragram(self, word_index):\n",
    "        print('loading paragram')\n",
    "        filepath = self.c.datadir / 'embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath, encoding=\"utf8\", errors='ignore')\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0053247833, 0.49346462\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 2\n",
    "def df_parallelize_run(df, func, num_cores=2):\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1175509, 3)\n",
      "Test shape :  (130613, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_data(c.datadir)\n",
    "train_df = df_parallelize_run(train_df, clean)\n",
    "test_df = df_parallelize_run(test_df, clean)\n",
    "train_x, train_y = train_df['question_text'].values, train_df['target'].values\n",
    "test_x = test_df['question_text'].values\n",
    "tokenizer = prepare_tokenizer(train_x, c.vocab_size)\n",
    "train_x = tokenize_and_padding(train_x, tokenizer, c.max_length)\n",
    "test_x = tokenize_and_padding(test_x, tokenizer, c.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_vocabs:  184279\n",
      "loading glove\n",
      "loading paragram\n",
      "loading fasttext\n",
      "48.56483697891235\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_vocabs = get_all_vocabs(train_df['question_text'])\n",
    "print('all_vocabs: ', len(all_vocabs))\n",
    "embeddings = Embeddings(c, tokenizer, all_vocabs)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(GRULayer, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          bias=False,\n",
    "                          bidirectional=True,\n",
    "                          batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_outputs, gru_state = self.gru(x)\n",
    "        return self.dropout(gru_outputs), gru_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=False,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_outputs, (lstm_states, _) = self.lstm(x)\n",
    "        return self.dropout(lstm_outputs), lstm_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, config: Config, embeddings):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.c = config\n",
    "        \n",
    "        self.embedding = embeddings\n",
    "        self.lstm1 = LSTMLayer(input_size=self.c.embedding_size,\n",
    "                              hidden_size=self.c.hidden_size,\n",
    "                              num_layers=self.c.num_layers,\n",
    "                              dropout_rate=self.c.layer_dropout)\n",
    "        self.lstm2 = LSTMLayer(input_size=self.c.hidden_size*2,\n",
    "                            hidden_size=self.c.hidden_size,\n",
    "                            num_layers=self.c.num_layers,\n",
    "                            dropout_rate=self.c.layer_dropout)\n",
    "        \n",
    "        self.cell_dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.linear = nn.Linear(self.c.dense_size[0], self.c.dense_size[1])\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(self.c.dense_size[1])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.out = nn.Linear(self.c.dense_size[1], self.c.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        o_lstm1, h_lstm1 = self.lstm1(h_embedding)\n",
    "        o_lstm2, h_lstm2 = self.lstm2(o_lstm1)\n",
    "        \n",
    "        avg_pool = torch.mean(o_lstm2, 1)\n",
    "        max_pool, _ = torch.max(o_lstm2, 1)\n",
    "        \n",
    "        h_lstm1 = self.cell_dropout(torch.cat(h_lstm1.split(1, 0), -1).squeeze(0))\n",
    "        h_lstm2 = self.cell_dropout(torch.cat(h_lstm2.split(1, 0), -1).squeeze(0))\n",
    "\n",
    "        concat = torch.cat([h_lstm1, h_lstm2, avg_pool, max_pool], 1)\n",
    "        concat = self.linear(concat)\n",
    "        concat = self.batch_norm(concat)\n",
    "        concat = self.relu(concat)\n",
    "        concat = self.dropout(concat)\n",
    "        out = self.out(concat)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_length(data, mask):\n",
    "    max_length = data.shape[1]\n",
    "    transposed = torch.transpose(data, 1, 0)\n",
    "    res = (transposed == mask).all(1)\n",
    "    for i, r in enumerate(res):\n",
    "        if r == 0:\n",
    "            break\n",
    "    data = data[:, -(max_length - i):]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_x, train_y, test_x, c, embeddings, trial=0):\n",
    "    splits = list(StratifiedKFold(n_splits=c.num_cv_splits, shuffle=True).split(train_x, train_y))\n",
    "    x_test_cuda = torch.tensor(test_x, dtype=torch.long).cuda(cuda_idx)\n",
    "    test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=c.test_batch_size, shuffle=False)\n",
    "    train_preds = np.zeros((len(train_x)))\n",
    "    test_preds = np.zeros((len(test_x)))\n",
    "\n",
    "    mask = torch.zeros((c.max_length, 1), dtype=torch.long).cuda(cuda_idx)\n",
    "    \n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        x_train_fold = torch.tensor(train_x[train_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "        x_val_fold = torch.tensor(train_x[valid_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "\n",
    "        model = SimpleRNN(c, embeddings)\n",
    "        model.cuda(cuda_idx)\n",
    "\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=c.learning_rate)\n",
    "\n",
    "        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "        valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=c.batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=c.test_batch_size, shuffle=False)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        print(f'Fold {i + 1}')\n",
    "\n",
    "        for epoch in range(c.num_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred = model(x_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), c.clip_grad)\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "            avg_val_loss = 0.\n",
    "\n",
    "            # validation prediction\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred = model(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            search_result = threshold_search(y_val_fold.cpu().numpy(), valid_preds_fold)\n",
    "            valid_pred_targets = valid_preds_fold > search_result['threshold']\n",
    "            val_f1 = f1_score(y_val_fold.cpu().numpy(), valid_pred_targets)\n",
    "\n",
    "            elapsed_time = time.time() - start_time \n",
    "            print('Epoch {}/{}  loss={:.4f}  val_loss={:.4f}  f1={:.3f}  time={:.2f}s'.format(\n",
    "                epoch + 1, c.num_epochs, avg_loss, avg_val_loss, val_f1, elapsed_time))\n",
    "            if best_f1 < val_f1:\n",
    "                print(f'model_saved at f1: {val_f1} from {best_f1}')\n",
    "                ckpt_path = Path(f'./ckpt/do_normal/{trial}/')\n",
    "                if not ckpt_path.exists():\n",
    "                    ckpt_path.mkdir(parents=True)\n",
    "                torch.save(model.state_dict(), ckpt_path / f'{i}_model.pt')\n",
    "                best_f1 = val_f1\n",
    "                best_epoch = epoch\n",
    "\n",
    "        # test prediction\n",
    "        model.load_state_dict(torch.load(f'./ckpt/do_normal/{trial}/{i}_model.pt'))  # load best model\n",
    "        test_preds_fold = np.zeros(len(test_x))\n",
    "        for i, (x_batch, ) in enumerate(test_loader):\n",
    "            x_batch = cut_length(x_batch, mask)\n",
    "            y_pred = model(x_batch).detach()\n",
    "            test_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "        train_preds[valid_idx] = valid_preds_fold\n",
    "        test_preds += test_preds_fold / len(splits)\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1622  val_loss=0.1118  f1=0.645  time=63.84s\n",
      "model_saved at f1: 0.6452745849297574 from 0.0\n",
      "Epoch 2/15  loss=0.1119  val_loss=0.1046  f1=0.660  time=64.80s\n",
      "model_saved at f1: 0.6603343708885537 from 0.6452745849297574\n",
      "Epoch 3/15  loss=0.1067  val_loss=0.1032  f1=0.670  time=64.35s\n",
      "model_saved at f1: 0.6695118866970156 from 0.6603343708885537\n",
      "Epoch 4/15  loss=0.1031  val_loss=0.1020  f1=0.676  time=64.50s\n",
      "model_saved at f1: 0.6760644418872266 from 0.6695118866970156\n",
      "Epoch 5/15  loss=0.1006  val_loss=0.1001  f1=0.679  time=65.35s\n",
      "model_saved at f1: 0.6788188347964885 from 0.6760644418872266\n",
      "Epoch 6/15  loss=0.0979  val_loss=0.0984  f1=0.684  time=65.07s\n",
      "model_saved at f1: 0.6841048883133699 from 0.6788188347964885\n",
      "Epoch 7/15  loss=0.0962  val_loss=0.0964  f1=0.687  time=65.05s\n",
      "model_saved at f1: 0.6869593495934958 from 0.6841048883133699\n",
      "Epoch 8/15  loss=0.0942  val_loss=0.0965  f1=0.688  time=65.89s\n",
      "model_saved at f1: 0.6878855325914149 from 0.6869593495934958\n",
      "Epoch 9/15  loss=0.0926  val_loss=0.0962  f1=0.689  time=65.41s\n",
      "model_saved at f1: 0.688881116657658 from 0.6878855325914149\n",
      "Epoch 10/15  loss=0.0909  val_loss=0.0969  f1=0.687  time=65.86s\n",
      "Epoch 11/15  loss=0.0896  val_loss=0.0980  f1=0.690  time=66.47s\n",
      "model_saved at f1: 0.6901276276276276 from 0.688881116657658\n",
      "Epoch 12/15  loss=0.0880  val_loss=0.0981  f1=0.688  time=65.83s\n",
      "Epoch 13/15  loss=0.0870  val_loss=0.0980  f1=0.689  time=65.94s\n",
      "Epoch 14/15  loss=0.0856  val_loss=0.0980  f1=0.689  time=66.25s\n",
      "Epoch 15/15  loss=0.0845  val_loss=0.1012  f1=0.687  time=65.85s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1680  val_loss=0.1121  f1=0.635  time=65.50s\n",
      "model_saved at f1: 0.6348609580605432 from 0.0\n",
      "Epoch 2/15  loss=0.1119  val_loss=0.1058  f1=0.666  time=65.89s\n",
      "model_saved at f1: 0.6658904238914939 from 0.6348609580605432\n",
      "Epoch 3/15  loss=0.1058  val_loss=0.1005  f1=0.676  time=66.26s\n",
      "model_saved at f1: 0.6755815845127705 from 0.6658904238914939\n",
      "Epoch 4/15  loss=0.1020  val_loss=0.0987  f1=0.683  time=65.96s\n",
      "model_saved at f1: 0.6825812712275595 from 0.6755815845127705\n",
      "Epoch 5/15  loss=0.0991  val_loss=0.0986  f1=0.685  time=66.22s\n",
      "model_saved at f1: 0.6848661195932912 from 0.6825812712275595\n",
      "Epoch 6/15  loss=0.0966  val_loss=0.0968  f1=0.689  time=66.89s\n",
      "model_saved at f1: 0.6885946453430024 from 0.6848661195932912\n",
      "Epoch 7/15  loss=0.0945  val_loss=0.0967  f1=0.692  time=66.88s\n",
      "model_saved at f1: 0.6915760869565216 from 0.6885946453430024\n",
      "Epoch 8/15  loss=0.0924  val_loss=0.0956  f1=0.693  time=65.78s\n",
      "model_saved at f1: 0.6934422509643748 from 0.6915760869565216\n",
      "Epoch 9/15  loss=0.0906  val_loss=0.0963  f1=0.692  time=66.57s\n",
      "Epoch 10/15  loss=0.0888  val_loss=0.0958  f1=0.693  time=66.46s\n",
      "model_saved at f1: 0.6934574703907783 from 0.6934422509643748\n",
      "Epoch 11/15  loss=0.0875  val_loss=0.0975  f1=0.692  time=65.61s\n",
      "Epoch 12/15  loss=0.0862  val_loss=0.0985  f1=0.691  time=66.58s\n",
      "Epoch 13/15  loss=0.0852  val_loss=0.0970  f1=0.693  time=66.79s\n",
      "Epoch 14/15  loss=0.0836  val_loss=0.0981  f1=0.693  time=65.93s\n",
      "Epoch 15/15  loss=0.0824  val_loss=0.0970  f1=0.692  time=67.07s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1499  val_loss=0.1108  f1=0.640  time=66.98s\n",
      "model_saved at f1: 0.6398607035759197 from 0.0\n",
      "Epoch 2/15  loss=0.1096  val_loss=0.1039  f1=0.662  time=66.82s\n",
      "model_saved at f1: 0.6621617252416825 from 0.6398607035759197\n",
      "Epoch 3/15  loss=0.1040  val_loss=0.1019  f1=0.669  time=66.58s\n",
      "model_saved at f1: 0.6689804204046886 from 0.6621617252416825\n",
      "Epoch 4/15  loss=0.1005  val_loss=0.1026  f1=0.672  time=66.72s\n",
      "model_saved at f1: 0.6724121297874394 from 0.6689804204046886\n",
      "Epoch 5/15  loss=0.0977  val_loss=0.0997  f1=0.678  time=66.83s\n",
      "model_saved at f1: 0.6780995220691595 from 0.6724121297874394\n",
      "Epoch 6/15  loss=0.0950  val_loss=0.0983  f1=0.683  time=65.87s\n",
      "model_saved at f1: 0.68284718515495 from 0.6780995220691595\n",
      "Epoch 7/15  loss=0.0933  val_loss=0.0993  f1=0.682  time=66.05s\n",
      "Epoch 8/15  loss=0.0911  val_loss=0.0990  f1=0.681  time=66.34s\n",
      "Epoch 9/15  loss=0.0894  val_loss=0.0999  f1=0.684  time=65.90s\n",
      "model_saved at f1: 0.6837139862139069 from 0.68284718515495\n",
      "Epoch 10/15  loss=0.0878  val_loss=0.0988  f1=0.684  time=67.29s\n",
      "Epoch 11/15  loss=0.0865  val_loss=0.0994  f1=0.683  time=86.91s\n",
      "Epoch 12/15  loss=0.0850  val_loss=0.1002  f1=0.684  time=123.71s\n",
      "model_saved at f1: 0.6839040445597824 from 0.6837139862139069\n",
      "Epoch 13/15  loss=0.0836  val_loss=0.1011  f1=0.684  time=123.78s\n",
      "Epoch 14/15  loss=0.0824  val_loss=0.1014  f1=0.684  time=122.68s\n",
      "Epoch 15/15  loss=0.0816  val_loss=0.1012  f1=0.681  time=122.53s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1562  val_loss=0.1108  f1=0.641  time=122.88s\n",
      "model_saved at f1: 0.6410997174513476 from 0.0\n",
      "Epoch 2/15  loss=0.1112  val_loss=0.1034  f1=0.664  time=122.19s\n",
      "model_saved at f1: 0.6640996602491506 from 0.6410997174513476\n",
      "Epoch 3/15  loss=0.1053  val_loss=0.1005  f1=0.671  time=122.77s\n",
      "model_saved at f1: 0.671302588741031 from 0.6640996602491506\n",
      "Epoch 4/15  loss=0.1019  val_loss=0.0983  f1=0.678  time=122.75s\n",
      "model_saved at f1: 0.6780427238865365 from 0.671302588741031\n",
      "Epoch 5/15  loss=0.0986  val_loss=0.1006  f1=0.683  time=121.69s\n",
      "model_saved at f1: 0.6834974169022322 from 0.6780427238865365\n",
      "Epoch 6/15  loss=0.0961  val_loss=0.0964  f1=0.688  time=122.55s\n",
      "model_saved at f1: 0.687577356524005 from 0.6834974169022322\n",
      "Epoch 7/15  loss=0.0940  val_loss=0.0960  f1=0.689  time=122.73s\n",
      "model_saved at f1: 0.6885990526247486 from 0.687577356524005\n",
      "Epoch 8/15  loss=0.0918  val_loss=0.0971  f1=0.691  time=121.91s\n",
      "model_saved at f1: 0.6906847388274406 from 0.6885990526247486\n",
      "Epoch 9/15  loss=0.0902  val_loss=0.0966  f1=0.690  time=122.59s\n",
      "Epoch 10/15  loss=0.0886  val_loss=0.0970  f1=0.688  time=122.89s\n",
      "Epoch 11/15  loss=0.0866  val_loss=0.0980  f1=0.690  time=120.77s\n",
      "Epoch 12/15  loss=0.0855  val_loss=0.0986  f1=0.690  time=122.58s\n",
      "Epoch 13/15  loss=0.0841  val_loss=0.0975  f1=0.691  time=123.34s\n",
      "Epoch 14/15  loss=0.0830  val_loss=0.0984  f1=0.692  time=122.77s\n",
      "model_saved at f1: 0.6916121585225087 from 0.6906847388274406\n",
      "Epoch 15/15  loss=0.0819  val_loss=0.1001  f1=0.689  time=122.26s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1586  val_loss=0.1126  f1=0.640  time=122.87s\n",
      "model_saved at f1: 0.639634601043997 from 0.0\n",
      "Epoch 2/15  loss=0.1103  val_loss=0.1064  f1=0.665  time=122.97s\n",
      "model_saved at f1: 0.6645567576295819 from 0.639634601043997\n",
      "Epoch 3/15  loss=0.1048  val_loss=0.1017  f1=0.678  time=122.99s\n",
      "model_saved at f1: 0.677819042297959 from 0.6645567576295819\n",
      "Epoch 4/15  loss=0.1012  val_loss=0.0992  f1=0.680  time=123.33s\n",
      "model_saved at f1: 0.6803608165849027 from 0.677819042297959\n",
      "Epoch 5/15  loss=0.0985  val_loss=0.0981  f1=0.685  time=122.61s\n",
      "model_saved at f1: 0.6850068775790922 from 0.6803608165849027\n",
      "Epoch 6/15  loss=0.0959  val_loss=0.0967  f1=0.688  time=122.55s\n",
      "model_saved at f1: 0.6883753953398308 from 0.6850068775790922\n",
      "Epoch 7/15  loss=0.0937  val_loss=0.0975  f1=0.688  time=123.07s\n",
      "Epoch 8/15  loss=0.0919  val_loss=0.0957  f1=0.692  time=122.75s\n",
      "model_saved at f1: 0.6916565125376771 from 0.6883753953398308\n",
      "Epoch 9/15  loss=0.0901  val_loss=0.0956  f1=0.694  time=123.59s\n",
      "model_saved at f1: 0.6937243098035407 from 0.6916565125376771\n",
      "Epoch 10/15  loss=0.0887  val_loss=0.0968  f1=0.691  time=122.87s\n",
      "Epoch 11/15  loss=0.0875  val_loss=0.0967  f1=0.693  time=120.42s\n",
      "Epoch 12/15  loss=0.0857  val_loss=0.0964  f1=0.692  time=123.27s\n",
      "Epoch 13/15  loss=0.0849  val_loss=0.0975  f1=0.692  time=122.60s\n",
      "Epoch 14/15  loss=0.0833  val_loss=0.0972  f1=0.692  time=123.13s\n",
      "Epoch 15/15  loss=0.0820  val_loss=0.0977  f1=0.688  time=123.10s\n",
      "{'threshold': 0.3002305328845978, 'f1': 0.6862580929676664}\n",
      "f1 score: 0.7027453633237499\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1484  val_loss=0.1096  f1=0.649  time=122.61s\n",
      "model_saved at f1: 0.6494371023005384 from 0.0\n",
      "Epoch 2/15  loss=0.1100  val_loss=0.1041  f1=0.664  time=122.70s\n",
      "model_saved at f1: 0.6639650276013912 from 0.6494371023005384\n",
      "Epoch 3/15  loss=0.1048  val_loss=0.1003  f1=0.675  time=122.31s\n",
      "model_saved at f1: 0.6746785008584107 from 0.6639650276013912\n",
      "Epoch 4/15  loss=0.1010  val_loss=0.0985  f1=0.681  time=121.73s\n",
      "model_saved at f1: 0.6806730800167303 from 0.6746785008584107\n",
      "Epoch 5/15  loss=0.0981  val_loss=0.0978  f1=0.686  time=123.20s\n",
      "model_saved at f1: 0.6857214178731903 from 0.6806730800167303\n",
      "Epoch 6/15  loss=0.0957  val_loss=0.0972  f1=0.687  time=122.14s\n",
      "model_saved at f1: 0.686875079103911 from 0.6857214178731903\n",
      "Epoch 7/15  loss=0.0933  val_loss=0.0974  f1=0.688  time=122.70s\n",
      "model_saved at f1: 0.6879518843176147 from 0.686875079103911\n",
      "Epoch 8/15  loss=0.0919  val_loss=0.0978  f1=0.691  time=122.71s\n",
      "model_saved at f1: 0.6905677020981664 from 0.6879518843176147\n",
      "Epoch 9/15  loss=0.0900  val_loss=0.0973  f1=0.689  time=122.11s\n",
      "Epoch 10/15  loss=0.0883  val_loss=0.0978  f1=0.691  time=123.20s\n",
      "model_saved at f1: 0.6908119727713905 from 0.6905677020981664\n",
      "Epoch 11/15  loss=0.0865  val_loss=0.0985  f1=0.691  time=120.39s\n",
      "model_saved at f1: 0.6909380982769624 from 0.6908119727713905\n",
      "Epoch 12/15  loss=0.0852  val_loss=0.0977  f1=0.691  time=122.71s\n",
      "Epoch 13/15  loss=0.0842  val_loss=0.0984  f1=0.689  time=122.27s\n",
      "Epoch 14/15  loss=0.0828  val_loss=0.0985  f1=0.687  time=122.41s\n",
      "Epoch 15/15  loss=0.0817  val_loss=0.0993  f1=0.689  time=122.18s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1450  val_loss=0.1091  f1=0.646  time=122.31s\n",
      "model_saved at f1: 0.6460549574160371 from 0.0\n",
      "Epoch 2/15  loss=0.1096  val_loss=0.1015  f1=0.675  time=122.15s\n",
      "model_saved at f1: 0.6751720555771977 from 0.6460549574160371\n",
      "Epoch 3/15  loss=0.1042  val_loss=0.0999  f1=0.679  time=122.93s\n",
      "model_saved at f1: 0.6787139473769588 from 0.6751720555771977\n",
      "Epoch 4/15  loss=0.1007  val_loss=0.0979  f1=0.685  time=122.46s\n",
      "model_saved at f1: 0.6851013263962489 from 0.6787139473769588\n",
      "Epoch 5/15  loss=0.0978  val_loss=0.0967  f1=0.689  time=122.35s\n",
      "model_saved at f1: 0.6886439804073214 from 0.6851013263962489\n",
      "Epoch 6/15  loss=0.0956  val_loss=0.0960  f1=0.692  time=122.77s\n",
      "model_saved at f1: 0.6915621251904438 from 0.6886439804073214\n",
      "Epoch 7/15  loss=0.0934  val_loss=0.0971  f1=0.691  time=122.84s\n",
      "Epoch 8/15  loss=0.0916  val_loss=0.0981  f1=0.691  time=122.48s\n",
      "Epoch 9/15  loss=0.0898  val_loss=0.0955  f1=0.693  time=123.16s\n",
      "model_saved at f1: 0.6934029119958768 from 0.6915621251904438\n",
      "Epoch 10/15  loss=0.0882  val_loss=0.0961  f1=0.692  time=122.95s\n",
      "Epoch 11/15  loss=0.0867  val_loss=0.0988  f1=0.689  time=120.90s\n",
      "Epoch 12/15  loss=0.0854  val_loss=0.0971  f1=0.692  time=123.26s\n",
      "Epoch 13/15  loss=0.0840  val_loss=0.0969  f1=0.691  time=122.35s\n",
      "Epoch 14/15  loss=0.0827  val_loss=0.0989  f1=0.688  time=122.64s\n",
      "Epoch 15/15  loss=0.0816  val_loss=0.0997  f1=0.689  time=122.59s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1697  val_loss=0.1143  f1=0.640  time=123.84s\n",
      "model_saved at f1: 0.6404265811860436 from 0.0\n",
      "Epoch 2/15  loss=0.1119  val_loss=0.1061  f1=0.667  time=123.79s\n",
      "model_saved at f1: 0.6668747269209163 from 0.6404265811860436\n",
      "Epoch 3/15  loss=0.1060  val_loss=0.1010  f1=0.677  time=122.45s\n",
      "model_saved at f1: 0.6772112922568309 from 0.6668747269209163\n",
      "Epoch 4/15  loss=0.1019  val_loss=0.0986  f1=0.682  time=124.08s\n",
      "model_saved at f1: 0.6823145153543181 from 0.6772112922568309\n",
      "Epoch 5/15  loss=0.0989  val_loss=0.0974  f1=0.686  time=122.80s\n",
      "model_saved at f1: 0.6856247060667817 from 0.6823145153543181\n",
      "Epoch 6/15  loss=0.0967  val_loss=0.0954  f1=0.690  time=123.27s\n",
      "model_saved at f1: 0.6903622250970246 from 0.6856247060667817\n",
      "Epoch 7/15  loss=0.0942  val_loss=0.0948  f1=0.692  time=122.83s\n",
      "model_saved at f1: 0.6915621436716077 from 0.6903622250970246\n",
      "Epoch 8/15  loss=0.0926  val_loss=0.0954  f1=0.691  time=123.76s\n",
      "Epoch 9/15  loss=0.0907  val_loss=0.0943  f1=0.693  time=122.45s\n",
      "model_saved at f1: 0.6927794263105835 from 0.6915621436716077\n",
      "Epoch 10/15  loss=0.0889  val_loss=0.0954  f1=0.693  time=123.89s\n",
      "model_saved at f1: 0.6932921731301047 from 0.6927794263105835\n",
      "Epoch 11/15  loss=0.0874  val_loss=0.0950  f1=0.694  time=121.67s\n",
      "model_saved at f1: 0.6937429264349232 from 0.6932921731301047\n",
      "Epoch 12/15  loss=0.0861  val_loss=0.0958  f1=0.694  time=123.13s\n",
      "model_saved at f1: 0.693910359462414 from 0.6937429264349232\n",
      "Epoch 13/15  loss=0.0848  val_loss=0.0967  f1=0.692  time=123.21s\n",
      "Epoch 14/15  loss=0.0836  val_loss=0.0966  f1=0.694  time=122.91s\n",
      "model_saved at f1: 0.6943405955910791 from 0.693910359462414\n",
      "Epoch 15/15  loss=0.0827  val_loss=0.0962  f1=0.693  time=123.04s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1535  val_loss=0.1136  f1=0.637  time=122.45s\n",
      "model_saved at f1: 0.6369386263229826 from 0.0\n",
      "Epoch 2/15  loss=0.1103  val_loss=0.1057  f1=0.660  time=122.85s\n",
      "model_saved at f1: 0.6602942615730923 from 0.6369386263229826\n",
      "Epoch 3/15  loss=0.1048  val_loss=0.1014  f1=0.670  time=123.18s\n",
      "model_saved at f1: 0.6704505818562323 from 0.6602942615730923\n",
      "Epoch 4/15  loss=0.1009  val_loss=0.1004  f1=0.675  time=122.84s\n",
      "model_saved at f1: 0.6753114993183476 from 0.6704505818562323\n",
      "Epoch 5/15  loss=0.0983  val_loss=0.0990  f1=0.679  time=122.52s\n",
      "model_saved at f1: 0.6793630159246019 from 0.6753114993183476\n",
      "Epoch 6/15  loss=0.0959  val_loss=0.0976  f1=0.684  time=122.86s\n",
      "model_saved at f1: 0.6837673526447544 from 0.6793630159246019\n",
      "Epoch 7/15  loss=0.0938  val_loss=0.0973  f1=0.686  time=122.30s\n",
      "model_saved at f1: 0.6861686653521673 from 0.6837673526447544\n",
      "Epoch 8/15  loss=0.0920  val_loss=0.0976  f1=0.685  time=122.38s\n",
      "Epoch 9/15  loss=0.0902  val_loss=0.0982  f1=0.686  time=123.06s\n",
      "Epoch 10/15  loss=0.0886  val_loss=0.0972  f1=0.690  time=122.95s\n",
      "model_saved at f1: 0.6895439592336967 from 0.6861686653521673\n",
      "Epoch 11/15  loss=0.0868  val_loss=0.0975  f1=0.688  time=121.01s\n",
      "Epoch 12/15  loss=0.0856  val_loss=0.0990  f1=0.689  time=122.73s\n",
      "Epoch 13/15  loss=0.0844  val_loss=0.0983  f1=0.690  time=123.15s\n",
      "model_saved at f1: 0.6903286384976526 from 0.6895439592336967\n",
      "Epoch 14/15  loss=0.0834  val_loss=0.1000  f1=0.686  time=123.57s\n",
      "Epoch 15/15  loss=0.0820  val_loss=0.1009  f1=0.687  time=122.64s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1680  val_loss=0.1123  f1=0.637  time=122.44s\n",
      "model_saved at f1: 0.6373336761586424 from 0.0\n",
      "Epoch 2/15  loss=0.1130  val_loss=0.1048  f1=0.658  time=122.65s\n",
      "model_saved at f1: 0.6581572384827404 from 0.6373336761586424\n",
      "Epoch 3/15  loss=0.1067  val_loss=0.1031  f1=0.668  time=122.81s\n",
      "model_saved at f1: 0.6677479147358666 from 0.6581572384827404\n",
      "Epoch 4/15  loss=0.1028  val_loss=0.1005  f1=0.675  time=122.84s\n",
      "model_saved at f1: 0.6750677939065242 from 0.6677479147358666\n",
      "Epoch 5/15  loss=0.0996  val_loss=0.0991  f1=0.681  time=123.11s\n",
      "model_saved at f1: 0.681041367193739 from 0.6750677939065242\n",
      "Epoch 6/15  loss=0.0969  val_loss=0.0981  f1=0.686  time=122.64s\n",
      "model_saved at f1: 0.6863598540613199 from 0.681041367193739\n",
      "Epoch 7/15  loss=0.0944  val_loss=0.0977  f1=0.685  time=122.88s\n",
      "Epoch 8/15  loss=0.0925  val_loss=0.0968  f1=0.686  time=122.99s\n",
      "model_saved at f1: 0.686408698433003 from 0.6863598540613199\n",
      "Epoch 9/15  loss=0.0906  val_loss=0.0977  f1=0.688  time=122.77s\n",
      "model_saved at f1: 0.6879320185173441 from 0.686408698433003\n",
      "Epoch 10/15  loss=0.0888  val_loss=0.1003  f1=0.689  time=123.54s\n",
      "model_saved at f1: 0.689344262295082 from 0.6879320185173441\n",
      "Epoch 11/15  loss=0.0875  val_loss=0.0977  f1=0.690  time=121.14s\n",
      "model_saved at f1: 0.6902114531782249 from 0.689344262295082\n",
      "Epoch 12/15  loss=0.0858  val_loss=0.0973  f1=0.687  time=122.21s\n",
      "Epoch 13/15  loss=0.0842  val_loss=0.0987  f1=0.688  time=122.89s\n",
      "Epoch 14/15  loss=0.0833  val_loss=0.0981  f1=0.688  time=121.88s\n",
      "Epoch 15/15  loss=0.0819  val_loss=0.1001  f1=0.690  time=122.36s\n",
      "{'threshold': 0.30991223454475403, 'f1': 0.689034873874105}\n",
      "f1 score: 0.7004501904651967\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1555  val_loss=0.1083  f1=0.648  time=122.94s\n",
      "model_saved at f1: 0.6481622790283607 from 0.0\n",
      "Epoch 2/15  loss=0.1108  val_loss=0.1035  f1=0.665  time=122.55s\n",
      "model_saved at f1: 0.6653256223283882 from 0.6481622790283607\n",
      "Epoch 3/15  loss=0.1047  val_loss=0.1001  f1=0.675  time=122.93s\n",
      "model_saved at f1: 0.6753594542034621 from 0.6653256223283882\n",
      "Epoch 4/15  loss=0.1014  val_loss=0.0976  f1=0.681  time=122.67s\n",
      "model_saved at f1: 0.6813062354495689 from 0.6753594542034621\n",
      "Epoch 5/15  loss=0.0984  val_loss=0.0979  f1=0.686  time=122.34s\n",
      "model_saved at f1: 0.6862763763287323 from 0.6813062354495689\n",
      "Epoch 6/15  loss=0.0959  val_loss=0.0964  f1=0.688  time=122.36s\n",
      "model_saved at f1: 0.6882955306727917 from 0.6862763763287323\n",
      "Epoch 7/15  loss=0.0938  val_loss=0.0976  f1=0.689  time=122.80s\n",
      "model_saved at f1: 0.6889541450192004 from 0.6882955306727917\n",
      "Epoch 8/15  loss=0.0919  val_loss=0.0968  f1=0.687  time=122.99s\n",
      "Epoch 9/15  loss=0.0901  val_loss=0.0963  f1=0.690  time=123.01s\n",
      "model_saved at f1: 0.6898444647758464 from 0.6889541450192004\n",
      "Epoch 10/15  loss=0.0883  val_loss=0.0960  f1=0.692  time=123.33s\n",
      "model_saved at f1: 0.6917207172834795 from 0.6898444647758464\n",
      "Epoch 11/15  loss=0.0871  val_loss=0.0988  f1=0.693  time=120.88s\n",
      "model_saved at f1: 0.6925101543460601 from 0.6917207172834795\n",
      "Epoch 12/15  loss=0.0856  val_loss=0.0965  f1=0.692  time=123.03s\n",
      "Epoch 13/15  loss=0.0843  val_loss=0.0974  f1=0.690  time=123.32s\n",
      "Epoch 14/15  loss=0.0832  val_loss=0.0976  f1=0.693  time=122.87s\n",
      "model_saved at f1: 0.6926701067273484 from 0.6925101543460601\n",
      "Epoch 15/15  loss=0.0819  val_loss=0.1002  f1=0.691  time=123.16s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1733  val_loss=0.1168  f1=0.623  time=123.09s\n",
      "model_saved at f1: 0.623211943281904 from 0.0\n",
      "Epoch 2/15  loss=0.1147  val_loss=0.1080  f1=0.654  time=122.87s\n",
      "model_saved at f1: 0.6535420468004738 from 0.623211943281904\n",
      "Epoch 3/15  loss=0.1104  val_loss=0.1053  f1=0.662  time=123.10s\n",
      "model_saved at f1: 0.6622679631266575 from 0.6535420468004738\n",
      "Epoch 4/15  loss=0.1054  val_loss=0.1022  f1=0.671  time=122.81s\n",
      "model_saved at f1: 0.6711186375690961 from 0.6622679631266575\n",
      "Epoch 5/15  loss=0.1027  val_loss=0.1008  f1=0.675  time=123.05s\n",
      "model_saved at f1: 0.6752919919984512 from 0.6711186375690961\n",
      "Epoch 6/15  loss=0.1001  val_loss=0.1005  f1=0.679  time=122.86s\n",
      "model_saved at f1: 0.6785115439184831 from 0.6752919919984512\n",
      "Epoch 7/15  loss=0.0974  val_loss=0.0990  f1=0.680  time=123.20s\n",
      "model_saved at f1: 0.6801645250773204 from 0.6785115439184831\n",
      "Epoch 8/15  loss=0.0956  val_loss=0.0982  f1=0.683  time=122.61s\n",
      "model_saved at f1: 0.6832079095689411 from 0.6801645250773204\n",
      "Epoch 9/15  loss=0.0935  val_loss=0.0980  f1=0.683  time=122.47s\n",
      "model_saved at f1: 0.6832403818886472 from 0.6832079095689411\n",
      "Epoch 10/15  loss=0.0918  val_loss=0.0981  f1=0.686  time=123.36s\n",
      "model_saved at f1: 0.6857775668406897 from 0.6832403818886472\n",
      "Epoch 11/15  loss=0.0899  val_loss=0.0984  f1=0.686  time=121.36s\n",
      "model_saved at f1: 0.6859196590439235 from 0.6857775668406897\n",
      "Epoch 12/15  loss=0.0882  val_loss=0.0978  f1=0.686  time=122.54s\n",
      "model_saved at f1: 0.6860598888998456 from 0.6859196590439235\n",
      "Epoch 13/15  loss=0.0871  val_loss=0.0977  f1=0.684  time=122.78s\n",
      "Epoch 14/15  loss=0.0855  val_loss=0.0990  f1=0.685  time=123.31s\n",
      "Epoch 15/15  loss=0.0844  val_loss=0.0994  f1=0.682  time=122.34s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1729  val_loss=0.1110  f1=0.644  time=122.92s\n",
      "model_saved at f1: 0.643809887626142 from 0.0\n",
      "Epoch 2/15  loss=0.1133  val_loss=0.1067  f1=0.663  time=122.70s\n",
      "model_saved at f1: 0.6630314112192255 from 0.643809887626142\n",
      "Epoch 3/15  loss=0.1069  val_loss=0.1013  f1=0.673  time=122.35s\n",
      "model_saved at f1: 0.6731738437001594 from 0.6630314112192255\n",
      "Epoch 4/15  loss=0.1033  val_loss=0.0983  f1=0.681  time=122.28s\n",
      "model_saved at f1: 0.6810715105413292 from 0.6731738437001594\n",
      "Epoch 5/15  loss=0.1002  val_loss=0.0972  f1=0.686  time=122.49s\n",
      "model_saved at f1: 0.6861823723647448 from 0.6810715105413292\n",
      "Epoch 6/15  loss=0.0976  val_loss=0.0959  f1=0.690  time=123.05s\n",
      "model_saved at f1: 0.690009094452384 from 0.6861823723647448\n",
      "Epoch 7/15  loss=0.0953  val_loss=0.0959  f1=0.689  time=122.83s\n",
      "Epoch 8/15  loss=0.0933  val_loss=0.0952  f1=0.691  time=122.88s\n",
      "model_saved at f1: 0.6909371582942048 from 0.690009094452384\n",
      "Epoch 9/15  loss=0.0915  val_loss=0.0951  f1=0.692  time=122.46s\n",
      "model_saved at f1: 0.6921266021449124 from 0.6909371582942048\n",
      "Epoch 10/15  loss=0.0896  val_loss=0.0948  f1=0.694  time=122.94s\n",
      "model_saved at f1: 0.693828798938288 from 0.6921266021449124\n",
      "Epoch 11/15  loss=0.0881  val_loss=0.0953  f1=0.693  time=120.04s\n",
      "Epoch 12/15  loss=0.0868  val_loss=0.0952  f1=0.694  time=123.38s\n",
      "model_saved at f1: 0.6943930598966639 from 0.693828798938288\n",
      "Epoch 13/15  loss=0.0856  val_loss=0.0961  f1=0.693  time=122.92s\n",
      "Epoch 14/15  loss=0.0843  val_loss=0.0957  f1=0.695  time=123.06s\n",
      "model_saved at f1: 0.6953432797261512 from 0.6943930598966639\n",
      "Epoch 15/15  loss=0.0834  val_loss=0.0969  f1=0.691  time=122.47s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1596  val_loss=0.1099  f1=0.642  time=122.86s\n",
      "model_saved at f1: 0.6416223194886168 from 0.0\n",
      "Epoch 2/15  loss=0.1104  val_loss=0.1045  f1=0.661  time=123.84s\n",
      "model_saved at f1: 0.6606860498886702 from 0.6416223194886168\n",
      "Epoch 3/15  loss=0.1049  val_loss=0.1006  f1=0.673  time=122.94s\n",
      "model_saved at f1: 0.6726763465438976 from 0.6606860498886702\n",
      "Epoch 4/15  loss=0.1011  val_loss=0.0989  f1=0.682  time=123.05s\n",
      "model_saved at f1: 0.6820186491935484 from 0.6726763465438976\n",
      "Epoch 5/15  loss=0.0984  val_loss=0.0985  f1=0.683  time=123.20s\n",
      "model_saved at f1: 0.6825044114763741 from 0.6820186491935484\n",
      "Epoch 6/15  loss=0.0959  val_loss=0.0977  f1=0.685  time=122.94s\n",
      "model_saved at f1: 0.685155395405701 from 0.6825044114763741\n",
      "Epoch 7/15  loss=0.0940  val_loss=0.0973  f1=0.685  time=123.00s\n",
      "Epoch 8/15  loss=0.0919  val_loss=0.0968  f1=0.686  time=123.91s\n",
      "model_saved at f1: 0.6861609727851766 from 0.685155395405701\n",
      "Epoch 9/15  loss=0.0903  val_loss=0.0966  f1=0.688  time=122.85s\n",
      "model_saved at f1: 0.6879519628099173 from 0.6861609727851766\n",
      "Epoch 10/15  loss=0.0889  val_loss=0.0974  f1=0.685  time=122.83s\n",
      "Epoch 11/15  loss=0.0872  val_loss=0.0976  f1=0.687  time=122.24s\n",
      "Epoch 12/15  loss=0.0856  val_loss=0.0972  f1=0.690  time=122.86s\n",
      "model_saved at f1: 0.6899155362170464 from 0.6879519628099173\n",
      "Epoch 13/15  loss=0.0845  val_loss=0.0978  f1=0.687  time=122.87s\n",
      "Epoch 14/15  loss=0.0830  val_loss=0.0987  f1=0.689  time=123.74s\n",
      "Epoch 15/15  loss=0.0821  val_loss=0.0993  f1=0.687  time=122.60s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1554  val_loss=0.1098  f1=0.640  time=123.03s\n",
      "model_saved at f1: 0.6401651721056362 from 0.0\n",
      "Epoch 2/15  loss=0.1120  val_loss=0.1053  f1=0.658  time=122.00s\n",
      "model_saved at f1: 0.6581294874584279 from 0.6401651721056362\n",
      "Epoch 3/15  loss=0.1073  val_loss=0.1089  f1=0.667  time=122.35s\n",
      "model_saved at f1: 0.6669779313350142 from 0.6581294874584279\n",
      "Epoch 4/15  loss=0.1033  val_loss=0.1004  f1=0.676  time=122.07s\n",
      "model_saved at f1: 0.6763303574285623 from 0.6669779313350142\n",
      "Epoch 5/15  loss=0.1004  val_loss=0.0989  f1=0.678  time=122.35s\n",
      "model_saved at f1: 0.6779245100255078 from 0.6763303574285623\n",
      "Epoch 6/15  loss=0.0981  val_loss=0.0986  f1=0.681  time=122.13s\n",
      "model_saved at f1: 0.6807715078054405 from 0.6779245100255078\n",
      "Epoch 7/15  loss=0.0963  val_loss=0.0972  f1=0.684  time=122.69s\n",
      "model_saved at f1: 0.6844539309854145 from 0.6807715078054405\n",
      "Epoch 8/15  loss=0.0945  val_loss=0.0975  f1=0.684  time=123.09s\n",
      "Epoch 9/15  loss=0.0927  val_loss=0.0978  f1=0.686  time=122.90s\n",
      "model_saved at f1: 0.6861418347430057 from 0.6844539309854145\n",
      "Epoch 10/15  loss=0.0912  val_loss=0.0982  f1=0.686  time=122.26s\n",
      "Epoch 11/15  loss=0.0900  val_loss=0.0976  f1=0.687  time=121.42s\n",
      "model_saved at f1: 0.6866511137387746 from 0.6861418347430057\n",
      "Epoch 12/15  loss=0.0884  val_loss=0.0968  f1=0.686  time=122.01s\n",
      "Epoch 13/15  loss=0.0871  val_loss=0.0977  f1=0.687  time=121.54s\n",
      "model_saved at f1: 0.6874208732348644 from 0.6866511137387746\n",
      "Epoch 14/15  loss=0.0860  val_loss=0.0979  f1=0.689  time=122.57s\n",
      "model_saved at f1: 0.6886618004866181 from 0.6874208732348644\n",
      "Epoch 15/15  loss=0.0848  val_loss=0.0980  f1=0.688  time=122.27s\n",
      "{'threshold': 0.3193740248680115, 'f1': 0.6856848675954824}\n",
      "f1 score: 0.7039413382218149\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1690  val_loss=0.1122  f1=0.637  time=121.69s\n",
      "model_saved at f1: 0.6365133662739135 from 0.0\n",
      "Epoch 2/15  loss=0.1117  val_loss=0.1078  f1=0.657  time=121.76s\n",
      "model_saved at f1: 0.657301388755252 from 0.6365133662739135\n",
      "Epoch 3/15  loss=0.1056  val_loss=0.1022  f1=0.671  time=121.91s\n",
      "model_saved at f1: 0.6706579434637698 from 0.657301388755252\n",
      "Epoch 4/15  loss=0.1017  val_loss=0.0997  f1=0.679  time=122.10s\n",
      "model_saved at f1: 0.6787330316742081 from 0.6706579434637698\n",
      "Epoch 5/15  loss=0.0989  val_loss=0.0985  f1=0.682  time=122.60s\n",
      "model_saved at f1: 0.6818444173667672 from 0.6787330316742081\n",
      "Epoch 6/15  loss=0.0969  val_loss=0.0979  f1=0.684  time=121.71s\n",
      "model_saved at f1: 0.6839012150026413 from 0.6818444173667672\n",
      "Epoch 7/15  loss=0.0944  val_loss=0.0974  f1=0.687  time=121.77s\n",
      "model_saved at f1: 0.6869829643919826 from 0.6839012150026413\n",
      "Epoch 8/15  loss=0.0925  val_loss=0.0966  f1=0.689  time=122.59s\n",
      "model_saved at f1: 0.6889831331541973 from 0.6869829643919826\n",
      "Epoch 9/15  loss=0.0910  val_loss=0.0968  f1=0.689  time=121.91s\n",
      "model_saved at f1: 0.6893839259808505 from 0.6889831331541973\n",
      "Epoch 10/15  loss=0.0893  val_loss=0.0974  f1=0.690  time=121.81s\n",
      "model_saved at f1: 0.6900883981478483 from 0.6893839259808505\n",
      "Epoch 11/15  loss=0.0878  val_loss=0.0982  f1=0.692  time=119.87s\n",
      "model_saved at f1: 0.6923249623143783 from 0.6900883981478483\n",
      "Epoch 12/15  loss=0.0864  val_loss=0.0974  f1=0.690  time=121.26s\n",
      "Epoch 13/15  loss=0.0852  val_loss=0.0980  f1=0.693  time=121.96s\n",
      "model_saved at f1: 0.6926545361517061 from 0.6923249623143783\n",
      "Epoch 14/15  loss=0.0837  val_loss=0.1009  f1=0.692  time=122.13s\n",
      "Epoch 15/15  loss=0.0826  val_loss=0.0999  f1=0.689  time=121.69s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1517  val_loss=0.1107  f1=0.641  time=122.23s\n",
      "model_saved at f1: 0.6411427080615706 from 0.0\n",
      "Epoch 2/15  loss=0.1104  val_loss=0.1040  f1=0.662  time=123.41s\n",
      "model_saved at f1: 0.6615572873059833 from 0.6411427080615706\n",
      "Epoch 3/15  loss=0.1048  val_loss=0.1008  f1=0.671  time=122.77s\n",
      "model_saved at f1: 0.6706270196168033 from 0.6615572873059833\n",
      "Epoch 4/15  loss=0.1012  val_loss=0.1004  f1=0.674  time=122.61s\n",
      "model_saved at f1: 0.6736690080745749 from 0.6706270196168033\n",
      "Epoch 5/15  loss=0.0980  val_loss=0.0987  f1=0.681  time=123.13s\n",
      "model_saved at f1: 0.6806874568696395 from 0.6736690080745749\n",
      "Epoch 6/15  loss=0.0955  val_loss=0.0977  f1=0.683  time=122.23s\n",
      "model_saved at f1: 0.6830115952102619 from 0.6806874568696395\n",
      "Epoch 7/15  loss=0.0936  val_loss=0.0976  f1=0.684  time=122.20s\n",
      "model_saved at f1: 0.6842189860803035 from 0.6830115952102619\n",
      "Epoch 8/15  loss=0.0916  val_loss=0.0970  f1=0.687  time=122.83s\n",
      "model_saved at f1: 0.687490060116416 from 0.6842189860803035\n",
      "Epoch 9/15  loss=0.0899  val_loss=0.0966  f1=0.687  time=122.76s\n",
      "Epoch 10/15  loss=0.0883  val_loss=0.0974  f1=0.686  time=122.14s\n",
      "Epoch 11/15  loss=0.0868  val_loss=0.0972  f1=0.691  time=121.86s\n",
      "model_saved at f1: 0.6906968540550942 from 0.687490060116416\n",
      "Epoch 12/15  loss=0.0856  val_loss=0.0981  f1=0.688  time=122.73s\n",
      "Epoch 13/15  loss=0.0842  val_loss=0.0985  f1=0.688  time=122.71s\n",
      "Epoch 14/15  loss=0.0831  val_loss=0.0994  f1=0.688  time=122.74s\n",
      "Epoch 15/15  loss=0.0818  val_loss=0.0991  f1=0.687  time=122.96s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1439  val_loss=0.1091  f1=0.645  time=122.84s\n",
      "model_saved at f1: 0.645299009143913 from 0.0\n",
      "Epoch 2/15  loss=0.1100  val_loss=0.1026  f1=0.667  time=122.91s\n",
      "model_saved at f1: 0.6667711304020809 from 0.645299009143913\n",
      "Epoch 3/15  loss=0.1046  val_loss=0.1001  f1=0.673  time=122.05s\n",
      "model_saved at f1: 0.6734401296515613 from 0.6667711304020809\n",
      "Epoch 4/15  loss=0.1009  val_loss=0.0981  f1=0.682  time=121.80s\n",
      "model_saved at f1: 0.6824496558756054 from 0.6734401296515613\n",
      "Epoch 5/15  loss=0.0980  val_loss=0.0970  f1=0.687  time=122.96s\n",
      "model_saved at f1: 0.6867573508330413 from 0.6824496558756054\n",
      "Epoch 6/15  loss=0.0955  val_loss=0.0967  f1=0.688  time=122.72s\n",
      "model_saved at f1: 0.6877591375901002 from 0.6867573508330413\n",
      "Epoch 7/15  loss=0.0933  val_loss=0.0984  f1=0.690  time=121.77s\n",
      "model_saved at f1: 0.6899515699669649 from 0.6877591375901002\n",
      "Epoch 8/15  loss=0.0915  val_loss=0.0969  f1=0.690  time=122.91s\n",
      "model_saved at f1: 0.689977286541476 from 0.6899515699669649\n",
      "Epoch 9/15  loss=0.0896  val_loss=0.0967  f1=0.690  time=122.81s\n",
      "Epoch 10/15  loss=0.0882  val_loss=0.0966  f1=0.692  time=123.09s\n",
      "model_saved at f1: 0.6924532537926168 from 0.689977286541476\n",
      "Epoch 11/15  loss=0.0866  val_loss=0.0986  f1=0.691  time=121.65s\n",
      "Epoch 12/15  loss=0.0850  val_loss=0.0968  f1=0.692  time=122.95s\n",
      "Epoch 13/15  loss=0.0841  val_loss=0.0978  f1=0.689  time=122.40s\n",
      "Epoch 14/15  loss=0.0827  val_loss=0.0999  f1=0.688  time=122.93s\n",
      "Epoch 15/15  loss=0.0817  val_loss=0.0996  f1=0.688  time=122.86s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1795  val_loss=0.1108  f1=0.635  time=122.82s\n",
      "model_saved at f1: 0.6350435624394967 from 0.0\n",
      "Epoch 2/15  loss=0.1133  val_loss=0.1041  f1=0.661  time=123.48s\n",
      "model_saved at f1: 0.6611504600516317 from 0.6350435624394967\n",
      "Epoch 3/15  loss=0.1080  val_loss=0.1010  f1=0.671  time=122.84s\n",
      "model_saved at f1: 0.6705827214497735 from 0.6611504600516317\n",
      "Epoch 4/15  loss=0.1031  val_loss=0.0982  f1=0.681  time=122.28s\n",
      "model_saved at f1: 0.6809324871639506 from 0.6705827214497735\n",
      "Epoch 5/15  loss=0.1002  val_loss=0.0973  f1=0.684  time=122.78s\n",
      "model_saved at f1: 0.6841007540510188 from 0.6809324871639506\n",
      "Epoch 6/15  loss=0.0976  val_loss=0.0955  f1=0.689  time=122.94s\n",
      "model_saved at f1: 0.6889757421105333 from 0.6841007540510188\n",
      "Epoch 7/15  loss=0.0952  val_loss=0.0961  f1=0.693  time=122.93s\n",
      "model_saved at f1: 0.6925659159795327 from 0.6889757421105333\n",
      "Epoch 8/15  loss=0.0933  val_loss=0.0973  f1=0.693  time=123.14s\n",
      "model_saved at f1: 0.6928729353622056 from 0.6925659159795327\n",
      "Epoch 9/15  loss=0.0915  val_loss=0.0952  f1=0.695  time=122.42s\n",
      "model_saved at f1: 0.6948626669714882 from 0.6928729353622056\n",
      "Epoch 10/15  loss=0.0897  val_loss=0.0959  f1=0.694  time=122.38s\n",
      "Epoch 11/15  loss=0.0881  val_loss=0.0953  f1=0.695  time=122.11s\n",
      "model_saved at f1: 0.6950263873340796 from 0.6948626669714882\n",
      "Epoch 12/15  loss=0.0864  val_loss=0.0964  f1=0.694  time=123.46s\n",
      "Epoch 13/15  loss=0.0851  val_loss=0.0959  f1=0.692  time=122.94s\n",
      "Epoch 14/15  loss=0.0838  val_loss=0.0972  f1=0.694  time=123.43s\n",
      "Epoch 15/15  loss=0.0828  val_loss=0.0993  f1=0.690  time=123.73s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1519  val_loss=0.1109  f1=0.634  time=122.65s\n",
      "model_saved at f1: 0.6344362351429489 from 0.0\n",
      "Epoch 2/15  loss=0.1108  val_loss=0.1051  f1=0.662  time=122.99s\n",
      "model_saved at f1: 0.6615418824052604 from 0.6344362351429489\n",
      "Epoch 3/15  loss=0.1051  val_loss=0.1004  f1=0.674  time=122.53s\n",
      "model_saved at f1: 0.6735831928413955 from 0.6615418824052604\n",
      "Epoch 4/15  loss=0.1015  val_loss=0.1002  f1=0.676  time=122.46s\n",
      "model_saved at f1: 0.6755434954090809 from 0.6735831928413955\n",
      "Epoch 5/15  loss=0.0985  val_loss=0.0981  f1=0.682  time=122.17s\n",
      "model_saved at f1: 0.6818656911418866 from 0.6755434954090809\n",
      "Epoch 6/15  loss=0.0962  val_loss=0.0971  f1=0.685  time=122.34s\n",
      "model_saved at f1: 0.6847670948488388 from 0.6818656911418866\n",
      "Epoch 7/15  loss=0.0938  val_loss=0.0978  f1=0.686  time=122.27s\n",
      "model_saved at f1: 0.6860472561951719 from 0.6847670948488388\n",
      "Epoch 8/15  loss=0.0921  val_loss=0.0977  f1=0.685  time=122.47s\n",
      "Epoch 9/15  loss=0.0901  val_loss=0.0968  f1=0.686  time=122.74s\n",
      "Epoch 10/15  loss=0.0884  val_loss=0.0982  f1=0.688  time=122.69s\n",
      "model_saved at f1: 0.6880464591063075 from 0.6860472561951719\n",
      "Epoch 11/15  loss=0.0871  val_loss=0.0968  f1=0.689  time=122.78s\n",
      "model_saved at f1: 0.6891006937690463 from 0.6880464591063075\n",
      "Epoch 12/15  loss=0.0856  val_loss=0.0988  f1=0.688  time=123.24s\n",
      "Epoch 13/15  loss=0.0843  val_loss=0.0971  f1=0.689  time=123.16s\n",
      "Epoch 14/15  loss=0.0834  val_loss=0.0990  f1=0.689  time=123.18s\n",
      "Epoch 15/15  loss=0.0821  val_loss=0.0979  f1=0.688  time=123.47s\n",
      "{'threshold': 0.32135018706321716, 'f1': 0.6877474433540101}\n",
      "f1 score: 0.7030868057529419\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1457  val_loss=0.1098  f1=0.648  time=122.66s\n",
      "model_saved at f1: 0.6478099240342654 from 0.0\n",
      "Epoch 2/15  loss=0.1099  val_loss=0.1029  f1=0.668  time=123.32s\n",
      "model_saved at f1: 0.6676056338028169 from 0.6478099240342654\n",
      "Epoch 3/15  loss=0.1046  val_loss=0.1006  f1=0.678  time=122.77s\n",
      "model_saved at f1: 0.6776112754923483 from 0.6676056338028169\n",
      "Epoch 4/15  loss=0.1009  val_loss=0.0988  f1=0.682  time=122.03s\n",
      "model_saved at f1: 0.6822001527883881 from 0.6776112754923483\n",
      "Epoch 5/15  loss=0.0981  val_loss=0.0980  f1=0.683  time=123.40s\n",
      "model_saved at f1: 0.6833579718806025 from 0.6822001527883881\n",
      "Epoch 6/15  loss=0.0958  val_loss=0.0979  f1=0.687  time=123.79s\n",
      "model_saved at f1: 0.6867109202134583 from 0.6833579718806025\n",
      "Epoch 7/15  loss=0.0936  val_loss=0.0962  f1=0.688  time=122.53s\n",
      "model_saved at f1: 0.6880971967347972 from 0.6867109202134583\n",
      "Epoch 8/15  loss=0.0916  val_loss=0.0967  f1=0.687  time=122.83s\n",
      "Epoch 9/15  loss=0.0898  val_loss=0.0964  f1=0.691  time=123.44s\n",
      "model_saved at f1: 0.6910336950314105 from 0.6880971967347972\n",
      "Epoch 10/15  loss=0.0882  val_loss=0.0965  f1=0.691  time=122.27s\n",
      "model_saved at f1: 0.6912134444885537 from 0.6910336950314105\n",
      "Epoch 11/15  loss=0.0869  val_loss=0.0976  f1=0.690  time=123.17s\n",
      "Epoch 12/15  loss=0.0854  val_loss=0.0972  f1=0.692  time=123.39s\n",
      "model_saved at f1: 0.6916674762783769 from 0.6912134444885537\n",
      "Epoch 13/15  loss=0.0839  val_loss=0.0982  f1=0.691  time=123.55s\n",
      "Epoch 14/15  loss=0.0834  val_loss=0.0986  f1=0.689  time=123.48s\n",
      "Epoch 15/15  loss=0.0817  val_loss=0.0996  f1=0.687  time=123.66s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1612  val_loss=0.1125  f1=0.642  time=122.97s\n",
      "model_saved at f1: 0.6421337154394763 from 0.0\n",
      "Epoch 2/15  loss=0.1102  val_loss=0.1074  f1=0.663  time=123.32s\n",
      "model_saved at f1: 0.6626460132046724 from 0.6421337154394763\n",
      "Epoch 3/15  loss=0.1047  val_loss=0.1021  f1=0.671  time=123.10s\n",
      "model_saved at f1: 0.6710666108624964 from 0.6626460132046724\n",
      "Epoch 4/15  loss=0.1006  val_loss=0.1001  f1=0.675  time=122.64s\n",
      "model_saved at f1: 0.6752810440625404 from 0.6710666108624964\n",
      "Epoch 5/15  loss=0.0975  val_loss=0.0999  f1=0.681  time=123.49s\n",
      "model_saved at f1: 0.6807762356339886 from 0.6752810440625404\n",
      "Epoch 6/15  loss=0.0953  val_loss=0.0977  f1=0.684  time=123.06s\n",
      "model_saved at f1: 0.6838898988932478 from 0.6807762356339886\n",
      "Epoch 7/15  loss=0.0930  val_loss=0.0988  f1=0.684  time=123.06s\n",
      "Epoch 8/15  loss=0.0913  val_loss=0.0992  f1=0.687  time=123.77s\n",
      "model_saved at f1: 0.6870553861788617 from 0.6838898988932478\n",
      "Epoch 9/15  loss=0.0892  val_loss=0.0988  f1=0.686  time=123.18s\n",
      "Epoch 10/15  loss=0.0877  val_loss=0.0991  f1=0.687  time=123.27s\n",
      "Epoch 11/15  loss=0.0861  val_loss=0.0991  f1=0.687  time=123.81s\n",
      "Epoch 12/15  loss=0.0848  val_loss=0.0994  f1=0.688  time=123.35s\n",
      "model_saved at f1: 0.6875098490340067 from 0.6870553861788617\n",
      "Epoch 13/15  loss=0.0831  val_loss=0.0998  f1=0.686  time=124.17s\n",
      "Epoch 14/15  loss=0.0822  val_loss=0.1002  f1=0.686  time=124.39s\n",
      "Epoch 15/15  loss=0.0813  val_loss=0.1013  f1=0.686  time=123.60s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1609  val_loss=0.1105  f1=0.640  time=124.02s\n",
      "model_saved at f1: 0.6402955602267661 from 0.0\n",
      "Epoch 2/15  loss=0.1112  val_loss=0.1042  f1=0.663  time=123.47s\n",
      "model_saved at f1: 0.6628245765696539 from 0.6402955602267661\n",
      "Epoch 3/15  loss=0.1055  val_loss=0.1011  f1=0.670  time=123.21s\n",
      "model_saved at f1: 0.6703541969917516 from 0.6628245765696539\n",
      "Epoch 4/15  loss=0.1017  val_loss=0.0991  f1=0.678  time=123.81s\n",
      "model_saved at f1: 0.677508628999658 from 0.6703541969917516\n",
      "Epoch 5/15  loss=0.0988  val_loss=0.0983  f1=0.682  time=124.08s\n",
      "model_saved at f1: 0.6818584070796461 from 0.677508628999658\n",
      "Epoch 6/15  loss=0.0963  val_loss=0.0977  f1=0.685  time=123.08s\n",
      "model_saved at f1: 0.6849049908055618 from 0.6818584070796461\n",
      "Epoch 7/15  loss=0.0941  val_loss=0.0969  f1=0.686  time=124.25s\n",
      "model_saved at f1: 0.6863468634686347 from 0.6849049908055618\n",
      "Epoch 8/15  loss=0.0920  val_loss=0.0966  f1=0.689  time=123.48s\n",
      "model_saved at f1: 0.6893742583148913 from 0.6863468634686347\n",
      "Epoch 9/15  loss=0.0905  val_loss=0.0973  f1=0.686  time=122.74s\n",
      "Epoch 10/15  loss=0.0888  val_loss=0.0983  f1=0.691  time=124.13s\n",
      "model_saved at f1: 0.6906108786610878 from 0.6893742583148913\n",
      "Epoch 11/15  loss=0.0873  val_loss=0.0989  f1=0.690  time=123.64s\n",
      "Epoch 12/15  loss=0.0859  val_loss=0.0984  f1=0.690  time=123.23s\n",
      "Epoch 13/15  loss=0.0845  val_loss=0.0994  f1=0.688  time=124.12s\n",
      "Epoch 14/15  loss=0.0834  val_loss=0.0994  f1=0.688  time=122.77s\n",
      "Epoch 15/15  loss=0.0820  val_loss=0.1003  f1=0.688  time=122.59s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1520  val_loss=0.1090  f1=0.645  time=122.93s\n",
      "model_saved at f1: 0.6452962107497346 from 0.0\n",
      "Epoch 2/15  loss=0.1106  val_loss=0.1035  f1=0.669  time=122.63s\n",
      "model_saved at f1: 0.6687328263786824 from 0.6452962107497346\n",
      "Epoch 3/15  loss=0.1049  val_loss=0.0999  f1=0.677  time=123.29s\n",
      "model_saved at f1: 0.6772067676319625 from 0.6687328263786824\n",
      "Epoch 4/15  loss=0.1013  val_loss=0.0983  f1=0.682  time=123.57s\n",
      "model_saved at f1: 0.6818496818496819 from 0.6772067676319625\n",
      "Epoch 5/15  loss=0.0984  val_loss=0.0975  f1=0.683  time=123.01s\n",
      "model_saved at f1: 0.6833763831754839 from 0.6818496818496819\n",
      "Epoch 6/15  loss=0.0960  val_loss=0.0965  f1=0.688  time=123.77s\n",
      "model_saved at f1: 0.6879026485325698 from 0.6833763831754839\n",
      "Epoch 7/15  loss=0.0939  val_loss=0.0970  f1=0.686  time=122.94s\n",
      "Epoch 8/15  loss=0.0920  val_loss=0.0959  f1=0.689  time=122.45s\n",
      "model_saved at f1: 0.6889448386358435 from 0.6879026485325698\n",
      "Epoch 9/15  loss=0.0902  val_loss=0.0987  f1=0.688  time=123.53s\n",
      "Epoch 10/15  loss=0.0888  val_loss=0.0967  f1=0.690  time=123.49s\n",
      "model_saved at f1: 0.6904700077901844 from 0.6889448386358435\n",
      "Epoch 11/15  loss=0.0872  val_loss=0.0965  f1=0.692  time=122.51s\n",
      "model_saved at f1: 0.691941909780234 from 0.6904700077901844\n",
      "Epoch 12/15  loss=0.0858  val_loss=0.0976  f1=0.690  time=124.04s\n",
      "Epoch 13/15  loss=0.0843  val_loss=0.1004  f1=0.691  time=124.13s\n",
      "Epoch 14/15  loss=0.0830  val_loss=0.0976  f1=0.692  time=123.31s\n",
      "model_saved at f1: 0.6923881685318454 from 0.691941909780234\n",
      "Epoch 15/15  loss=0.0820  val_loss=0.1002  f1=0.691  time=123.39s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1718  val_loss=0.1130  f1=0.640  time=123.76s\n",
      "model_saved at f1: 0.6395550921098366 from 0.0\n",
      "Epoch 2/15  loss=0.1127  val_loss=0.1038  f1=0.662  time=123.27s\n",
      "model_saved at f1: 0.6623318385650224 from 0.6395550921098366\n",
      "Epoch 3/15  loss=0.1065  val_loss=0.0998  f1=0.677  time=122.81s\n",
      "model_saved at f1: 0.6769211490868081 from 0.6623318385650224\n",
      "Epoch 4/15  loss=0.1031  val_loss=0.0997  f1=0.678  time=124.09s\n",
      "model_saved at f1: 0.6777835791672105 from 0.6769211490868081\n",
      "Epoch 5/15  loss=0.1001  val_loss=0.0971  f1=0.686  time=123.82s\n",
      "model_saved at f1: 0.6861791718308069 from 0.6777835791672105\n",
      "Epoch 6/15  loss=0.0977  val_loss=0.0963  f1=0.687  time=123.38s\n",
      "model_saved at f1: 0.6874920392306713 from 0.6861791718308069\n",
      "Epoch 7/15  loss=0.0956  val_loss=0.0963  f1=0.691  time=124.01s\n",
      "model_saved at f1: 0.6907420868301064 from 0.6874920392306713\n",
      "Epoch 8/15  loss=0.0934  val_loss=0.0972  f1=0.691  time=123.29s\n",
      "Epoch 9/15  loss=0.0917  val_loss=0.0946  f1=0.693  time=123.26s\n",
      "model_saved at f1: 0.6931198140315759 from 0.6907420868301064\n",
      "Epoch 10/15  loss=0.0898  val_loss=0.0943  f1=0.695  time=124.93s\n",
      "model_saved at f1: 0.695411618811409 from 0.6931198140315759\n",
      "Epoch 11/15  loss=0.0886  val_loss=0.0955  f1=0.694  time=123.30s\n",
      "Epoch 12/15  loss=0.0872  val_loss=0.0955  f1=0.696  time=123.03s\n",
      "model_saved at f1: 0.696283152191134 from 0.695411618811409\n",
      "Epoch 13/15  loss=0.0856  val_loss=0.0978  f1=0.695  time=123.90s\n",
      "Epoch 14/15  loss=0.0843  val_loss=0.1006  f1=0.692  time=123.50s\n",
      "Epoch 15/15  loss=0.0830  val_loss=0.0971  f1=0.695  time=123.37s\n",
      "{'threshold': 0.29441753029823303, 'f1': 0.6877574538254375}\n",
      "f1 score: 0.70295524908528\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
