{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# use only for tokenizer and padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1019):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# SEED = 1019\n",
    "# seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "class Config:\n",
    "    num_epochs = 15\n",
    "    batch_size = 512\n",
    "    test_batch_size = 512\n",
    "    vocab_size = 120000\n",
    "    max_length = 72\n",
    "    embedding_size = 300\n",
    "    hidden_size = 64\n",
    "    num_layers = 1\n",
    "    embedding_noise_var = 0.01\n",
    "    embedding_dropout = 0.3\n",
    "    layer_dropout = 0.1\n",
    "    dense_size = [hidden_size*2*4, int(hidden_size/4)] # depend on concat num\n",
    "    output_size = 1\n",
    "    num_cv_splits = 5\n",
    "    learning_rate = 0.001\n",
    "    clip_grad = 5.0\n",
    "    embeddings = ['glove', 'paragram', 'fasttext']\n",
    "    datadir = Path('./data/')\n",
    "    # datadir = Path('../input') # for kernel\n",
    "\n",
    "c = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [\n",
    "    ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '¬', '░', '¶', '↑', '±',  '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '₹', '´'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"'re\": \" are\",\n",
    "    \"tryin'\": \"trying\",\n",
    "    \"doesn'\": \"does not\",\n",
    "    'howdo': 'how do',\n",
    "    'whatare': 'what are',\n",
    "    'howcan': 'how can',\n",
    "    'howmuch': 'how much',\n",
    "    'howmany': 'how many',\n",
    "    'whydo': 'why do',\n",
    "    'doI': 'do I',\n",
    "    'theBest': 'the best',\n",
    "    'howdoes': 'how does',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spells = {\n",
    "    'colour': 'color',\n",
    "    'centre': 'center',\n",
    "    'favourite': 'favorite',\n",
    "    'travelling': 'traveling',\n",
    "    'counselling': 'counseling',\n",
    "    'theatre': 'theater',\n",
    "    'cancelled': 'canceled',\n",
    "    'labour': 'labor',\n",
    "    'organisation': 'organization',\n",
    "    'wwii': 'world war 2',\n",
    "    'citicise': 'criticize',\n",
    "    'youtu.be': 'youtube',\n",
    "    'youtu ': 'youtube ',\n",
    "    'qoura': 'quora',\n",
    "    'sallary': 'salary',\n",
    "    'Whta': 'what',\n",
    "    'whta': 'what',\n",
    "    'narcisist': 'narcissist',\n",
    "    'mastrubation': 'masturbation',\n",
    "    'mastrubate': 'masturbate',\n",
    "    \"mastrubating\": 'masturbating',\n",
    "    'pennis': 'penis',\n",
    "    'Etherium': 'ethereum',\n",
    "    'etherium': 'ethereum',\n",
    "    'narcissit': 'narcissist',\n",
    "    'bigdata': 'big data',\n",
    "    '2k17': '2017',\n",
    "    '2k18': '2018',\n",
    "    'qouta': 'quota',\n",
    "    'exboyfriend': 'ex boyfriend',\n",
    "    'exgirlfriend': 'ex girlfriend',\n",
    "    'airhostess': 'air hostess',\n",
    "    \"whst\": 'what',\n",
    "    'watsapp': 'whatsapp',\n",
    "    'demonitisation': 'demonetization',\n",
    "    'demonitization': 'demonetization',\n",
    "    'demonetisation': 'demonetization',\n",
    "    'quorans': 'quora user',\n",
    "    'quoran': 'quora user',\n",
    "    'pokémon': 'pokemon',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    train_df = pd.read_csv(datadir / 'train_local.csv')\n",
    "    test_df = pd.read_csv(datadir / 'test_local.csv')\n",
    "    print(\"Train shape : \", train_df.shape)\n",
    "    print(\"Test shape : \", test_df.shape)\n",
    "    return train_df, test_df\n",
    "\n",
    "def clean(df):\n",
    "    df = clean_lower(df)\n",
    "    df = clean_unicode(df)\n",
    "    df = clean_abbreviation(df, abbreviations)\n",
    "    df = clean_spells(df, spells)\n",
    "    df = clean_language(df)\n",
    "    df = clean_puncts(df, puncts)\n",
    "    df = clean_space(df)\n",
    "    return df\n",
    "\n",
    "def clean_unicode(df):\n",
    "    codes = ['\\x7f', '\\u200b', '\\xa0', '\\ufeff', '\\u200e', '\\u202a', '\\u202c', '\\u2060', '\\uf0d8', '\\ue019', '\\uf02d', '\\u200f', '\\u2061', '\\ue01b']\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: _clean_unicode(x, codes))\n",
    "    return df\n",
    "\n",
    "def _clean_unicode(x, codes):\n",
    "    for u in codes:\n",
    "        if u in x:\n",
    "            x = x.replace(u, '')\n",
    "    return x\n",
    "\n",
    "def clean_language(df):\n",
    "    langs1 = r'[\\p{Katakana}\\p{Hiragana}\\p{Han}]' # regex\n",
    "    langs2 = r'[ஆய்தஎழுத்துஆயுதஎழுத்துशुषछछशुषدوउसशुष북한내제តើបងប្អូនមានមធ្យបាយអ្វីខ្លះដើម្បីរកឃើញឯកសារអំពីប្រវត្តិស្ត្រនៃប្រាសាទអង្គរវट्टरौरआदસંઘરાજ્યपीतऊनअहএকটিবাড়িএকটিখামারএরঅধীনেপদেরবাছাইপরীক্ষাএরপ্রশ্নওউত্তরসহকোথায়পেতেপারিص、。Емелядуракلكلمقاممقال수능ί서로가를행복하게기乡국고등학교는몇시간업니《》싱관없어나이रचा키کپڤ」मिलगईकलेजेकोठंडकऋॠऌॡर]'\n",
    "    compiled_langs1 = regex.compile(langs1)\n",
    "    compiled_langs2 = re.compile(langs2)\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs1))\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs2))\n",
    "    return df\n",
    "\n",
    "def _clean_language(x, compiled_re):\n",
    "    return compiled_re.sub(' <lang> ', x)\n",
    "\n",
    "def clean_lower(df):\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "def clean_puncts(df, puncts):\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_puncts(x, puncts))\n",
    "    return df\n",
    "    \n",
    "def _clean_puncts(x, puncts):\n",
    "    x = str(x)\n",
    "    # added space around puncts after replace\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_spells(df, spells):\n",
    "    compiled_spells = re.compile('(%s)' % '|'.join(spells.keys()))\n",
    "    def replace(match):\n",
    "        return spells[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_spells(x, compiled_spells, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_spells(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_abbreviation(df, abbreviations):\n",
    "    compiled_abbreviation = re.compile('(%s)' % '|'.join(abbreviations.keys()))\n",
    "    def replace(match):\n",
    "        return abbreviations[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_abreviation(x, compiled_abbreviation, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_abreviation(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_space(df):\n",
    "    compiled_re = re.compile(r\"\\s+\")\n",
    "    df['question_text'] = df[\"question_text\"].apply(lambda x: _clean_space(x, compiled_re))\n",
    "    return df\n",
    "\n",
    "def _clean_space(x, compiled_re):\n",
    "    return compiled_re.sub(\" \", x)\n",
    "        \n",
    "def prepare_tokenizer(texts, max_words):\n",
    "    tokenizer = Tokenizer(num_words=max_words, filters='', oov_token='<unk>')\n",
    "    tokenizer.fit_on_texts(list(texts))\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_and_padding(texts, tokenizer, max_length):\n",
    "    texts = tokenizer.texts_to_sequences(texts)\n",
    "    texts = pad_sequences(texts, maxlen=max_length)\n",
    "    return texts\n",
    "\n",
    "def get_all_vocabs(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config, tokenizer, all_vocabs, embedding_weights = None):\n",
    "        super(Embeddings, self).__init__()\n",
    "        \n",
    "        self.embedding_map = {\n",
    "            'fasttext': self._load_fasttext,\n",
    "            'glove': self._load_glove,\n",
    "            'paragram': self._load_paragram\n",
    "        }\n",
    "        self.c = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.all_vocabs = all_vocabs\n",
    "        \n",
    "        if embedding_weights is None:\n",
    "            embedding_weights = self._load_embeddings(self.c.embeddings)\n",
    "            \n",
    "        self.original_embedding_weights = embedding_weights\n",
    "        self.embeddings = nn.Embedding(self.c.vocab_size + 1, self.c.embedding_size, padding_idx=0)\n",
    "        self.embeddings.weight = nn.Parameter(embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.embedding_dropout = nn.Dropout2d(self.c.embedding_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.embeddings(x)\n",
    "        if self.training:\n",
    "            embedding += torch.randn_like(embedding) * self.c.embedding_noise_var\n",
    "        return embedding\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        self.embeddings.weight = nn.Parameter(self.original_embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "    \n",
    "    def _load_embeddings(self, embedding_list: list):\n",
    "        embedding_weights = np.zeros((self.c.vocab_size, self.c.embedding_size))\n",
    "        pool = Pool(num_cores)\n",
    "        embedding_weights = np.mean(pool.map(self._load_an_embedding, embedding_list), 0)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return torch.tensor(embedding_weights, dtype=torch.float32)\n",
    "\n",
    "    def _load_an_embedding(self, emb):\n",
    "        return self.embedding_map[emb](self.tokenizer.word_index)\n",
    "        \n",
    "    def _get_embeddings_pair(self, word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "        \n",
    "    def _make_embeddings(self, embeddings_index, word_index, emb_mean, emb_std):\n",
    "        nb_words = min(self.c.vocab_size, len(word_index))\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, self.c.embedding_size))\n",
    "        embedding_matrix[0] = np.zeros(self.c.embedding_size)\n",
    "        for word, i in word_index.items():\n",
    "            if i >= self.c.vocab_size:\n",
    "                continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return embedding_matrix\n",
    "    \n",
    "    def _load_glove(self, word_index):\n",
    "        print('loading glove')\n",
    "        filepath = self.c.datadir / 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "    \n",
    "    def _load_fasttext(self, word_index):    \n",
    "        print('loading fasttext')\n",
    "        filepath = self.c.datadir / 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0033469985, 0.109855495\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "\n",
    "    def _load_paragram(self, word_index):\n",
    "        print('loading paragram')\n",
    "        filepath = self.c.datadir / 'embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath, encoding=\"utf8\", errors='ignore')\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0053247833, 0.49346462\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 2\n",
    "def df_parallelize_run(df, func, num_cores=2):\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1175509, 3)\n",
      "Test shape :  (130613, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_data(c.datadir)\n",
    "train_df = df_parallelize_run(train_df, clean)\n",
    "test_df = df_parallelize_run(test_df, clean)\n",
    "train_x, train_y = train_df['question_text'].values, train_df['target'].values\n",
    "test_x = test_df['question_text'].values\n",
    "tokenizer = prepare_tokenizer(train_x, c.vocab_size)\n",
    "train_x = tokenize_and_padding(train_x, tokenizer, c.max_length)\n",
    "test_x = tokenize_and_padding(test_x, tokenizer, c.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_vocabs:  184279\n",
      "loading glove\n",
      "loading paragram\n",
      "loading fasttext\n",
      "48.40730881690979\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_vocabs = get_all_vocabs(train_df['question_text'])\n",
    "print('all_vocabs: ', len(all_vocabs))\n",
    "embeddings = Embeddings(c, tokenizer, all_vocabs)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(GRULayer, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          bias=False,\n",
    "                          bidirectional=True,\n",
    "                          batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_outputs, gru_state = self.gru(x)\n",
    "        return self.dropout(gru_outputs), gru_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=False,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_outputs, (lstm_states, _) = self.lstm(x)\n",
    "        return self.dropout(lstm_outputs), lstm_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, config: Config, embeddings):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.c = config\n",
    "        \n",
    "        self.embedding = embeddings\n",
    "        self.lstm1 = LSTMLayer(input_size=self.c.embedding_size,\n",
    "                              hidden_size=self.c.hidden_size,\n",
    "                              num_layers=self.c.num_layers,\n",
    "                              dropout_rate=self.c.layer_dropout)\n",
    "        self.lstm2 = LSTMLayer(input_size=self.c.hidden_size*2,\n",
    "                            hidden_size=self.c.hidden_size,\n",
    "                            num_layers=self.c.num_layers,\n",
    "                            dropout_rate=self.c.layer_dropout)\n",
    "        \n",
    "        self.cell_dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.linear = nn.Linear(self.c.dense_size[0], self.c.dense_size[1])\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(self.c.dense_size[1])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.out = nn.Linear(self.c.dense_size[1], self.c.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        o_lstm1, h_lstm1 = self.lstm1(h_embedding)\n",
    "        o_lstm2, h_lstm2 = self.lstm2(o_lstm1)\n",
    "        \n",
    "        avg_pool = torch.mean(o_lstm2, 1)\n",
    "        max_pool, _ = torch.max(o_lstm2, 1)\n",
    "        \n",
    "        h_lstm1 = self.cell_dropout(torch.cat(h_lstm1.split(1, 0), -1).squeeze(0))\n",
    "        h_lstm2 = self.cell_dropout(torch.cat(h_lstm2.split(1, 0), -1).squeeze(0))\n",
    "\n",
    "        concat = torch.cat([h_lstm1, h_lstm2, avg_pool, max_pool], 1)\n",
    "        concat = self.linear(concat)\n",
    "        concat = self.batch_norm(concat)\n",
    "        concat = self.relu(concat)\n",
    "        concat = self.dropout(concat)\n",
    "        out = self.out(concat)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_length(data, mask):\n",
    "    max_length = data.shape[1]\n",
    "    transposed = torch.transpose(data, 1, 0)\n",
    "    res = (transposed == mask).all(1)\n",
    "    for i, r in enumerate(res):\n",
    "        if r == 0:\n",
    "            break\n",
    "    data = data[:, -(max_length - i):]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_x, train_y, test_x, c, embeddings, trial=0):\n",
    "    splits = list(StratifiedKFold(n_splits=c.num_cv_splits, shuffle=True).split(train_x, train_y))\n",
    "    x_test_cuda = torch.tensor(test_x, dtype=torch.long).cuda(cuda_idx)\n",
    "    test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=c.test_batch_size, shuffle=False)\n",
    "    train_preds = np.zeros((len(train_x)))\n",
    "    test_preds = np.zeros((len(test_x)))\n",
    "\n",
    "    mask = torch.zeros((c.max_length, 1), dtype=torch.long).cuda(cuda_idx)\n",
    "    \n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        x_train_fold = torch.tensor(train_x[train_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "        x_val_fold = torch.tensor(train_x[valid_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "\n",
    "        model = SimpleRNN(c, embeddings)\n",
    "        model.cuda(cuda_idx)\n",
    "\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=c.learning_rate)\n",
    "\n",
    "        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "        valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=c.batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=c.test_batch_size, shuffle=False)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        print(f'Fold {i + 1}')\n",
    "\n",
    "        for epoch in range(c.num_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred = model(x_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), c.clip_grad)\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "            avg_val_loss = 0.\n",
    "\n",
    "            # validation prediction\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred = model(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            search_result = threshold_search(y_val_fold.cpu().numpy(), valid_preds_fold)\n",
    "            valid_pred_targets = valid_preds_fold > search_result['threshold']\n",
    "            val_f1 = f1_score(y_val_fold.cpu().numpy(), valid_pred_targets)\n",
    "\n",
    "            elapsed_time = time.time() - start_time \n",
    "            print('Epoch {}/{}  loss={:.4f}  val_loss={:.4f}  f1={:.3f}  time={:.2f}s'.format(\n",
    "                epoch + 1, c.num_epochs, avg_loss, avg_val_loss, val_f1, elapsed_time))\n",
    "            if best_f1 < val_f1:\n",
    "                print(f'model_saved at f1: {val_f1} from {best_f1}')\n",
    "                ckpt_path = Path(f'./ckpt/gauss/{trial}/')\n",
    "                if not ckpt_path.exists():\n",
    "                    ckpt_path.mkdir(parents=True)\n",
    "                torch.save(model.state_dict(),ckpt_path / f'{i}_model.pt')\n",
    "                best_f1 = val_f1\n",
    "                best_epoch = epoch\n",
    "\n",
    "        # test prediction\n",
    "        model.load_state_dict(torch.load(f'./ckpt/gauss/{trial}/{i}_model.pt'))  # load best model\n",
    "        test_preds_fold = np.zeros(len(test_x))\n",
    "        for i, (x_batch, ) in enumerate(test_loader):\n",
    "            x_batch = cut_length(x_batch, mask)\n",
    "            y_pred = model(x_batch).detach()\n",
    "            test_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "        train_preds[valid_idx] = valid_preds_fold\n",
    "        test_preds += test_preds_fold / len(splits)\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1575  val_loss=0.1140  f1=0.649  time=125.51s\n",
      "model_saved at f1: 0.6494195900354962 from 0.0\n",
      "Epoch 2/15  loss=0.1057  val_loss=0.1031  f1=0.669  time=125.49s\n",
      "model_saved at f1: 0.6690724912947136 from 0.6494195900354962\n",
      "Epoch 3/15  loss=0.0994  val_loss=0.0990  f1=0.678  time=125.11s\n",
      "model_saved at f1: 0.6784468132782739 from 0.6690724912947136\n",
      "Epoch 4/15  loss=0.0946  val_loss=0.0985  f1=0.682  time=125.16s\n",
      "model_saved at f1: 0.682486799094795 from 0.6784468132782739\n",
      "Epoch 5/15  loss=0.0903  val_loss=0.0970  f1=0.683  time=125.91s\n",
      "model_saved at f1: 0.6834326053386278 from 0.682486799094795\n",
      "Epoch 6/15  loss=0.0862  val_loss=0.0971  f1=0.688  time=125.54s\n",
      "model_saved at f1: 0.6876944744685729 from 0.6834326053386278\n",
      "Epoch 7/15  loss=0.0818  val_loss=0.1002  f1=0.675  time=126.05s\n",
      "Epoch 8/15  loss=0.0776  val_loss=0.1029  f1=0.682  time=126.32s\n",
      "Epoch 9/15  loss=0.0734  val_loss=0.1057  f1=0.678  time=125.64s\n",
      "Epoch 10/15  loss=0.0692  val_loss=0.1058  f1=0.676  time=125.71s\n",
      "Epoch 11/15  loss=0.0650  val_loss=0.1156  f1=0.672  time=125.82s\n",
      "Epoch 12/15  loss=0.0610  val_loss=0.1198  f1=0.667  time=125.48s\n",
      "Epoch 13/15  loss=0.0573  val_loss=0.1272  f1=0.665  time=125.49s\n",
      "Epoch 14/15  loss=0.0542  val_loss=0.1335  f1=0.659  time=124.99s\n",
      "Epoch 15/15  loss=0.0509  val_loss=0.1357  f1=0.663  time=126.20s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1476  val_loss=0.1082  f1=0.654  time=125.94s\n",
      "model_saved at f1: 0.6537875904073087 from 0.0\n",
      "Epoch 2/15  loss=0.1047  val_loss=0.1031  f1=0.669  time=126.31s\n",
      "model_saved at f1: 0.668600573796621 from 0.6537875904073087\n",
      "Epoch 3/15  loss=0.0983  val_loss=0.1001  f1=0.676  time=125.81s\n",
      "model_saved at f1: 0.6758531457221413 from 0.668600573796621\n",
      "Epoch 4/15  loss=0.0935  val_loss=0.0996  f1=0.682  time=125.66s\n",
      "model_saved at f1: 0.6815189241473992 from 0.6758531457221413\n",
      "Epoch 5/15  loss=0.0891  val_loss=0.1009  f1=0.683  time=125.72s\n",
      "model_saved at f1: 0.6834590604461424 from 0.6815189241473992\n",
      "Epoch 6/15  loss=0.0849  val_loss=0.0992  f1=0.684  time=126.17s\n",
      "model_saved at f1: 0.6840455475946776 from 0.6834590604461424\n",
      "Epoch 7/15  loss=0.0805  val_loss=0.1009  f1=0.681  time=125.16s\n",
      "Epoch 8/15  loss=0.0759  val_loss=0.1042  f1=0.678  time=126.60s\n",
      "Epoch 9/15  loss=0.0718  val_loss=0.1057  f1=0.672  time=126.91s\n",
      "Epoch 10/15  loss=0.0675  val_loss=0.1127  f1=0.671  time=125.58s\n",
      "Epoch 11/15  loss=0.0637  val_loss=0.1177  f1=0.666  time=126.21s\n",
      "Epoch 12/15  loss=0.0596  val_loss=0.1238  f1=0.663  time=126.47s\n",
      "Epoch 13/15  loss=0.0562  val_loss=0.1251  f1=0.667  time=125.68s\n",
      "Epoch 14/15  loss=0.0531  val_loss=0.1375  f1=0.656  time=125.70s\n",
      "Epoch 15/15  loss=0.0499  val_loss=0.1497  f1=0.653  time=126.44s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1556  val_loss=0.1084  f1=0.650  time=125.67s\n",
      "model_saved at f1: 0.649559795720149 from 0.0\n",
      "Epoch 2/15  loss=0.1059  val_loss=0.1022  f1=0.669  time=126.10s\n",
      "model_saved at f1: 0.6688330679491763 from 0.649559795720149\n",
      "Epoch 3/15  loss=0.0996  val_loss=0.1010  f1=0.677  time=125.31s\n",
      "model_saved at f1: 0.6770721555620589 from 0.6688330679491763\n",
      "Epoch 4/15  loss=0.0949  val_loss=0.0985  f1=0.683  time=125.29s\n",
      "model_saved at f1: 0.6834458373513339 from 0.6770721555620589\n",
      "Epoch 5/15  loss=0.0906  val_loss=0.0983  f1=0.686  time=125.45s\n",
      "model_saved at f1: 0.686347580932629 from 0.6834458373513339\n",
      "Epoch 6/15  loss=0.0863  val_loss=0.0987  f1=0.687  time=125.91s\n",
      "model_saved at f1: 0.6867880816011327 from 0.686347580932629\n",
      "Epoch 7/15  loss=0.0821  val_loss=0.0994  f1=0.683  time=125.79s\n",
      "Epoch 8/15  loss=0.0777  val_loss=0.1044  f1=0.674  time=125.79s\n",
      "Epoch 9/15  loss=0.0736  val_loss=0.1046  f1=0.678  time=125.43s\n",
      "Epoch 10/15  loss=0.0693  val_loss=0.1180  f1=0.673  time=125.45s\n",
      "Epoch 11/15  loss=0.0655  val_loss=0.1160  f1=0.671  time=125.82s\n",
      "Epoch 12/15  loss=0.0615  val_loss=0.1251  f1=0.669  time=125.57s\n",
      "Epoch 13/15  loss=0.0578  val_loss=0.1277  f1=0.666  time=125.82s\n",
      "Epoch 14/15  loss=0.0543  val_loss=0.1350  f1=0.663  time=125.85s\n",
      "Epoch 15/15  loss=0.0512  val_loss=0.1411  f1=0.655  time=126.09s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1347  val_loss=0.1043  f1=0.657  time=124.92s\n",
      "model_saved at f1: 0.6574139367908198 from 0.0\n",
      "Epoch 2/15  loss=0.1030  val_loss=0.1031  f1=0.675  time=125.54s\n",
      "model_saved at f1: 0.6753140188095689 from 0.6574139367908198\n",
      "Epoch 3/15  loss=0.0974  val_loss=0.0983  f1=0.680  time=126.04s\n",
      "model_saved at f1: 0.6800945626477543 from 0.6753140188095689\n",
      "Epoch 4/15  loss=0.0928  val_loss=0.0987  f1=0.683  time=125.60s\n",
      "model_saved at f1: 0.6829456864892726 from 0.6800945626477543\n",
      "Epoch 5/15  loss=0.0884  val_loss=0.0973  f1=0.684  time=126.25s\n",
      "model_saved at f1: 0.6844832643483362 from 0.6829456864892726\n",
      "Epoch 6/15  loss=0.0841  val_loss=0.1008  f1=0.686  time=125.48s\n",
      "model_saved at f1: 0.6860486949990612 from 0.6844832643483362\n",
      "Epoch 7/15  loss=0.0797  val_loss=0.1013  f1=0.685  time=125.95s\n",
      "Epoch 8/15  loss=0.0754  val_loss=0.1085  f1=0.677  time=125.96s\n",
      "Epoch 9/15  loss=0.0713  val_loss=0.1086  f1=0.677  time=125.79s\n",
      "Epoch 10/15  loss=0.0670  val_loss=0.1138  f1=0.673  time=125.42s\n",
      "Epoch 11/15  loss=0.0633  val_loss=0.1150  f1=0.674  time=126.28s\n",
      "Epoch 12/15  loss=0.0595  val_loss=0.1228  f1=0.670  time=125.85s\n",
      "Epoch 13/15  loss=0.0562  val_loss=0.1321  f1=0.666  time=126.06s\n",
      "Epoch 14/15  loss=0.0529  val_loss=0.1363  f1=0.663  time=125.93s\n",
      "Epoch 15/15  loss=0.0500  val_loss=0.1442  f1=0.662  time=126.71s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1559  val_loss=0.1076  f1=0.654  time=125.52s\n",
      "model_saved at f1: 0.6538079264504908 from 0.0\n",
      "Epoch 2/15  loss=0.1056  val_loss=0.1020  f1=0.670  time=125.91s\n",
      "model_saved at f1: 0.6701187168476889 from 0.6538079264504908\n",
      "Epoch 3/15  loss=0.0990  val_loss=0.0992  f1=0.680  time=125.93s\n",
      "model_saved at f1: 0.6798780487804879 from 0.6701187168476889\n",
      "Epoch 4/15  loss=0.0939  val_loss=0.0973  f1=0.685  time=125.41s\n",
      "model_saved at f1: 0.6846995401934358 from 0.6798780487804879\n",
      "Epoch 5/15  loss=0.0897  val_loss=0.0972  f1=0.686  time=125.26s\n",
      "model_saved at f1: 0.6856994717910495 from 0.6846995401934358\n",
      "Epoch 6/15  loss=0.0852  val_loss=0.1009  f1=0.682  time=125.36s\n",
      "Epoch 7/15  loss=0.0812  val_loss=0.1022  f1=0.681  time=125.60s\n",
      "Epoch 8/15  loss=0.0770  val_loss=0.1029  f1=0.679  time=126.00s\n",
      "Epoch 9/15  loss=0.0728  val_loss=0.1045  f1=0.678  time=126.18s\n",
      "Epoch 10/15  loss=0.0687  val_loss=0.1096  f1=0.673  time=125.51s\n",
      "Epoch 11/15  loss=0.0647  val_loss=0.1151  f1=0.673  time=125.72s\n",
      "Epoch 12/15  loss=0.0609  val_loss=0.1190  f1=0.668  time=125.50s\n",
      "Epoch 13/15  loss=0.0576  val_loss=0.1259  f1=0.663  time=125.22s\n",
      "Epoch 14/15  loss=0.0541  val_loss=0.1362  f1=0.660  time=125.66s\n",
      "Epoch 15/15  loss=0.0515  val_loss=0.1372  f1=0.659  time=125.80s\n",
      "{'threshold': 0.296668142080307, 'f1': 0.6573940867949604}\n",
      "f1 score: 0.6973573942678362\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1443  val_loss=0.1090  f1=0.649  time=125.32s\n",
      "model_saved at f1: 0.6493894683286695 from 0.0\n",
      "Epoch 2/15  loss=0.1044  val_loss=0.1064  f1=0.664  time=126.02s\n",
      "model_saved at f1: 0.6638476476852722 from 0.6493894683286695\n",
      "Epoch 3/15  loss=0.0983  val_loss=0.1004  f1=0.672  time=126.11s\n",
      "model_saved at f1: 0.672359912125994 from 0.6638476476852722\n",
      "Epoch 4/15  loss=0.0935  val_loss=0.0983  f1=0.681  time=125.77s\n",
      "model_saved at f1: 0.6808872644779128 from 0.672359912125994\n",
      "Epoch 5/15  loss=0.0890  val_loss=0.0982  f1=0.680  time=125.85s\n",
      "Epoch 6/15  loss=0.0847  val_loss=0.1019  f1=0.681  time=126.11s\n",
      "Epoch 7/15  loss=0.0802  val_loss=0.1016  f1=0.682  time=125.82s\n",
      "model_saved at f1: 0.6815918820358332 from 0.6808872644779128\n",
      "Epoch 8/15  loss=0.0759  val_loss=0.1037  f1=0.678  time=125.87s\n",
      "Epoch 9/15  loss=0.0714  val_loss=0.1089  f1=0.675  time=126.27s\n",
      "Epoch 10/15  loss=0.0671  val_loss=0.1130  f1=0.670  time=125.81s\n",
      "Epoch 11/15  loss=0.0630  val_loss=0.1185  f1=0.666  time=126.14s\n",
      "Epoch 12/15  loss=0.0592  val_loss=0.1221  f1=0.667  time=126.14s\n",
      "Epoch 13/15  loss=0.0559  val_loss=0.1292  f1=0.659  time=125.55s\n",
      "Epoch 14/15  loss=0.0523  val_loss=0.1323  f1=0.659  time=126.16s\n",
      "Epoch 15/15  loss=0.0493  val_loss=0.1501  f1=0.652  time=126.12s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1427  val_loss=0.1108  f1=0.643  time=125.76s\n",
      "model_saved at f1: 0.6433803183697386 from 0.0\n",
      "Epoch 2/15  loss=0.1048  val_loss=0.1077  f1=0.669  time=125.96s\n",
      "model_saved at f1: 0.6692198764160658 from 0.6433803183697386\n",
      "Epoch 3/15  loss=0.0986  val_loss=0.1003  f1=0.679  time=125.50s\n",
      "model_saved at f1: 0.6793113324634822 from 0.6692198764160658\n",
      "Epoch 4/15  loss=0.0939  val_loss=0.0988  f1=0.682  time=125.37s\n",
      "model_saved at f1: 0.6816713964184362 from 0.6793113324634822\n",
      "Epoch 5/15  loss=0.0897  val_loss=0.0994  f1=0.681  time=126.94s\n",
      "Epoch 6/15  loss=0.0855  val_loss=0.1004  f1=0.686  time=125.72s\n",
      "model_saved at f1: 0.6858343179729552 from 0.6816713964184362\n",
      "Epoch 7/15  loss=0.0809  val_loss=0.1001  f1=0.684  time=124.80s\n",
      "Epoch 8/15  loss=0.0767  val_loss=0.1026  f1=0.680  time=125.73s\n",
      "Epoch 9/15  loss=0.0723  val_loss=0.1063  f1=0.680  time=125.34s\n",
      "Epoch 10/15  loss=0.0682  val_loss=0.1116  f1=0.673  time=125.43s\n",
      "Epoch 11/15  loss=0.0640  val_loss=0.1191  f1=0.672  time=126.03s\n",
      "Epoch 12/15  loss=0.0602  val_loss=0.1194  f1=0.671  time=125.29s\n",
      "Epoch 13/15  loss=0.0568  val_loss=0.1297  f1=0.665  time=125.56s\n",
      "Epoch 14/15  loss=0.0535  val_loss=0.1330  f1=0.662  time=125.69s\n",
      "Epoch 15/15  loss=0.0505  val_loss=0.1414  f1=0.659  time=125.96s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1552  val_loss=0.1098  f1=0.652  time=125.74s\n",
      "model_saved at f1: 0.6524159999999999 from 0.0\n",
      "Epoch 2/15  loss=0.1055  val_loss=0.1024  f1=0.672  time=125.75s\n",
      "model_saved at f1: 0.6719789223403914 from 0.6524159999999999\n",
      "Epoch 3/15  loss=0.0992  val_loss=0.1002  f1=0.681  time=126.03s\n",
      "model_saved at f1: 0.680675811525116 from 0.6719789223403914\n",
      "Epoch 4/15  loss=0.0946  val_loss=0.0969  f1=0.686  time=125.98s\n",
      "model_saved at f1: 0.6856992327702591 from 0.680675811525116\n",
      "Epoch 5/15  loss=0.0902  val_loss=0.0965  f1=0.690  time=125.93s\n",
      "model_saved at f1: 0.689793450240285 from 0.6856992327702591\n",
      "Epoch 6/15  loss=0.0858  val_loss=0.1005  f1=0.685  time=126.22s\n",
      "Epoch 7/15  loss=0.0814  val_loss=0.0994  f1=0.688  time=125.96s\n",
      "Epoch 8/15  loss=0.0770  val_loss=0.1027  f1=0.683  time=126.40s\n",
      "Epoch 9/15  loss=0.0725  val_loss=0.1057  f1=0.682  time=126.40s\n",
      "Epoch 10/15  loss=0.0682  val_loss=0.1095  f1=0.675  time=126.02s\n",
      "Epoch 11/15  loss=0.0642  val_loss=0.1139  f1=0.674  time=126.25s\n",
      "Epoch 12/15  loss=0.0606  val_loss=0.1193  f1=0.668  time=126.14s\n",
      "Epoch 13/15  loss=0.0570  val_loss=0.1234  f1=0.668  time=126.14s\n",
      "Epoch 14/15  loss=0.0539  val_loss=0.1350  f1=0.668  time=126.57s\n",
      "Epoch 15/15  loss=0.0508  val_loss=0.1349  f1=0.667  time=126.34s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1707  val_loss=0.1139  f1=0.638  time=125.98s\n",
      "model_saved at f1: 0.6384690318323225 from 0.0\n",
      "Epoch 2/15  loss=0.1071  val_loss=0.1042  f1=0.662  time=125.99s\n",
      "model_saved at f1: 0.6623638048375244 from 0.6384690318323225\n",
      "Epoch 3/15  loss=0.1003  val_loss=0.1002  f1=0.677  time=124.82s\n",
      "model_saved at f1: 0.6774352085449362 from 0.6623638048375244\n",
      "Epoch 4/15  loss=0.0955  val_loss=0.0992  f1=0.680  time=125.45s\n",
      "model_saved at f1: 0.6801520324679508 from 0.6774352085449362\n",
      "Epoch 5/15  loss=0.0912  val_loss=0.1008  f1=0.679  time=125.93s\n",
      "Epoch 6/15  loss=0.0870  val_loss=0.1033  f1=0.681  time=126.02s\n",
      "model_saved at f1: 0.680939226519337 from 0.6801520324679508\n",
      "Epoch 7/15  loss=0.0827  val_loss=0.1023  f1=0.683  time=125.80s\n",
      "model_saved at f1: 0.6825026778400857 from 0.680939226519337\n",
      "Epoch 8/15  loss=0.0784  val_loss=0.1080  f1=0.679  time=126.27s\n",
      "Epoch 9/15  loss=0.0740  val_loss=0.1119  f1=0.676  time=125.91s\n",
      "Epoch 10/15  loss=0.0697  val_loss=0.1119  f1=0.673  time=125.97s\n",
      "Epoch 11/15  loss=0.0656  val_loss=0.1254  f1=0.664  time=126.04s\n",
      "Epoch 12/15  loss=0.0617  val_loss=0.1249  f1=0.664  time=125.62s\n",
      "Epoch 13/15  loss=0.0581  val_loss=0.1318  f1=0.667  time=125.90s\n",
      "Epoch 14/15  loss=0.0547  val_loss=0.1447  f1=0.659  time=126.54s\n",
      "Epoch 15/15  loss=0.0513  val_loss=0.1465  f1=0.657  time=126.25s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1633  val_loss=0.1105  f1=0.641  time=125.96s\n",
      "model_saved at f1: 0.6405631036176134 from 0.0\n",
      "Epoch 2/15  loss=0.1066  val_loss=0.1041  f1=0.663  time=125.58s\n",
      "model_saved at f1: 0.6625494853523356 from 0.6405631036176134\n",
      "Epoch 3/15  loss=0.1001  val_loss=0.0999  f1=0.678  time=126.08s\n",
      "model_saved at f1: 0.6783558493820063 from 0.6625494853523356\n",
      "Epoch 4/15  loss=0.0953  val_loss=0.0992  f1=0.681  time=126.11s\n",
      "model_saved at f1: 0.6805036395829234 from 0.6783558493820063\n",
      "Epoch 5/15  loss=0.0910  val_loss=0.0985  f1=0.682  time=126.31s\n",
      "model_saved at f1: 0.6815557388490578 from 0.6805036395829234\n",
      "Epoch 6/15  loss=0.0868  val_loss=0.1026  f1=0.680  time=126.15s\n",
      "Epoch 7/15  loss=0.0824  val_loss=0.1024  f1=0.684  time=126.14s\n",
      "model_saved at f1: 0.6841919661462508 from 0.6815557388490578\n",
      "Epoch 8/15  loss=0.0781  val_loss=0.1032  f1=0.683  time=125.84s\n",
      "Epoch 9/15  loss=0.0737  val_loss=0.1106  f1=0.679  time=126.29s\n",
      "Epoch 10/15  loss=0.0693  val_loss=0.1081  f1=0.677  time=126.06s\n",
      "Epoch 11/15  loss=0.0650  val_loss=0.1162  f1=0.672  time=126.11s\n",
      "Epoch 12/15  loss=0.0609  val_loss=0.1249  f1=0.668  time=126.01s\n",
      "Epoch 13/15  loss=0.0572  val_loss=0.1276  f1=0.664  time=125.65s\n",
      "Epoch 14/15  loss=0.0538  val_loss=0.1347  f1=0.663  time=125.96s\n",
      "Epoch 15/15  loss=0.0502  val_loss=0.1397  f1=0.662  time=126.17s\n",
      "{'threshold': 0.2642848491668701, 'f1': 0.6582050347987807}\n",
      "f1 score: 0.6999382473474429\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1456  val_loss=0.1133  f1=0.650  time=125.55s\n",
      "model_saved at f1: 0.6498026315789474 from 0.0\n",
      "Epoch 2/15  loss=0.1054  val_loss=0.1020  f1=0.672  time=125.89s\n",
      "model_saved at f1: 0.671716051338932 from 0.6498026315789474\n",
      "Epoch 3/15  loss=0.0992  val_loss=0.1006  f1=0.679  time=125.84s\n",
      "model_saved at f1: 0.6792659017131103 from 0.671716051338932\n",
      "Epoch 4/15  loss=0.0945  val_loss=0.0990  f1=0.681  time=125.95s\n",
      "model_saved at f1: 0.6805019305019305 from 0.6792659017131103\n",
      "Epoch 5/15  loss=0.0900  val_loss=0.0982  f1=0.684  time=125.82s\n",
      "model_saved at f1: 0.6836442350544343 from 0.6805019305019305\n",
      "Epoch 6/15  loss=0.0856  val_loss=0.1009  f1=0.685  time=125.40s\n",
      "model_saved at f1: 0.6846183401639344 from 0.6836442350544343\n",
      "Epoch 7/15  loss=0.0812  val_loss=0.0994  f1=0.686  time=125.80s\n",
      "model_saved at f1: 0.6862223972175738 from 0.6846183401639344\n",
      "Epoch 8/15  loss=0.0768  val_loss=0.1040  f1=0.683  time=125.67s\n",
      "Epoch 9/15  loss=0.0726  val_loss=0.1052  f1=0.680  time=125.49s\n",
      "Epoch 10/15  loss=0.0682  val_loss=0.1119  f1=0.676  time=125.49s\n",
      "Epoch 11/15  loss=0.0641  val_loss=0.1191  f1=0.670  time=125.90s\n",
      "Epoch 12/15  loss=0.0604  val_loss=0.1270  f1=0.671  time=125.37s\n",
      "Epoch 13/15  loss=0.0570  val_loss=0.1269  f1=0.663  time=125.57s\n",
      "Epoch 14/15  loss=0.0533  val_loss=0.1313  f1=0.663  time=125.81s\n",
      "Epoch 15/15  loss=0.0504  val_loss=0.1386  f1=0.660  time=126.32s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1468  val_loss=0.1065  f1=0.660  time=126.25s\n",
      "model_saved at f1: 0.6596925841542856 from 0.0\n",
      "Epoch 2/15  loss=0.1049  val_loss=0.1020  f1=0.675  time=126.10s\n",
      "model_saved at f1: 0.6748942396386653 from 0.6596925841542856\n",
      "Epoch 3/15  loss=0.0988  val_loss=0.0980  f1=0.683  time=126.45s\n",
      "model_saved at f1: 0.6831043869089067 from 0.6748942396386653\n",
      "Epoch 4/15  loss=0.0941  val_loss=0.0985  f1=0.685  time=126.05s\n",
      "model_saved at f1: 0.685333420459372 from 0.6831043869089067\n",
      "Epoch 5/15  loss=0.0899  val_loss=0.0963  f1=0.691  time=125.73s\n",
      "model_saved at f1: 0.6910425182657954 from 0.685333420459372\n",
      "Epoch 6/15  loss=0.0854  val_loss=0.0975  f1=0.690  time=126.70s\n",
      "Epoch 7/15  loss=0.0810  val_loss=0.0998  f1=0.687  time=126.72s\n",
      "Epoch 8/15  loss=0.0767  val_loss=0.1028  f1=0.685  time=126.18s\n",
      "Epoch 9/15  loss=0.0723  val_loss=0.1044  f1=0.683  time=126.54s\n",
      "Epoch 10/15  loss=0.0680  val_loss=0.1132  f1=0.675  time=125.95s\n",
      "Epoch 11/15  loss=0.0640  val_loss=0.1135  f1=0.677  time=126.24s\n",
      "Epoch 12/15  loss=0.0602  val_loss=0.1184  f1=0.676  time=126.72s\n",
      "Epoch 13/15  loss=0.0564  val_loss=0.1233  f1=0.671  time=126.29s\n",
      "Epoch 14/15  loss=0.0532  val_loss=0.1331  f1=0.668  time=125.91s\n",
      "Epoch 15/15  loss=0.0500  val_loss=0.1372  f1=0.660  time=126.25s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1477  val_loss=0.1076  f1=0.649  time=125.82s\n",
      "model_saved at f1: 0.6487732482869683 from 0.0\n",
      "Epoch 2/15  loss=0.1052  val_loss=0.1021  f1=0.666  time=125.98s\n",
      "model_saved at f1: 0.6661202874795108 from 0.6487732482869683\n",
      "Epoch 3/15  loss=0.0990  val_loss=0.1000  f1=0.675  time=124.98s\n",
      "model_saved at f1: 0.6749727056707984 from 0.6661202874795108\n",
      "Epoch 4/15  loss=0.0941  val_loss=0.0995  f1=0.679  time=125.59s\n",
      "model_saved at f1: 0.6790330537740503 from 0.6749727056707984\n",
      "Epoch 5/15  loss=0.0898  val_loss=0.1007  f1=0.684  time=125.98s\n",
      "model_saved at f1: 0.6838527470362221 from 0.6790330537740503\n",
      "Epoch 6/15  loss=0.0856  val_loss=0.1024  f1=0.683  time=125.59s\n",
      "Epoch 7/15  loss=0.0812  val_loss=0.1012  f1=0.685  time=125.31s\n",
      "model_saved at f1: 0.6850300968332896 from 0.6838527470362221\n",
      "Epoch 8/15  loss=0.0768  val_loss=0.1057  f1=0.681  time=125.78s\n",
      "Epoch 9/15  loss=0.0723  val_loss=0.1099  f1=0.679  time=125.86s\n",
      "Epoch 10/15  loss=0.0680  val_loss=0.1117  f1=0.679  time=126.42s\n",
      "Epoch 11/15  loss=0.0636  val_loss=0.1246  f1=0.668  time=126.50s\n",
      "Epoch 12/15  loss=0.0596  val_loss=0.1223  f1=0.673  time=125.36s\n",
      "Epoch 13/15  loss=0.0559  val_loss=0.1352  f1=0.664  time=125.61s\n",
      "Epoch 14/15  loss=0.0522  val_loss=0.1384  f1=0.666  time=125.18s\n",
      "Epoch 15/15  loss=0.0492  val_loss=0.1474  f1=0.661  time=125.86s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1769  val_loss=0.1106  f1=0.648  time=126.31s\n",
      "model_saved at f1: 0.6477626303127506 from 0.0\n",
      "Epoch 2/15  loss=0.1072  val_loss=0.1020  f1=0.671  time=126.27s\n",
      "model_saved at f1: 0.6706973768394113 from 0.6477626303127506\n",
      "Epoch 3/15  loss=0.1003  val_loss=0.0994  f1=0.678  time=125.95s\n",
      "model_saved at f1: 0.6778544740675069 from 0.6706973768394113\n",
      "Epoch 4/15  loss=0.0954  val_loss=0.0988  f1=0.680  time=125.47s\n",
      "model_saved at f1: 0.6801735690128262 from 0.6778544740675069\n",
      "Epoch 5/15  loss=0.0912  val_loss=0.0985  f1=0.684  time=126.21s\n",
      "model_saved at f1: 0.6841323168368006 from 0.6801735690128262\n",
      "Epoch 6/15  loss=0.0872  val_loss=0.0995  f1=0.683  time=126.39s\n",
      "Epoch 7/15  loss=0.0830  val_loss=0.1030  f1=0.681  time=126.00s\n",
      "Epoch 8/15  loss=0.0786  val_loss=0.1008  f1=0.683  time=126.04s\n",
      "Epoch 9/15  loss=0.0745  val_loss=0.1067  f1=0.678  time=126.21s\n",
      "Epoch 10/15  loss=0.0705  val_loss=0.1082  f1=0.675  time=125.96s\n",
      "Epoch 11/15  loss=0.0665  val_loss=0.1100  f1=0.674  time=126.58s\n",
      "Epoch 12/15  loss=0.0626  val_loss=0.1207  f1=0.671  time=126.31s\n",
      "Epoch 13/15  loss=0.0590  val_loss=0.1315  f1=0.662  time=126.50s\n",
      "Epoch 14/15  loss=0.0557  val_loss=0.1334  f1=0.665  time=126.53s\n",
      "Epoch 15/15  loss=0.0525  val_loss=0.1417  f1=0.661  time=125.88s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1370  val_loss=0.1100  f1=0.645  time=125.63s\n",
      "model_saved at f1: 0.6447152418234626 from 0.0\n",
      "Epoch 2/15  loss=0.1045  val_loss=0.1015  f1=0.672  time=126.05s\n",
      "model_saved at f1: 0.6722356842239511 from 0.6447152418234626\n",
      "Epoch 3/15  loss=0.0984  val_loss=0.1015  f1=0.674  time=125.31s\n",
      "model_saved at f1: 0.6739969135802468 from 0.6722356842239511\n",
      "Epoch 4/15  loss=0.0936  val_loss=0.0991  f1=0.680  time=125.04s\n",
      "model_saved at f1: 0.6797193669440366 from 0.6739969135802468\n",
      "Epoch 5/15  loss=0.0895  val_loss=0.0985  f1=0.682  time=125.94s\n",
      "model_saved at f1: 0.6820282539578484 from 0.6797193669440366\n",
      "Epoch 6/15  loss=0.0849  val_loss=0.1004  f1=0.678  time=124.89s\n",
      "Epoch 7/15  loss=0.0806  val_loss=0.1034  f1=0.679  time=125.18s\n",
      "Epoch 8/15  loss=0.0763  val_loss=0.1062  f1=0.678  time=125.27s\n",
      "Epoch 9/15  loss=0.0719  val_loss=0.1098  f1=0.669  time=125.19s\n",
      "Epoch 10/15  loss=0.0677  val_loss=0.1136  f1=0.672  time=126.06s\n",
      "Epoch 11/15  loss=0.0638  val_loss=0.1196  f1=0.667  time=125.83s\n",
      "Epoch 12/15  loss=0.0600  val_loss=0.1269  f1=0.664  time=125.41s\n",
      "Epoch 13/15  loss=0.0563  val_loss=0.1383  f1=0.664  time=126.08s\n",
      "Epoch 14/15  loss=0.0533  val_loss=0.1413  f1=0.659  time=125.13s\n",
      "Epoch 15/15  loss=0.0503  val_loss=0.1578  f1=0.651  time=125.33s\n",
      "{'threshold': 0.23971335589885712, 'f1': 0.6576959606325814}\n",
      "f1 score: 0.6960919034793602\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1586  val_loss=0.1084  f1=0.650  time=63.85s\n",
      "model_saved at f1: 0.6497954044696255 from 0.0\n",
      "Epoch 2/15  loss=0.1059  val_loss=0.1030  f1=0.665  time=64.85s\n",
      "model_saved at f1: 0.664816763946034 from 0.6497954044696255\n",
      "Epoch 3/15  loss=0.1000  val_loss=0.1025  f1=0.672  time=64.82s\n",
      "model_saved at f1: 0.672419344452315 from 0.664816763946034\n",
      "Epoch 4/15  loss=0.0951  val_loss=0.0983  f1=0.679  time=65.57s\n",
      "model_saved at f1: 0.679297451849059 from 0.672419344452315\n",
      "Epoch 5/15  loss=0.0910  val_loss=0.0995  f1=0.681  time=66.73s\n",
      "model_saved at f1: 0.6810161192890981 from 0.679297451849059\n",
      "Epoch 6/15  loss=0.0871  val_loss=0.1005  f1=0.683  time=66.24s\n",
      "model_saved at f1: 0.6830136188658493 from 0.6810161192890981\n",
      "Epoch 7/15  loss=0.0831  val_loss=0.1010  f1=0.680  time=66.48s\n",
      "Epoch 8/15  loss=0.0788  val_loss=0.1068  f1=0.679  time=67.24s\n",
      "Epoch 9/15  loss=0.0743  val_loss=0.1090  f1=0.681  time=66.73s\n",
      "Epoch 10/15  loss=0.0702  val_loss=0.1111  f1=0.678  time=66.69s\n",
      "Epoch 11/15  loss=0.0661  val_loss=0.1176  f1=0.673  time=67.41s\n",
      "Epoch 12/15  loss=0.0624  val_loss=0.1227  f1=0.669  time=66.90s\n",
      "Epoch 13/15  loss=0.0588  val_loss=0.1304  f1=0.671  time=66.87s\n",
      "Epoch 14/15  loss=0.0554  val_loss=0.1435  f1=0.665  time=67.44s\n",
      "Epoch 15/15  loss=0.0521  val_loss=0.1457  f1=0.660  time=66.83s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1577  val_loss=0.1113  f1=0.642  time=66.45s\n",
      "model_saved at f1: 0.6419916698220371 from 0.0\n",
      "Epoch 2/15  loss=0.1063  val_loss=0.1030  f1=0.665  time=67.05s\n",
      "model_saved at f1: 0.6646165798077234 from 0.6419916698220371\n",
      "Epoch 3/15  loss=0.1001  val_loss=0.1015  f1=0.677  time=66.71s\n",
      "model_saved at f1: 0.6768770526452704 from 0.6646165798077234\n",
      "Epoch 4/15  loss=0.0954  val_loss=0.0996  f1=0.680  time=66.40s\n",
      "model_saved at f1: 0.679623865969141 from 0.6768770526452704\n",
      "Epoch 5/15  loss=0.0914  val_loss=0.1016  f1=0.683  time=67.25s\n",
      "model_saved at f1: 0.6832860067357854 from 0.679623865969141\n",
      "Epoch 6/15  loss=0.0873  val_loss=0.1027  f1=0.686  time=66.84s\n",
      "model_saved at f1: 0.6856533953308147 from 0.6832860067357854\n",
      "Epoch 7/15  loss=0.0827  val_loss=0.0994  f1=0.686  time=66.54s\n",
      "model_saved at f1: 0.6864371228500435 from 0.6856533953308147\n",
      "Epoch 8/15  loss=0.0787  val_loss=0.1064  f1=0.681  time=67.40s\n",
      "Epoch 9/15  loss=0.0746  val_loss=0.1087  f1=0.679  time=66.90s\n",
      "Epoch 10/15  loss=0.0704  val_loss=0.1141  f1=0.677  time=66.61s\n",
      "Epoch 11/15  loss=0.0661  val_loss=0.1158  f1=0.674  time=67.27s\n",
      "Epoch 12/15  loss=0.0625  val_loss=0.1205  f1=0.672  time=66.71s\n",
      "Epoch 13/15  loss=0.0588  val_loss=0.1269  f1=0.666  time=66.62s\n",
      "Epoch 14/15  loss=0.0555  val_loss=0.1333  f1=0.661  time=67.24s\n",
      "Epoch 15/15  loss=0.0523  val_loss=0.1433  f1=0.664  time=66.90s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1466  val_loss=0.1095  f1=0.649  time=66.80s\n",
      "model_saved at f1: 0.6493895490802539 from 0.0\n",
      "Epoch 2/15  loss=0.1051  val_loss=0.1020  f1=0.672  time=67.48s\n",
      "model_saved at f1: 0.6717911253692469 from 0.6493895490802539\n",
      "Epoch 3/15  loss=0.0986  val_loss=0.1019  f1=0.677  time=67.10s\n",
      "model_saved at f1: 0.6774305665265886 from 0.6717911253692469\n",
      "Epoch 4/15  loss=0.0937  val_loss=0.0980  f1=0.683  time=66.96s\n",
      "model_saved at f1: 0.6829424991969162 from 0.6774305665265886\n",
      "Epoch 5/15  loss=0.0894  val_loss=0.0979  f1=0.685  time=67.54s\n",
      "model_saved at f1: 0.6845524230219566 from 0.6829424991969162\n",
      "Epoch 6/15  loss=0.0851  val_loss=0.0982  f1=0.686  time=67.15s\n",
      "model_saved at f1: 0.6861445977828771 from 0.6845524230219566\n",
      "Epoch 7/15  loss=0.0807  val_loss=0.0998  f1=0.685  time=66.95s\n",
      "Epoch 8/15  loss=0.0764  val_loss=0.1047  f1=0.681  time=67.62s\n",
      "Epoch 9/15  loss=0.0721  val_loss=0.1056  f1=0.683  time=67.16s\n",
      "Epoch 10/15  loss=0.0677  val_loss=0.1095  f1=0.676  time=67.04s\n",
      "Epoch 11/15  loss=0.0640  val_loss=0.1143  f1=0.675  time=67.81s\n",
      "Epoch 12/15  loss=0.0603  val_loss=0.1207  f1=0.672  time=67.12s\n",
      "Epoch 13/15  loss=0.0566  val_loss=0.1271  f1=0.669  time=66.79s\n",
      "Epoch 14/15  loss=0.0534  val_loss=0.1337  f1=0.662  time=67.58s\n",
      "Epoch 15/15  loss=0.0503  val_loss=0.1366  f1=0.662  time=67.54s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1741  val_loss=0.1086  f1=0.655  time=66.85s\n",
      "model_saved at f1: 0.6550221079369062 from 0.0\n",
      "Epoch 2/15  loss=0.1069  val_loss=0.1029  f1=0.672  time=67.60s\n",
      "model_saved at f1: 0.6715182094572743 from 0.6550221079369062\n",
      "Epoch 3/15  loss=0.1006  val_loss=0.0998  f1=0.676  time=67.03s\n",
      "model_saved at f1: 0.6755478883138458 from 0.6715182094572743\n",
      "Epoch 4/15  loss=0.0957  val_loss=0.0984  f1=0.687  time=66.83s\n",
      "model_saved at f1: 0.6870204705357996 from 0.6755478883138458\n",
      "Epoch 5/15  loss=0.0912  val_loss=0.0995  f1=0.687  time=67.63s\n",
      "model_saved at f1: 0.6870492117188995 from 0.6870204705357996\n",
      "Epoch 6/15  loss=0.0871  val_loss=0.0987  f1=0.688  time=67.07s\n",
      "model_saved at f1: 0.687873405457775 from 0.6870492117188995\n",
      "Epoch 7/15  loss=0.0830  val_loss=0.0998  f1=0.685  time=66.74s\n",
      "Epoch 8/15  loss=0.0787  val_loss=0.1038  f1=0.685  time=67.49s\n",
      "Epoch 9/15  loss=0.0748  val_loss=0.1019  f1=0.686  time=67.29s\n",
      "Epoch 10/15  loss=0.0706  val_loss=0.1093  f1=0.684  time=66.79s\n",
      "Epoch 11/15  loss=0.0667  val_loss=0.1103  f1=0.677  time=67.33s\n",
      "Epoch 12/15  loss=0.0628  val_loss=0.1171  f1=0.669  time=67.16s\n",
      "Epoch 13/15  loss=0.0591  val_loss=0.1188  f1=0.673  time=66.83s\n",
      "Epoch 14/15  loss=0.0559  val_loss=0.1315  f1=0.668  time=67.59s\n",
      "Epoch 15/15  loss=0.0527  val_loss=0.1363  f1=0.665  time=67.15s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1460  val_loss=0.1109  f1=0.651  time=67.19s\n",
      "model_saved at f1: 0.6508633304067895 from 0.0\n",
      "Epoch 2/15  loss=0.1043  val_loss=0.1027  f1=0.670  time=67.88s\n",
      "model_saved at f1: 0.6695270662623395 from 0.6508633304067895\n",
      "Epoch 3/15  loss=0.0986  val_loss=0.1005  f1=0.675  time=67.39s\n",
      "model_saved at f1: 0.675208734746307 from 0.6695270662623395\n",
      "Epoch 4/15  loss=0.0937  val_loss=0.0997  f1=0.680  time=67.23s\n",
      "model_saved at f1: 0.6800466140100997 from 0.675208734746307\n",
      "Epoch 5/15  loss=0.0894  val_loss=0.0991  f1=0.685  time=67.86s\n",
      "model_saved at f1: 0.6848562174894495 from 0.6800466140100997\n",
      "Epoch 6/15  loss=0.0852  val_loss=0.1034  f1=0.682  time=67.39s\n",
      "Epoch 7/15  loss=0.0808  val_loss=0.1038  f1=0.681  time=67.15s\n",
      "Epoch 8/15  loss=0.0764  val_loss=0.1041  f1=0.681  time=67.82s\n",
      "Epoch 9/15  loss=0.0722  val_loss=0.1111  f1=0.677  time=67.23s\n",
      "Epoch 10/15  loss=0.0679  val_loss=0.1134  f1=0.671  time=67.28s\n",
      "Epoch 11/15  loss=0.0641  val_loss=0.1167  f1=0.669  time=67.84s\n",
      "Epoch 12/15  loss=0.0601  val_loss=0.1276  f1=0.667  time=67.70s\n",
      "Epoch 13/15  loss=0.0562  val_loss=0.1354  f1=0.657  time=67.41s\n",
      "Epoch 14/15  loss=0.0531  val_loss=0.1357  f1=0.658  time=67.87s\n",
      "Epoch 15/15  loss=0.0500  val_loss=0.1464  f1=0.656  time=67.82s\n",
      "{'threshold': 0.25868991017341614, 'f1': 0.659601910724818}\n",
      "f1 score: 0.6946277611126261\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1346  val_loss=0.1077  f1=0.655  time=66.99s\n",
      "model_saved at f1: 0.6545002616431188 from 0.0\n",
      "Epoch 2/15  loss=0.1046  val_loss=0.1033  f1=0.671  time=67.14s\n",
      "model_saved at f1: 0.6714133143467427 from 0.6545002616431188\n",
      "Epoch 3/15  loss=0.0988  val_loss=0.1002  f1=0.683  time=67.27s\n",
      "model_saved at f1: 0.6833594484487622 from 0.6714133143467427\n",
      "Epoch 4/15  loss=0.0940  val_loss=0.0976  f1=0.682  time=67.01s\n",
      "Epoch 5/15  loss=0.0897  val_loss=0.1040  f1=0.682  time=67.14s\n",
      "Epoch 6/15  loss=0.0854  val_loss=0.0992  f1=0.684  time=67.07s\n",
      "model_saved at f1: 0.683672131147541 from 0.6833594484487622\n",
      "Epoch 7/15  loss=0.0811  val_loss=0.1042  f1=0.681  time=66.92s\n",
      "Epoch 8/15  loss=0.0765  val_loss=0.1064  f1=0.677  time=67.17s\n",
      "Epoch 9/15  loss=0.0724  val_loss=0.1048  f1=0.674  time=67.00s\n",
      "Epoch 10/15  loss=0.0681  val_loss=0.1079  f1=0.674  time=66.90s\n",
      "Epoch 11/15  loss=0.0639  val_loss=0.1184  f1=0.669  time=67.19s\n",
      "Epoch 12/15  loss=0.0602  val_loss=0.1263  f1=0.658  time=67.06s\n",
      "Epoch 13/15  loss=0.0569  val_loss=0.1311  f1=0.662  time=67.07s\n",
      "Epoch 14/15  loss=0.0534  val_loss=0.1297  f1=0.663  time=67.48s\n",
      "Epoch 15/15  loss=0.0504  val_loss=0.1402  f1=0.661  time=67.19s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1500  val_loss=0.1082  f1=0.650  time=67.23s\n",
      "model_saved at f1: 0.6496604165989666 from 0.0\n",
      "Epoch 2/15  loss=0.1075  val_loss=0.1030  f1=0.665  time=67.73s\n",
      "model_saved at f1: 0.6652083061251143 from 0.6496604165989666\n",
      "Epoch 3/15  loss=0.1026  val_loss=0.1011  f1=0.675  time=67.63s\n",
      "model_saved at f1: 0.6750186621661095 from 0.6652083061251143\n",
      "Epoch 4/15  loss=0.0984  val_loss=0.1003  f1=0.682  time=67.18s\n",
      "model_saved at f1: 0.6815117334370544 from 0.6750186621661095\n",
      "Epoch 5/15  loss=0.0949  val_loss=0.0986  f1=0.686  time=67.83s\n",
      "model_saved at f1: 0.6862058617585275 from 0.6815117334370544\n",
      "Epoch 6/15  loss=0.0916  val_loss=0.0977  f1=0.686  time=67.61s\n",
      "model_saved at f1: 0.6864845027191139 from 0.6862058617585275\n",
      "Epoch 7/15  loss=0.0884  val_loss=0.0987  f1=0.692  time=67.19s\n",
      "model_saved at f1: 0.6917965838006029 from 0.6864845027191139\n",
      "Epoch 8/15  loss=0.0846  val_loss=0.0991  f1=0.688  time=67.76s\n",
      "Epoch 9/15  loss=0.0812  val_loss=0.0993  f1=0.690  time=67.59s\n",
      "Epoch 10/15  loss=0.0777  val_loss=0.1014  f1=0.685  time=67.33s\n",
      "Epoch 11/15  loss=0.0736  val_loss=0.1075  f1=0.682  time=67.77s\n",
      "Epoch 12/15  loss=0.0698  val_loss=0.1051  f1=0.680  time=67.72s\n",
      "Epoch 13/15  loss=0.0655  val_loss=0.1155  f1=0.675  time=67.32s\n",
      "Epoch 14/15  loss=0.0616  val_loss=0.1167  f1=0.674  time=67.60s\n",
      "Epoch 15/15  loss=0.0578  val_loss=0.1247  f1=0.667  time=67.60s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1413  val_loss=0.1080  f1=0.655  time=66.77s\n",
      "model_saved at f1: 0.6547695467967646 from 0.0\n",
      "Epoch 2/15  loss=0.1045  val_loss=0.1036  f1=0.667  time=66.96s\n",
      "model_saved at f1: 0.6672624557147874 from 0.6547695467967646\n",
      "Epoch 3/15  loss=0.0985  val_loss=0.1029  f1=0.678  time=67.06s\n",
      "model_saved at f1: 0.6775328405900011 from 0.6672624557147874\n",
      "Epoch 4/15  loss=0.0939  val_loss=0.0992  f1=0.684  time=66.97s\n",
      "model_saved at f1: 0.6843594497024099 from 0.6775328405900011\n",
      "Epoch 5/15  loss=0.0894  val_loss=0.0992  f1=0.683  time=67.19s\n",
      "Epoch 6/15  loss=0.0851  val_loss=0.0993  f1=0.681  time=67.03s\n",
      "Epoch 7/15  loss=0.0808  val_loss=0.1038  f1=0.684  time=67.07s\n",
      "Epoch 8/15  loss=0.0766  val_loss=0.1037  f1=0.683  time=67.17s\n",
      "Epoch 9/15  loss=0.0723  val_loss=0.1091  f1=0.671  time=67.13s\n",
      "Epoch 10/15  loss=0.0682  val_loss=0.1162  f1=0.673  time=66.94s\n",
      "Epoch 11/15  loss=0.0641  val_loss=0.1204  f1=0.676  time=67.19s\n",
      "Epoch 12/15  loss=0.0603  val_loss=0.1202  f1=0.665  time=67.23s\n",
      "Epoch 13/15  loss=0.0569  val_loss=0.1332  f1=0.655  time=67.00s\n",
      "Epoch 14/15  loss=0.0539  val_loss=0.1428  f1=0.662  time=67.30s\n",
      "Epoch 15/15  loss=0.0509  val_loss=0.1491  f1=0.652  time=67.64s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1446  val_loss=0.1074  f1=0.653  time=67.32s\n",
      "model_saved at f1: 0.6531380619635622 from 0.0\n",
      "Epoch 2/15  loss=0.1046  val_loss=0.1033  f1=0.667  time=67.85s\n",
      "model_saved at f1: 0.667076763686988 from 0.6531380619635622\n",
      "Epoch 3/15  loss=0.0987  val_loss=0.1013  f1=0.677  time=67.84s\n",
      "model_saved at f1: 0.6770695416430237 from 0.667076763686988\n",
      "Epoch 4/15  loss=0.0937  val_loss=0.0981  f1=0.680  time=67.47s\n",
      "model_saved at f1: 0.6801326895917497 from 0.6770695416430237\n",
      "Epoch 5/15  loss=0.0894  val_loss=0.1009  f1=0.683  time=67.92s\n",
      "model_saved at f1: 0.6826742306804314 from 0.6801326895917497\n",
      "Epoch 6/15  loss=0.0852  val_loss=0.1007  f1=0.682  time=67.85s\n",
      "Epoch 7/15  loss=0.0807  val_loss=0.1020  f1=0.681  time=67.47s\n",
      "Epoch 8/15  loss=0.0762  val_loss=0.1045  f1=0.678  time=67.24s\n",
      "Epoch 9/15  loss=0.0719  val_loss=0.1098  f1=0.670  time=67.88s\n",
      "Epoch 10/15  loss=0.0677  val_loss=0.1105  f1=0.671  time=67.52s\n",
      "Epoch 11/15  loss=0.0638  val_loss=0.1193  f1=0.668  time=67.24s\n",
      "Epoch 12/15  loss=0.0600  val_loss=0.1228  f1=0.665  time=67.61s\n",
      "Epoch 13/15  loss=0.0565  val_loss=0.1327  f1=0.662  time=67.72s\n",
      "Epoch 14/15  loss=0.0530  val_loss=0.1427  f1=0.658  time=67.04s\n",
      "Epoch 15/15  loss=0.0500  val_loss=0.1391  f1=0.658  time=67.66s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1456  val_loss=0.1159  f1=0.653  time=67.51s\n",
      "model_saved at f1: 0.6526153944713156 from 0.0\n",
      "Epoch 2/15  loss=0.1046  val_loss=0.1032  f1=0.667  time=67.73s\n",
      "model_saved at f1: 0.6670956947036489 from 0.6526153944713156\n",
      "Epoch 3/15  loss=0.0984  val_loss=0.1001  f1=0.675  time=67.36s\n",
      "model_saved at f1: 0.6749035758645486 from 0.6670956947036489\n",
      "Epoch 4/15  loss=0.0936  val_loss=0.0994  f1=0.679  time=67.60s\n",
      "model_saved at f1: 0.6789372599231753 from 0.6749035758645486\n",
      "Epoch 5/15  loss=0.0892  val_loss=0.0984  f1=0.683  time=67.55s\n",
      "model_saved at f1: 0.6826478855038284 from 0.6789372599231753\n",
      "Epoch 6/15  loss=0.0849  val_loss=0.1009  f1=0.682  time=67.97s\n",
      "Epoch 7/15  loss=0.0806  val_loss=0.1030  f1=0.678  time=68.01s\n",
      "Epoch 8/15  loss=0.0762  val_loss=0.1037  f1=0.681  time=67.54s\n",
      "Epoch 9/15  loss=0.0719  val_loss=0.1074  f1=0.677  time=67.56s\n",
      "Epoch 10/15  loss=0.0676  val_loss=0.1158  f1=0.666  time=67.61s\n",
      "Epoch 11/15  loss=0.0637  val_loss=0.1220  f1=0.667  time=67.77s\n",
      "Epoch 12/15  loss=0.0597  val_loss=0.1196  f1=0.666  time=67.57s\n",
      "Epoch 13/15  loss=0.0565  val_loss=0.1337  f1=0.661  time=67.67s\n",
      "Epoch 14/15  loss=0.0529  val_loss=0.1372  f1=0.659  time=67.76s\n",
      "Epoch 15/15  loss=0.0501  val_loss=0.1481  f1=0.653  time=67.69s\n",
      "{'threshold': 0.25936976075172424, 'f1': 0.6577773192685631}\n",
      "f1 score: 0.6929751837727298\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
