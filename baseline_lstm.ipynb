{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# use only for tokenizer and padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1019):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# SEED = 1019\n",
    "# seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "class Config:\n",
    "    num_epochs = 10\n",
    "    batch_size = 512\n",
    "    test_batch_size = 512\n",
    "    vocab_size = 120000\n",
    "    max_length = 72\n",
    "    embedding_size = 300\n",
    "    hidden_size = 64\n",
    "    num_layers = 1\n",
    "    embedding_dropout = 0.3\n",
    "    layer_dropout = 0.1\n",
    "    dense_size = [hidden_size*2*4, int(hidden_size/4)] # depend on concat num\n",
    "    output_size = 1\n",
    "    num_cv_splits = 5\n",
    "    learning_rate = 0.001\n",
    "    clip_grad = 5.0\n",
    "    embeddings = ['glove', 'paragram', 'fasttext']\n",
    "    datadir = Path('./data/')\n",
    "    # datadir = Path('../input') # for kernel\n",
    "\n",
    "c = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [\n",
    "    ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '¬', '░', '¶', '↑', '±',  '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '₹', '´'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"'re\": \" are\",\n",
    "    \"tryin'\": \"trying\",\n",
    "    \"doesn'\": \"does not\",\n",
    "    'howdo': 'how do',\n",
    "    'whatare': 'what are',\n",
    "    'howcan': 'how can',\n",
    "    'howmuch': 'how much',\n",
    "    'howmany': 'how many',\n",
    "    'whydo': 'why do',\n",
    "    'doI': 'do I',\n",
    "    'theBest': 'the best',\n",
    "    'howdoes': 'how does',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spells = {\n",
    "    'colour': 'color',\n",
    "    'centre': 'center',\n",
    "    'favourite': 'favorite',\n",
    "    'travelling': 'traveling',\n",
    "    'counselling': 'counseling',\n",
    "    'theatre': 'theater',\n",
    "    'cancelled': 'canceled',\n",
    "    'labour': 'labor',\n",
    "    'organisation': 'organization',\n",
    "    'wwii': 'world war 2',\n",
    "    'citicise': 'criticize',\n",
    "    'youtu.be': 'youtube',\n",
    "    'youtu ': 'youtube ',\n",
    "    'qoura': 'quora',\n",
    "    'sallary': 'salary',\n",
    "    'Whta': 'what',\n",
    "    'whta': 'what',\n",
    "    'narcisist': 'narcissist',\n",
    "    'mastrubation': 'masturbation',\n",
    "    'mastrubate': 'masturbate',\n",
    "    \"mastrubating\": 'masturbating',\n",
    "    'pennis': 'penis',\n",
    "    'Etherium': 'ethereum',\n",
    "    'etherium': 'ethereum',\n",
    "    'narcissit': 'narcissist',\n",
    "    'bigdata': 'big data',\n",
    "    '2k17': '2017',\n",
    "    '2k18': '2018',\n",
    "    'qouta': 'quota',\n",
    "    'exboyfriend': 'ex boyfriend',\n",
    "    'exgirlfriend': 'ex girlfriend',\n",
    "    'airhostess': 'air hostess',\n",
    "    \"whst\": 'what',\n",
    "    'watsapp': 'whatsapp',\n",
    "    'demonitisation': 'demonetization',\n",
    "    'demonitization': 'demonetization',\n",
    "    'demonetisation': 'demonetization',\n",
    "    'quorans': 'quora user',\n",
    "    'quoran': 'quora user',\n",
    "    'pokémon': 'pokemon',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    train_df = pd.read_csv(datadir / 'train_local.csv')\n",
    "    test_df = pd.read_csv(datadir / 'test_local.csv')\n",
    "    print(\"Train shape : \", train_df.shape)\n",
    "    print(\"Test shape : \", test_df.shape)\n",
    "    return train_df, test_df\n",
    "\n",
    "def clean(df):\n",
    "    df = clean_lower(df)\n",
    "    df = clean_unicode(df)\n",
    "    df = clean_abbreviation(df, abbreviations)\n",
    "    df = clean_spells(df, spells)\n",
    "    df = clean_language(df)\n",
    "    df = clean_puncts(df, puncts)\n",
    "    df = clean_space(df)\n",
    "    return df\n",
    "\n",
    "def clean_unicode(df):\n",
    "    codes = ['\\x7f', '\\u200b', '\\xa0', '\\ufeff', '\\u200e', '\\u202a', '\\u202c', '\\u2060', '\\uf0d8', '\\ue019', '\\uf02d', '\\u200f', '\\u2061', '\\ue01b']\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: _clean_unicode(x, codes))\n",
    "    return df\n",
    "\n",
    "def _clean_unicode(x, codes):\n",
    "    for u in codes:\n",
    "        if u in x:\n",
    "            x = x.replace(u, '')\n",
    "    return x\n",
    "\n",
    "def clean_language(df):\n",
    "    langs1 = r'[\\p{Katakana}\\p{Hiragana}\\p{Han}]' # regex\n",
    "    langs2 = r'[ஆய்தஎழுத்துஆயுதஎழுத்துशुषछछशुषدوउसशुष북한내제តើបងប្អូនមានមធ្យបាយអ្វីខ្លះដើម្បីរកឃើញឯកសារអំពីប្រវត្តិស្ត្រនៃប្រាសាទអង្គរវट्टरौरआदસંઘરાજ્યपीतऊनअहএকটিবাড়িএকটিখামারএরঅধীনেপদেরবাছাইপরীক্ষাএরপ্রশ্নওউত্তরসহকোথায়পেতেপারিص、。Емелядуракلكلمقاممقال수능ί서로가를행복하게기乡국고등학교는몇시간업니《》싱관없어나이रचा키کپڤ」मिलगईकलेजेकोठंडकऋॠऌॡर]'\n",
    "    compiled_langs1 = regex.compile(langs1)\n",
    "    compiled_langs2 = re.compile(langs2)\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs1))\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs2))\n",
    "    return df\n",
    "\n",
    "def _clean_language(x, compiled_re):\n",
    "    return compiled_re.sub(' <lang> ', x)\n",
    "\n",
    "def clean_lower(df):\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "def clean_puncts(df, puncts):\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_puncts(x, puncts))\n",
    "    return df\n",
    "    \n",
    "def _clean_puncts(x, puncts):\n",
    "    x = str(x)\n",
    "    # added space around puncts after replace\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_spells(df, spells):\n",
    "    compiled_spells = re.compile('(%s)' % '|'.join(spells.keys()))\n",
    "    def replace(match):\n",
    "        return spells[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_spells(x, compiled_spells, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_spells(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_abbreviation(df, abbreviations):\n",
    "    compiled_abbreviation = re.compile('(%s)' % '|'.join(abbreviations.keys()))\n",
    "    def replace(match):\n",
    "        return abbreviations[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_abreviation(x, compiled_abbreviation, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_abreviation(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_space(df):\n",
    "    compiled_re = re.compile(r\"\\s+\")\n",
    "    df['question_text'] = df[\"question_text\"].apply(lambda x: _clean_space(x, compiled_re))\n",
    "    return df\n",
    "\n",
    "def _clean_space(x, compiled_re):\n",
    "    return compiled_re.sub(\" \", x)\n",
    "        \n",
    "def prepare_tokenizer(texts, max_words):\n",
    "    tokenizer = Tokenizer(num_words=max_words, filters='', oov_token='<unk>')\n",
    "    tokenizer.fit_on_texts(list(texts))\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_and_padding(texts, tokenizer, max_length):\n",
    "    texts = tokenizer.texts_to_sequences(texts)\n",
    "    texts = pad_sequences(texts, maxlen=max_length)\n",
    "    return texts\n",
    "\n",
    "def get_all_vocabs(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config, tokenizer, all_vocabs, embedding_weights = None):\n",
    "        super(Embeddings, self).__init__()\n",
    "        \n",
    "        self.embedding_map = {\n",
    "            'fasttext': self._load_fasttext,\n",
    "            'glove': self._load_glove,\n",
    "            'paragram': self._load_paragram\n",
    "        }\n",
    "        self.c = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.all_vocabs = all_vocabs\n",
    "        \n",
    "        if embedding_weights is None:\n",
    "            embedding_weights = self._load_embeddings(self.c.embeddings)\n",
    "            \n",
    "        self.original_embedding_weights = embedding_weights\n",
    "        self.embeddings = nn.Embedding(self.c.vocab_size + 1, self.c.embedding_size, padding_idx=0)\n",
    "        self.embeddings.weight = nn.Parameter(embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.embeddings(x)\n",
    "        return embedding\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        self.embeddings.weight = nn.Parameter(self.original_embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "    \n",
    "    def _load_embeddings(self, embedding_list: list):\n",
    "        embedding_weights = np.zeros((self.c.vocab_size, self.c.embedding_size))\n",
    "        pool = Pool(num_cores)\n",
    "        embedding_weights = np.mean(pool.map(self._load_an_embedding, embedding_list), 0)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return torch.tensor(embedding_weights, dtype=torch.float32)\n",
    "\n",
    "    def _load_an_embedding(self, emb):\n",
    "        return self.embedding_map[emb](self.tokenizer.word_index)\n",
    "        \n",
    "    def _get_embeddings_pair(self, word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "        \n",
    "    def _make_embeddings(self, embeddings_index, word_index, emb_mean, emb_std):\n",
    "        nb_words = min(self.c.vocab_size, len(word_index))\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, self.c.embedding_size))\n",
    "        embedding_matrix[0] = np.zeros(self.c.embedding_size)\n",
    "        for word, i in word_index.items():\n",
    "            if i >= self.c.vocab_size:\n",
    "                continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return embedding_matrix\n",
    "    \n",
    "    def _load_glove(self, word_index):\n",
    "        print('loading glove')\n",
    "        filepath = self.c.datadir / 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "    \n",
    "    def _load_fasttext(self, word_index):    \n",
    "        print('loading fasttext')\n",
    "        filepath = self.c.datadir / 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0033469985, 0.109855495\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "\n",
    "    def _load_paragram(self, word_index):\n",
    "        print('loading paragram')\n",
    "        filepath = self.c.datadir / 'embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath, encoding=\"utf8\", errors='ignore')\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0053247833, 0.49346462\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 2\n",
    "def df_parallelize_run(df, func, num_cores=2):\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1175509, 3)\n",
      "Test shape :  (130613, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_data(c.datadir)\n",
    "train_df = df_parallelize_run(train_df, clean)\n",
    "test_df = df_parallelize_run(test_df, clean)\n",
    "train_x, train_y = train_df['question_text'].values, train_df['target'].values\n",
    "test_x = test_df['question_text'].values\n",
    "tokenizer = prepare_tokenizer(train_x, c.vocab_size)\n",
    "train_x = tokenize_and_padding(train_x, tokenizer, c.max_length)\n",
    "test_x = tokenize_and_padding(test_x, tokenizer, c.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_vocabs:  184279\n",
      "loading glove\n",
      "loading paragram\n",
      "loading fasttext\n",
      "48.2561469078064\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_vocabs = get_all_vocabs(train_df['question_text'])\n",
    "print('all_vocabs: ', len(all_vocabs))\n",
    "embeddings = Embeddings(c, tokenizer, all_vocabs)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(GRULayer, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          bias=False,\n",
    "                          bidirectional=True,\n",
    "                          batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_outputs, gru_state = self.gru(x)\n",
    "        return self.dropout(gru_outputs), gru_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=False,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_outputs, (lstm_states, _) = self.lstm(x)\n",
    "        return self.dropout(lstm_outputs), lstm_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, config: Config, embeddings):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.c = config\n",
    "        \n",
    "        self.embedding = embeddings\n",
    "        self.lstm1 = LSTMLayer(input_size=self.c.embedding_size,\n",
    "                              hidden_size=self.c.hidden_size,\n",
    "                              num_layers=self.c.num_layers,\n",
    "                              dropout_rate=self.c.layer_dropout)\n",
    "        self.lstm2 = LSTMLayer(input_size=self.c.hidden_size*2,\n",
    "                            hidden_size=self.c.hidden_size,\n",
    "                            num_layers=self.c.num_layers,\n",
    "                            dropout_rate=self.c.layer_dropout)\n",
    "        \n",
    "        self.cell_dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.linear = nn.Linear(self.c.dense_size[0], self.c.dense_size[1])\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(self.c.dense_size[1])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.out = nn.Linear(self.c.dense_size[1], self.c.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        o_lstm1, h_lstm1 = self.lstm1(h_embedding)\n",
    "        o_lstm2, h_lstm2 = self.lstm2(o_lstm1)\n",
    "        \n",
    "        avg_pool = torch.mean(o_lstm2, 1)\n",
    "        max_pool, _ = torch.max(o_lstm2, 1)\n",
    "        \n",
    "        h_lstm1 = self.cell_dropout(torch.cat(h_lstm1.split(1, 0), -1).squeeze(0))\n",
    "        h_lstm2 = self.cell_dropout(torch.cat(h_lstm2.split(1, 0), -1).squeeze(0))\n",
    "\n",
    "        concat = torch.cat([h_lstm1, h_lstm2, avg_pool, max_pool], 1)\n",
    "        concat = self.linear(concat)\n",
    "        concat = self.batch_norm(concat)\n",
    "        concat = self.relu(concat)\n",
    "        concat = self.dropout(concat)\n",
    "        out = self.out(concat)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_length(data, mask):\n",
    "    max_length = data.shape[1]\n",
    "    transposed = torch.transpose(data, 1, 0)\n",
    "    res = (transposed == mask).all(1)\n",
    "    for i, r in enumerate(res):\n",
    "        if r == 0:\n",
    "            break\n",
    "    data = data[:, -(max_length - i):]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_x, train_y, test_x, c, embeddings, trial=0):\n",
    "    splits = list(StratifiedKFold(n_splits=c.num_cv_splits, shuffle=True).split(train_x, train_y))\n",
    "    x_test_cuda = torch.tensor(test_x, dtype=torch.long).cuda(cuda_idx)\n",
    "    test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=c.test_batch_size, shuffle=False)\n",
    "    train_preds = np.zeros((len(train_x)))\n",
    "    test_preds = np.zeros((len(test_x)))\n",
    "\n",
    "    mask = torch.zeros((c.max_length, 1), dtype=torch.long).cuda(cuda_idx)\n",
    "    \n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        x_train_fold = torch.tensor(train_x[train_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "        x_val_fold = torch.tensor(train_x[valid_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "\n",
    "        model = SimpleRNN(c, embeddings)\n",
    "        model.cuda(cuda_idx)\n",
    "\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=c.learning_rate)\n",
    "\n",
    "        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "        valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=c.batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=c.test_batch_size, shuffle=False)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        print(f'Fold {i + 1}')\n",
    "\n",
    "        for epoch in range(c.num_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred = model(x_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), c.clip_grad)\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "            avg_val_loss = 0.\n",
    "\n",
    "            # validation prediction\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred = model(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            search_result = threshold_search(y_val_fold.cpu().numpy(), valid_preds_fold)\n",
    "            valid_pred_targets = valid_preds_fold > search_result['threshold']\n",
    "            val_f1 = f1_score(y_val_fold.cpu().numpy(), valid_pred_targets)\n",
    "\n",
    "            elapsed_time = time.time() - start_time \n",
    "            print('Epoch {}/{}  loss={:.4f}  val_loss={:.4f}  f1={:.3f}  time={:.2f}s'.format(\n",
    "                epoch + 1, c.num_epochs, avg_loss, avg_val_loss, val_f1, elapsed_time))\n",
    "            if best_f1 < val_f1:\n",
    "                print(f'model_saved at f1: {val_f1} from {best_f1}')\n",
    "                ckpt_path = Path(f'./ckpt/baseline/{trial}/')\n",
    "                if not ckpt_path.exists():\n",
    "                    ckpt_path.mkdir(parents=True)\n",
    "                torch.save(model.state_dict(),ckpt_path / f'{i}_model.pt')\n",
    "                best_f1 = val_f1\n",
    "                best_epoch = epoch\n",
    "\n",
    "        # test prediction\n",
    "        model.load_state_dict(torch.load(f'./ckpt/baseline/{trial}/{i}_model.pt'))  # load best model\n",
    "        test_preds_fold = np.zeros(len(test_x))\n",
    "        for i, (x_batch, ) in enumerate(test_loader):\n",
    "            x_batch = cut_length(x_batch, mask)\n",
    "            y_pred = model(x_batch).detach()\n",
    "            test_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "        train_preds[valid_idx] = valid_preds_fold\n",
    "        test_preds += test_preds_fold / len(splits)\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  loss=0.1503  val_loss=0.1095  f1=0.651  time=63.22s\n",
      "model_saved at f1: 0.6509699776532695 from 0.0\n",
      "Epoch 2/10  loss=0.1057  val_loss=0.1050  f1=0.665  time=64.35s\n",
      "model_saved at f1: 0.6645824604376832 from 0.6509699776532695\n",
      "Epoch 3/10  loss=0.0993  val_loss=0.0994  f1=0.677  time=65.02s\n",
      "model_saved at f1: 0.6765874908582149 from 0.6645824604376832\n",
      "Epoch 4/10  loss=0.0944  val_loss=0.0984  f1=0.682  time=65.05s\n",
      "model_saved at f1: 0.6816266658343997 from 0.6765874908582149\n",
      "Epoch 5/10  loss=0.0901  val_loss=0.0976  f1=0.684  time=65.83s\n",
      "model_saved at f1: 0.6837535642200364 from 0.6816266658343997\n",
      "Epoch 6/10  loss=0.0859  val_loss=0.1020  f1=0.686  time=65.12s\n",
      "model_saved at f1: 0.6861450246432369 from 0.6837535642200364\n",
      "Epoch 7/10  loss=0.0814  val_loss=0.1009  f1=0.680  time=65.30s\n",
      "Epoch 8/10  loss=0.0768  val_loss=0.1024  f1=0.680  time=65.82s\n",
      "Epoch 9/10  loss=0.0724  val_loss=0.1040  f1=0.677  time=65.31s\n",
      "Epoch 10/10  loss=0.0683  val_loss=0.1106  f1=0.673  time=65.37s\n",
      "Fold 2\n",
      "Epoch 1/10  loss=0.1738  val_loss=0.1106  f1=0.639  time=66.04s\n",
      "model_saved at f1: 0.6391811202729598 from 0.0\n",
      "Epoch 2/10  loss=0.1092  val_loss=0.1043  f1=0.664  time=65.61s\n",
      "model_saved at f1: 0.663866893213221 from 0.6391811202729598\n",
      "Epoch 3/10  loss=0.1021  val_loss=0.1023  f1=0.667  time=65.92s\n",
      "model_saved at f1: 0.6668759811616953 from 0.663866893213221\n",
      "Epoch 4/10  loss=0.0973  val_loss=0.0992  f1=0.678  time=66.10s\n",
      "model_saved at f1: 0.6775777414075287 from 0.6668759811616953\n",
      "Epoch 5/10  loss=0.0926  val_loss=0.1034  f1=0.682  time=65.71s\n",
      "model_saved at f1: 0.6821840752594317 from 0.6775777414075287\n",
      "Epoch 6/10  loss=0.0880  val_loss=0.1004  f1=0.681  time=65.84s\n",
      "Epoch 7/10  loss=0.0837  val_loss=0.1009  f1=0.681  time=66.02s\n",
      "Epoch 8/10  loss=0.0793  val_loss=0.1010  f1=0.678  time=65.87s\n",
      "Epoch 9/10  loss=0.0748  val_loss=0.1057  f1=0.676  time=65.97s\n",
      "Epoch 10/10  loss=0.0704  val_loss=0.1096  f1=0.673  time=66.10s\n",
      "Fold 3\n",
      "Epoch 1/10  loss=0.1451  val_loss=0.1081  f1=0.645  time=65.84s\n",
      "model_saved at f1: 0.6451163085504215 from 0.0\n",
      "Epoch 2/10  loss=0.1046  val_loss=0.1015  f1=0.668  time=65.98s\n",
      "model_saved at f1: 0.6681243464207625 from 0.6451163085504215\n",
      "Epoch 3/10  loss=0.0984  val_loss=0.0999  f1=0.676  time=66.18s\n",
      "model_saved at f1: 0.6755372313843078 from 0.6681243464207625\n",
      "Epoch 4/10  loss=0.0935  val_loss=0.0981  f1=0.681  time=65.82s\n",
      "model_saved at f1: 0.681465393337379 from 0.6755372313843078\n",
      "Epoch 5/10  loss=0.0892  val_loss=0.0986  f1=0.681  time=66.07s\n",
      "Epoch 6/10  loss=0.0849  val_loss=0.0990  f1=0.682  time=65.94s\n",
      "model_saved at f1: 0.6823964544208143 from 0.681465393337379\n",
      "Epoch 7/10  loss=0.0806  val_loss=0.1018  f1=0.682  time=65.74s\n",
      "Epoch 8/10  loss=0.0759  val_loss=0.1046  f1=0.676  time=66.07s\n",
      "Epoch 9/10  loss=0.0719  val_loss=0.1104  f1=0.675  time=66.08s\n",
      "Epoch 10/10  loss=0.0674  val_loss=0.1149  f1=0.673  time=65.92s\n",
      "Fold 4\n",
      "Epoch 1/10  loss=0.1571  val_loss=0.1266  f1=0.626  time=66.10s\n",
      "model_saved at f1: 0.6261817226890756 from 0.0\n",
      "Epoch 2/10  loss=0.1065  val_loss=0.1042  f1=0.667  time=65.76s\n",
      "model_saved at f1: 0.6669693530079454 from 0.6261817226890756\n",
      "Epoch 3/10  loss=0.1002  val_loss=0.1019  f1=0.675  time=65.81s\n",
      "model_saved at f1: 0.6753628874068262 from 0.6669693530079454\n",
      "Epoch 4/10  loss=0.0955  val_loss=0.1001  f1=0.682  time=66.20s\n",
      "model_saved at f1: 0.6817944705268649 from 0.6753628874068262\n",
      "Epoch 5/10  loss=0.0912  val_loss=0.0988  f1=0.684  time=65.84s\n",
      "model_saved at f1: 0.6844046364594311 from 0.6817944705268649\n",
      "Epoch 6/10  loss=0.0871  val_loss=0.0988  f1=0.684  time=65.83s\n",
      "Epoch 7/10  loss=0.0832  val_loss=0.1003  f1=0.683  time=66.27s\n",
      "Epoch 8/10  loss=0.0791  val_loss=0.1019  f1=0.678  time=65.83s\n",
      "Epoch 9/10  loss=0.0749  val_loss=0.1049  f1=0.683  time=65.79s\n",
      "Epoch 10/10  loss=0.0704  val_loss=0.1077  f1=0.676  time=66.12s\n",
      "Fold 5\n",
      "Epoch 1/10  loss=0.1700  val_loss=0.1094  f1=0.647  time=65.86s\n",
      "model_saved at f1: 0.6471452292757696 from 0.0\n",
      "Epoch 2/10  loss=0.1071  val_loss=0.1044  f1=0.668  time=66.06s\n",
      "model_saved at f1: 0.6676933936535437 from 0.6471452292757696\n",
      "Epoch 3/10  loss=0.1004  val_loss=0.1008  f1=0.680  time=65.97s\n",
      "model_saved at f1: 0.6801340100175807 from 0.6676933936535437\n",
      "Epoch 4/10  loss=0.0957  val_loss=0.0981  f1=0.685  time=66.05s\n",
      "model_saved at f1: 0.6853572363255692 from 0.6801340100175807\n",
      "Epoch 5/10  loss=0.0913  val_loss=0.0977  f1=0.687  time=66.49s\n",
      "model_saved at f1: 0.6874607217279133 from 0.6853572363255692\n",
      "Epoch 6/10  loss=0.0869  val_loss=0.0986  f1=0.686  time=66.02s\n",
      "Epoch 7/10  loss=0.0831  val_loss=0.0999  f1=0.683  time=66.04s\n",
      "Epoch 8/10  loss=0.0787  val_loss=0.1033  f1=0.681  time=65.98s\n",
      "Epoch 9/10  loss=0.0746  val_loss=0.1028  f1=0.678  time=65.96s\n",
      "Epoch 10/10  loss=0.0703  val_loss=0.1095  f1=0.677  time=66.12s\n",
      "{'threshold': 0.3115536570549011, 'f1': 0.6731734990555638}\n",
      "f1 score: 0.6948017307806296\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings, trial=0)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  loss=0.1387  val_loss=0.1079  f1=0.653  time=66.36s\n",
      "model_saved at f1: 0.6533039647577092 from 0.0\n",
      "Epoch 2/10  loss=0.1044  val_loss=0.1032  f1=0.665  time=66.68s\n",
      "model_saved at f1: 0.6647861453686696 from 0.6533039647577092\n",
      "Epoch 3/10  loss=0.0984  val_loss=0.0997  f1=0.678  time=66.43s\n",
      "model_saved at f1: 0.6777443073235513 from 0.6647861453686696\n",
      "Epoch 4/10  loss=0.0936  val_loss=0.0984  f1=0.681  time=66.63s\n",
      "model_saved at f1: 0.681273149601356 from 0.6777443073235513\n",
      "Epoch 5/10  loss=0.0893  val_loss=0.0989  f1=0.682  time=66.56s\n",
      "model_saved at f1: 0.6818863740134369 from 0.681273149601356\n",
      "Epoch 6/10  loss=0.0850  val_loss=0.0989  f1=0.681  time=66.56s\n",
      "Epoch 7/10  loss=0.0805  val_loss=0.1037  f1=0.679  time=66.78s\n",
      "Epoch 8/10  loss=0.0763  val_loss=0.1057  f1=0.676  time=66.64s\n",
      "Epoch 9/10  loss=0.0720  val_loss=0.1089  f1=0.674  time=66.91s\n",
      "Epoch 10/10  loss=0.0675  val_loss=0.1118  f1=0.672  time=66.74s\n",
      "Fold 2\n",
      "Epoch 1/10  loss=0.1615  val_loss=0.1095  f1=0.646  time=65.90s\n",
      "model_saved at f1: 0.6457686212361331 from 0.0\n",
      "Epoch 2/10  loss=0.1065  val_loss=0.1040  f1=0.667  time=65.78s\n",
      "model_saved at f1: 0.666946065314807 from 0.6457686212361331\n",
      "Epoch 3/10  loss=0.0999  val_loss=0.0993  f1=0.679  time=65.91s\n",
      "model_saved at f1: 0.6786045470533687 from 0.666946065314807\n",
      "Epoch 4/10  loss=0.0947  val_loss=0.0982  f1=0.684  time=65.76s\n",
      "model_saved at f1: 0.6842619745845552 from 0.6786045470533687\n",
      "Epoch 5/10  loss=0.0902  val_loss=0.0970  f1=0.687  time=65.54s\n",
      "model_saved at f1: 0.6869399242987934 from 0.6842619745845552\n",
      "Epoch 6/10  loss=0.0857  val_loss=0.0994  f1=0.688  time=65.75s\n",
      "model_saved at f1: 0.6881326520775088 from 0.6869399242987934\n",
      "Epoch 7/10  loss=0.0812  val_loss=0.1015  f1=0.677  time=65.71s\n",
      "Epoch 8/10  loss=0.0768  val_loss=0.1038  f1=0.681  time=65.26s\n",
      "Epoch 9/10  loss=0.0720  val_loss=0.1098  f1=0.677  time=66.06s\n",
      "Epoch 10/10  loss=0.0676  val_loss=0.1113  f1=0.677  time=65.48s\n",
      "Fold 3\n",
      "Epoch 1/10  loss=0.1653  val_loss=0.1108  f1=0.643  time=65.50s\n",
      "model_saved at f1: 0.6429930656345751 from 0.0\n",
      "Epoch 2/10  loss=0.1060  val_loss=0.1026  f1=0.669  time=65.39s\n",
      "model_saved at f1: 0.6692161820480405 from 0.6429930656345751\n",
      "Epoch 3/10  loss=0.0996  val_loss=0.1005  f1=0.676  time=66.04s\n",
      "model_saved at f1: 0.6757207988184679 from 0.6692161820480405\n",
      "Epoch 4/10  loss=0.0951  val_loss=0.0988  f1=0.682  time=65.50s\n",
      "model_saved at f1: 0.6824925816023739 from 0.6757207988184679\n",
      "Epoch 5/10  loss=0.0908  val_loss=0.0998  f1=0.681  time=65.32s\n",
      "Epoch 6/10  loss=0.0864  val_loss=0.0978  f1=0.685  time=65.74s\n",
      "model_saved at f1: 0.6851151157570705 from 0.6824925816023739\n",
      "Epoch 7/10  loss=0.0824  val_loss=0.0992  f1=0.683  time=65.51s\n",
      "Epoch 8/10  loss=0.0779  val_loss=0.1038  f1=0.675  time=65.35s\n",
      "Epoch 9/10  loss=0.0735  val_loss=0.1069  f1=0.673  time=66.00s\n",
      "Epoch 10/10  loss=0.0695  val_loss=0.1077  f1=0.671  time=65.26s\n",
      "Fold 4\n",
      "Epoch 1/10  loss=0.1526  val_loss=0.1086  f1=0.654  time=65.40s\n",
      "model_saved at f1: 0.6535612535612536 from 0.0\n",
      "Epoch 2/10  loss=0.1054  val_loss=0.1016  f1=0.669  time=65.73s\n",
      "model_saved at f1: 0.6692167925791337 from 0.6535612535612536\n",
      "Epoch 3/10  loss=0.0994  val_loss=0.1036  f1=0.678  time=65.22s\n",
      "model_saved at f1: 0.6779585040983607 from 0.6692167925791337\n",
      "Epoch 4/10  loss=0.0947  val_loss=0.0998  f1=0.683  time=65.05s\n",
      "model_saved at f1: 0.6834908571242976 from 0.6779585040983607\n",
      "Epoch 5/10  loss=0.0900  val_loss=0.0970  f1=0.686  time=65.72s\n",
      "model_saved at f1: 0.686271396360634 from 0.6834908571242976\n",
      "Epoch 6/10  loss=0.0858  val_loss=0.0982  f1=0.687  time=64.91s\n",
      "model_saved at f1: 0.6867230552351661 from 0.686271396360634\n",
      "Epoch 7/10  loss=0.0813  val_loss=0.1024  f1=0.681  time=64.90s\n",
      "Epoch 8/10  loss=0.0768  val_loss=0.1011  f1=0.682  time=65.65s\n",
      "Epoch 9/10  loss=0.0721  val_loss=0.1078  f1=0.677  time=65.05s\n",
      "Epoch 10/10  loss=0.0678  val_loss=0.1118  f1=0.676  time=65.01s\n",
      "Fold 5\n",
      "Epoch 1/10  loss=0.1417  val_loss=0.1097  f1=0.647  time=65.91s\n",
      "model_saved at f1: 0.6469749310683611 from 0.0\n",
      "Epoch 2/10  loss=0.1053  val_loss=0.1018  f1=0.671  time=65.87s\n",
      "model_saved at f1: 0.6712512794268167 from 0.6469749310683611\n",
      "Epoch 3/10  loss=0.0989  val_loss=0.0997  f1=0.681  time=65.33s\n",
      "model_saved at f1: 0.6811603650586703 from 0.6712512794268167\n",
      "Epoch 4/10  loss=0.0941  val_loss=0.0982  f1=0.686  time=66.01s\n",
      "model_saved at f1: 0.6856216073083543 from 0.6811603650586703\n",
      "Epoch 5/10  loss=0.0896  val_loss=0.0984  f1=0.687  time=66.06s\n",
      "model_saved at f1: 0.6873628501355364 from 0.6856216073083543\n",
      "Epoch 6/10  loss=0.0851  val_loss=0.1021  f1=0.685  time=65.28s\n",
      "Epoch 7/10  loss=0.0809  val_loss=0.1048  f1=0.683  time=66.08s\n",
      "Epoch 8/10  loss=0.0763  val_loss=0.1037  f1=0.680  time=66.11s\n",
      "Epoch 9/10  loss=0.0719  val_loss=0.1119  f1=0.676  time=65.55s\n",
      "Epoch 10/10  loss=0.0676  val_loss=0.1103  f1=0.676  time=65.94s\n",
      "{'threshold': 0.31946423649787903, 'f1': 0.6737245985505186}\n",
      "f1 score: 0.6987005486572335\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings, trial=1)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  loss=0.1683  val_loss=0.1111  f1=0.643  time=65.95s\n",
      "model_saved at f1: 0.6425483214649034 from 0.0\n",
      "Epoch 2/10  loss=0.1068  val_loss=0.1049  f1=0.662  time=65.82s\n",
      "model_saved at f1: 0.6620488516223114 from 0.6425483214649034\n",
      "Epoch 3/10  loss=0.1002  val_loss=0.1028  f1=0.673  time=65.65s\n",
      "model_saved at f1: 0.6728659834321077 from 0.6620488516223114\n",
      "Epoch 4/10  loss=0.0952  val_loss=0.0988  f1=0.681  time=65.90s\n",
      "model_saved at f1: 0.6811608083516905 from 0.6728659834321077\n",
      "Epoch 5/10  loss=0.0908  val_loss=0.0974  f1=0.687  time=65.61s\n",
      "model_saved at f1: 0.6866495632124184 from 0.6811608083516905\n",
      "Epoch 6/10  loss=0.0866  val_loss=0.0982  f1=0.685  time=65.43s\n",
      "Epoch 7/10  loss=0.0823  val_loss=0.0996  f1=0.681  time=65.89s\n",
      "Epoch 8/10  loss=0.0780  val_loss=0.1012  f1=0.684  time=65.66s\n",
      "Epoch 9/10  loss=0.0737  val_loss=0.1043  f1=0.679  time=65.55s\n",
      "Epoch 10/10  loss=0.0695  val_loss=0.1099  f1=0.675  time=65.71s\n",
      "Fold 2\n",
      "Epoch 1/10  loss=0.1516  val_loss=0.1068  f1=0.654  time=65.25s\n",
      "model_saved at f1: 0.6535602077954397 from 0.0\n",
      "Epoch 2/10  loss=0.1045  val_loss=0.1007  f1=0.675  time=65.26s\n",
      "model_saved at f1: 0.6752653586362174 from 0.6535602077954397\n",
      "Epoch 3/10  loss=0.0985  val_loss=0.0984  f1=0.683  time=65.53s\n",
      "model_saved at f1: 0.6834035166672024 from 0.6752653586362174\n",
      "Epoch 4/10  loss=0.0939  val_loss=0.0998  f1=0.687  time=65.55s\n",
      "model_saved at f1: 0.6871423490996088 from 0.6834035166672024\n",
      "Epoch 5/10  loss=0.0895  val_loss=0.0995  f1=0.686  time=64.94s\n",
      "Epoch 6/10  loss=0.0852  val_loss=0.1000  f1=0.686  time=65.70s\n",
      "Epoch 7/10  loss=0.0808  val_loss=0.1003  f1=0.684  time=65.32s\n",
      "Epoch 8/10  loss=0.0763  val_loss=0.1064  f1=0.683  time=65.15s\n",
      "Epoch 9/10  loss=0.0718  val_loss=0.1115  f1=0.677  time=65.64s\n",
      "Epoch 10/10  loss=0.0674  val_loss=0.1105  f1=0.675  time=65.35s\n",
      "Fold 3\n",
      "Epoch 1/10  loss=0.1428  val_loss=0.1073  f1=0.651  time=65.05s\n",
      "model_saved at f1: 0.651308502895478 from 0.0\n",
      "Epoch 2/10  loss=0.1050  val_loss=0.1033  f1=0.670  time=65.02s\n",
      "model_saved at f1: 0.670227222450463 from 0.651308502895478\n",
      "Epoch 3/10  loss=0.0986  val_loss=0.1005  f1=0.677  time=65.56s\n",
      "model_saved at f1: 0.6771337579617834 from 0.670227222450463\n",
      "Epoch 4/10  loss=0.0941  val_loss=0.0996  f1=0.682  time=65.21s\n",
      "model_saved at f1: 0.6817074734008461 from 0.6771337579617834\n",
      "Epoch 5/10  loss=0.0896  val_loss=0.0983  f1=0.682  time=64.96s\n",
      "model_saved at f1: 0.6817241824996768 from 0.6817074734008461\n",
      "Epoch 6/10  loss=0.0852  val_loss=0.0991  f1=0.684  time=65.58s\n",
      "model_saved at f1: 0.6837706134830734 from 0.6817241824996768\n",
      "Epoch 7/10  loss=0.0807  val_loss=0.0999  f1=0.680  time=65.29s\n",
      "Epoch 8/10  loss=0.0766  val_loss=0.1061  f1=0.682  time=65.08s\n",
      "Epoch 9/10  loss=0.0719  val_loss=0.1073  f1=0.677  time=65.58s\n",
      "Epoch 10/10  loss=0.0676  val_loss=0.1113  f1=0.675  time=65.22s\n",
      "Fold 4\n",
      "Epoch 1/10  loss=0.1620  val_loss=0.1151  f1=0.646  time=65.21s\n",
      "model_saved at f1: 0.6461557663504744 from 0.0\n",
      "Epoch 2/10  loss=0.1055  val_loss=0.1029  f1=0.666  time=65.61s\n",
      "model_saved at f1: 0.665500259201659 from 0.6461557663504744\n",
      "Epoch 3/10  loss=0.0992  val_loss=0.1003  f1=0.673  time=65.18s\n",
      "model_saved at f1: 0.6731728610626091 from 0.665500259201659\n",
      "Epoch 4/10  loss=0.0944  val_loss=0.0996  f1=0.673  time=65.23s\n",
      "Epoch 5/10  loss=0.0905  val_loss=0.0984  f1=0.679  time=65.50s\n",
      "model_saved at f1: 0.6788655664156386 from 0.6731728610626091\n",
      "Epoch 6/10  loss=0.0866  val_loss=0.0989  f1=0.680  time=65.32s\n",
      "model_saved at f1: 0.679752724491747 from 0.6788655664156386\n",
      "Epoch 7/10  loss=0.0824  val_loss=0.1005  f1=0.678  time=65.12s\n",
      "Epoch 8/10  loss=0.0779  val_loss=0.1034  f1=0.675  time=65.59s\n",
      "Epoch 9/10  loss=0.0736  val_loss=0.1089  f1=0.670  time=65.33s\n",
      "Epoch 10/10  loss=0.0692  val_loss=0.1096  f1=0.667  time=65.39s\n",
      "Fold 5\n",
      "Epoch 1/10  loss=0.1795  val_loss=0.1103  f1=0.646  time=65.86s\n",
      "model_saved at f1: 0.6457283184696911 from 0.0\n",
      "Epoch 2/10  loss=0.1072  val_loss=0.1021  f1=0.669  time=66.14s\n",
      "model_saved at f1: 0.669478455194989 from 0.6457283184696911\n",
      "Epoch 3/10  loss=0.1005  val_loss=0.0994  f1=0.681  time=66.58s\n",
      "model_saved at f1: 0.6814027354637971 from 0.669478455194989\n",
      "Epoch 4/10  loss=0.0960  val_loss=0.0977  f1=0.684  time=66.19s\n",
      "model_saved at f1: 0.6840209561231173 from 0.6814027354637971\n",
      "Epoch 5/10  loss=0.0917  val_loss=0.0978  f1=0.686  time=66.56s\n",
      "model_saved at f1: 0.6864367816091953 from 0.6840209561231173\n",
      "Epoch 6/10  loss=0.0874  val_loss=0.1029  f1=0.682  time=66.59s\n",
      "Epoch 7/10  loss=0.0830  val_loss=0.1009  f1=0.683  time=66.07s\n",
      "Epoch 8/10  loss=0.0790  val_loss=0.1007  f1=0.679  time=66.44s\n",
      "Epoch 9/10  loss=0.0746  val_loss=0.1076  f1=0.677  time=66.73s\n",
      "Epoch 10/10  loss=0.0703  val_loss=0.1104  f1=0.675  time=66.40s\n",
      "{'threshold': 0.30910423398017883, 'f1': 0.6723784024988844}\n",
      "f1 score: 0.7002980967667966\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings, trial=2)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  loss=0.1729  val_loss=0.1115  f1=0.638  time=66.07s\n",
      "model_saved at f1: 0.638089970501475 from 0.0\n",
      "Epoch 2/10  loss=0.1080  val_loss=0.1030  f1=0.667  time=66.20s\n",
      "model_saved at f1: 0.6668362641905428 from 0.638089970501475\n",
      "Epoch 3/10  loss=0.1007  val_loss=0.1008  f1=0.676  time=66.74s\n",
      "model_saved at f1: 0.6763196827095597 from 0.6668362641905428\n",
      "Epoch 4/10  loss=0.0961  val_loss=0.0992  f1=0.681  time=66.28s\n",
      "model_saved at f1: 0.6810817788678758 from 0.6763196827095597\n",
      "Epoch 5/10  loss=0.0920  val_loss=0.0973  f1=0.684  time=66.41s\n",
      "model_saved at f1: 0.6841198303287381 from 0.6810817788678758\n",
      "Epoch 6/10  loss=0.0875  val_loss=0.0984  f1=0.685  time=66.28s\n",
      "model_saved at f1: 0.6848411636031213 from 0.6841198303287381\n",
      "Epoch 7/10  loss=0.0834  val_loss=0.0981  f1=0.685  time=66.28s\n",
      "Epoch 8/10  loss=0.0789  val_loss=0.1004  f1=0.684  time=66.26s\n",
      "Epoch 9/10  loss=0.0745  val_loss=0.1045  f1=0.676  time=66.22s\n",
      "Epoch 10/10  loss=0.0702  val_loss=0.1087  f1=0.678  time=66.28s\n",
      "Fold 2\n",
      "Epoch 1/10  loss=0.1598  val_loss=0.1103  f1=0.643  time=65.94s\n",
      "model_saved at f1: 0.6434894991922455 from 0.0\n",
      "Epoch 2/10  loss=0.1065  val_loss=0.1048  f1=0.658  time=65.78s\n",
      "model_saved at f1: 0.6580722060161225 from 0.6434894991922455\n",
      "Epoch 3/10  loss=0.0999  val_loss=0.1007  f1=0.673  time=66.43s\n",
      "model_saved at f1: 0.6731154966030412 from 0.6580722060161225\n",
      "Epoch 4/10  loss=0.0950  val_loss=0.0992  f1=0.680  time=65.84s\n",
      "model_saved at f1: 0.6795122107352047 from 0.6731154966030412\n",
      "Epoch 5/10  loss=0.0909  val_loss=0.0980  f1=0.684  time=65.75s\n",
      "model_saved at f1: 0.683690172199306 from 0.6795122107352047\n",
      "Epoch 6/10  loss=0.0867  val_loss=0.0987  f1=0.684  time=66.84s\n",
      "model_saved at f1: 0.6837695654997726 from 0.683690172199306\n",
      "Epoch 7/10  loss=0.0827  val_loss=0.1002  f1=0.684  time=65.93s\n",
      "model_saved at f1: 0.6839368320306268 from 0.6837695654997726\n",
      "Epoch 8/10  loss=0.0785  val_loss=0.1031  f1=0.681  time=65.70s\n",
      "Epoch 9/10  loss=0.0742  val_loss=0.1045  f1=0.681  time=66.68s\n",
      "Epoch 10/10  loss=0.0699  val_loss=0.1103  f1=0.679  time=65.86s\n",
      "Fold 3\n",
      "Epoch 1/10  loss=0.1540  val_loss=0.1086  f1=0.650  time=65.45s\n",
      "model_saved at f1: 0.6502639577667574 from 0.0\n",
      "Epoch 2/10  loss=0.1054  val_loss=0.1037  f1=0.667  time=66.03s\n",
      "model_saved at f1: 0.6672129081053826 from 0.6502639577667574\n",
      "Epoch 3/10  loss=0.0989  val_loss=0.1013  f1=0.679  time=65.94s\n",
      "model_saved at f1: 0.6791063884265779 from 0.6672129081053826\n",
      "Epoch 4/10  loss=0.0939  val_loss=0.0975  f1=0.683  time=65.52s\n",
      "model_saved at f1: 0.6834167729096053 from 0.6791063884265779\n",
      "Epoch 5/10  loss=0.0895  val_loss=0.0990  f1=0.684  time=66.10s\n",
      "model_saved at f1: 0.684436255043365 from 0.6834167729096053\n",
      "Epoch 6/10  loss=0.0850  val_loss=0.0993  f1=0.683  time=66.04s\n",
      "Epoch 7/10  loss=0.0805  val_loss=0.1022  f1=0.679  time=65.45s\n",
      "Epoch 8/10  loss=0.0759  val_loss=0.1063  f1=0.676  time=66.29s\n",
      "Epoch 9/10  loss=0.0714  val_loss=0.1049  f1=0.679  time=65.98s\n",
      "Epoch 10/10  loss=0.0671  val_loss=0.1123  f1=0.669  time=65.56s\n",
      "Fold 4\n",
      "Epoch 1/10  loss=0.1550  val_loss=0.1074  f1=0.649  time=66.51s\n",
      "model_saved at f1: 0.6493721189000158 from 0.0\n",
      "Epoch 2/10  loss=0.1056  val_loss=0.1026  f1=0.669  time=66.39s\n",
      "model_saved at f1: 0.6690769179740622 from 0.6493721189000158\n",
      "Epoch 3/10  loss=0.0994  val_loss=0.1002  f1=0.678  time=66.34s\n",
      "model_saved at f1: 0.6782230424466121 from 0.6690769179740622\n",
      "Epoch 4/10  loss=0.0945  val_loss=0.0985  f1=0.683  time=66.46s\n",
      "model_saved at f1: 0.6830404133839431 from 0.6782230424466121\n",
      "Epoch 5/10  loss=0.0902  val_loss=0.0978  f1=0.685  time=66.48s\n",
      "model_saved at f1: 0.6848356445814072 from 0.6830404133839431\n",
      "Epoch 6/10  loss=0.0861  val_loss=0.0992  f1=0.682  time=66.29s\n",
      "Epoch 7/10  loss=0.0819  val_loss=0.0995  f1=0.685  time=66.32s\n",
      "model_saved at f1: 0.6848641655886157 from 0.6848356445814072\n",
      "Epoch 8/10  loss=0.0772  val_loss=0.1044  f1=0.683  time=66.42s\n",
      "Epoch 9/10  loss=0.0730  val_loss=0.1094  f1=0.683  time=66.46s\n",
      "Epoch 10/10  loss=0.0688  val_loss=0.1120  f1=0.677  time=66.32s\n",
      "Fold 5\n",
      "Epoch 1/10  loss=0.1576  val_loss=0.1095  f1=0.648  time=66.62s\n",
      "model_saved at f1: 0.6482827463580496 from 0.0\n",
      "Epoch 2/10  loss=0.1064  val_loss=0.1018  f1=0.670  time=65.98s\n",
      "model_saved at f1: 0.6697933277053435 from 0.6482827463580496\n",
      "Epoch 3/10  loss=0.0996  val_loss=0.0988  f1=0.677  time=66.46s\n",
      "model_saved at f1: 0.6766439909297052 from 0.6697933277053435\n",
      "Epoch 4/10  loss=0.0949  val_loss=0.1014  f1=0.677  time=66.53s\n",
      "model_saved at f1: 0.6771145425213182 from 0.6766439909297052\n",
      "Epoch 5/10  loss=0.0906  val_loss=0.0979  f1=0.685  time=66.19s\n",
      "model_saved at f1: 0.6845615957206844 from 0.6771145425213182\n",
      "Epoch 6/10  loss=0.0864  val_loss=0.0981  f1=0.685  time=66.64s\n",
      "model_saved at f1: 0.6849975880366618 from 0.6845615957206844\n",
      "Epoch 7/10  loss=0.0822  val_loss=0.1033  f1=0.683  time=66.61s\n",
      "Epoch 8/10  loss=0.0778  val_loss=0.1045  f1=0.679  time=66.31s\n",
      "Epoch 9/10  loss=0.0733  val_loss=0.1087  f1=0.673  time=66.48s\n",
      "Epoch 10/10  loss=0.0689  val_loss=0.1092  f1=0.674  time=66.45s\n",
      "{'threshold': 0.3046725392341614, 'f1': 0.6751183318408597}\n",
      "f1 score: 0.6978502510861592\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings, trial=3)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  loss=0.1715  val_loss=0.1095  f1=0.647  time=66.56s\n",
      "model_saved at f1: 0.6466296487603306 from 0.0\n",
      "Epoch 2/10  loss=0.1076  val_loss=0.1029  f1=0.665  time=66.30s\n",
      "model_saved at f1: 0.6651628911159089 from 0.6466296487603306\n",
      "Epoch 3/10  loss=0.1009  val_loss=0.1014  f1=0.669  time=66.18s\n",
      "model_saved at f1: 0.669432480364834 from 0.6651628911159089\n",
      "Epoch 4/10  loss=0.0961  val_loss=0.0984  f1=0.680  time=66.43s\n",
      "model_saved at f1: 0.6799886888490904 from 0.669432480364834\n",
      "Epoch 5/10  loss=0.0918  val_loss=0.1009  f1=0.683  time=66.31s\n",
      "model_saved at f1: 0.6831840087977488 from 0.6799886888490904\n",
      "Epoch 6/10  loss=0.0874  val_loss=0.0976  f1=0.684  time=66.62s\n",
      "model_saved at f1: 0.6841394751572224 from 0.6831840087977488\n",
      "Epoch 7/10  loss=0.0834  val_loss=0.0988  f1=0.685  time=66.56s\n",
      "model_saved at f1: 0.6849983729254799 from 0.6841394751572224\n",
      "Epoch 8/10  loss=0.0792  val_loss=0.1003  f1=0.681  time=66.39s\n",
      "Epoch 9/10  loss=0.0748  val_loss=0.1034  f1=0.678  time=66.47s\n",
      "Epoch 10/10  loss=0.0707  val_loss=0.1103  f1=0.674  time=66.56s\n",
      "Fold 2\n",
      "Epoch 1/10  loss=0.1440  val_loss=0.1101  f1=0.649  time=66.17s\n",
      "model_saved at f1: 0.6493457114793575 from 0.0\n",
      "Epoch 2/10  loss=0.1047  val_loss=0.1025  f1=0.666  time=66.24s\n",
      "model_saved at f1: 0.6662529396394042 from 0.6493457114793575\n",
      "Epoch 3/10  loss=0.0986  val_loss=0.0998  f1=0.673  time=66.27s\n",
      "model_saved at f1: 0.6730401529636711 from 0.6662529396394042\n",
      "Epoch 4/10  loss=0.0941  val_loss=0.0987  f1=0.681  time=66.03s\n",
      "model_saved at f1: 0.680663969159206 from 0.6730401529636711\n",
      "Epoch 5/10  loss=0.0896  val_loss=0.0983  f1=0.683  time=66.39s\n",
      "model_saved at f1: 0.6828792550179325 from 0.680663969159206\n",
      "Epoch 6/10  loss=0.0852  val_loss=0.0983  f1=0.684  time=66.25s\n",
      "model_saved at f1: 0.684034046943513 from 0.6828792550179325\n",
      "Epoch 7/10  loss=0.0810  val_loss=0.1040  f1=0.684  time=65.97s\n",
      "Epoch 8/10  loss=0.0765  val_loss=0.1033  f1=0.674  time=66.51s\n",
      "Epoch 9/10  loss=0.0721  val_loss=0.1048  f1=0.675  time=66.38s\n",
      "Epoch 10/10  loss=0.0679  val_loss=0.1124  f1=0.665  time=66.74s\n",
      "Fold 3\n",
      "Epoch 1/10  loss=0.1453  val_loss=0.1085  f1=0.652  time=66.51s\n",
      "model_saved at f1: 0.6516662995248135 from 0.0\n",
      "Epoch 2/10  loss=0.1056  val_loss=0.1020  f1=0.671  time=66.18s\n",
      "model_saved at f1: 0.670550612671374 from 0.6516662995248135\n",
      "Epoch 3/10  loss=0.0994  val_loss=0.0998  f1=0.678  time=65.76s\n",
      "model_saved at f1: 0.6777438826507431 from 0.670550612671374\n",
      "Epoch 4/10  loss=0.0947  val_loss=0.0982  f1=0.683  time=66.41s\n",
      "model_saved at f1: 0.6828410689170183 from 0.6777438826507431\n",
      "Epoch 5/10  loss=0.0904  val_loss=0.0979  f1=0.686  time=66.14s\n",
      "model_saved at f1: 0.6864840062926061 from 0.6828410689170183\n",
      "Epoch 6/10  loss=0.0860  val_loss=0.0986  f1=0.686  time=65.84s\n",
      "Epoch 7/10  loss=0.0818  val_loss=0.1006  f1=0.682  time=66.45s\n",
      "Epoch 8/10  loss=0.0775  val_loss=0.1025  f1=0.682  time=66.05s\n",
      "Epoch 9/10  loss=0.0731  val_loss=0.1075  f1=0.682  time=65.99s\n",
      "Epoch 10/10  loss=0.0689  val_loss=0.1151  f1=0.671  time=66.49s\n",
      "Fold 4\n",
      "Epoch 1/10  loss=0.1476  val_loss=0.1068  f1=0.653  time=65.81s\n",
      "model_saved at f1: 0.6532084098007018 from 0.0\n",
      "Epoch 2/10  loss=0.1052  val_loss=0.1034  f1=0.672  time=66.14s\n",
      "model_saved at f1: 0.671532389424884 from 0.6532084098007018\n",
      "Epoch 3/10  loss=0.0991  val_loss=0.0997  f1=0.682  time=65.94s\n",
      "model_saved at f1: 0.6821669836689186 from 0.671532389424884\n",
      "Epoch 4/10  loss=0.0945  val_loss=0.1017  f1=0.686  time=65.64s\n",
      "model_saved at f1: 0.6864572471324296 from 0.6821669836689186\n",
      "Epoch 5/10  loss=0.0901  val_loss=0.0997  f1=0.688  time=66.29s\n",
      "model_saved at f1: 0.6882944915254237 from 0.6864572471324296\n",
      "Epoch 6/10  loss=0.0860  val_loss=0.0978  f1=0.684  time=65.94s\n",
      "Epoch 7/10  loss=0.0816  val_loss=0.0983  f1=0.686  time=65.72s\n",
      "Epoch 8/10  loss=0.0772  val_loss=0.1005  f1=0.678  time=66.11s\n",
      "Epoch 9/10  loss=0.0728  val_loss=0.1041  f1=0.680  time=66.05s\n",
      "Epoch 10/10  loss=0.0687  val_loss=0.1083  f1=0.679  time=66.21s\n",
      "Fold 5\n",
      "Epoch 1/10  loss=0.1562  val_loss=0.1077  f1=0.648  time=66.19s\n",
      "model_saved at f1: 0.6482872320937901 from 0.0\n",
      "Epoch 2/10  loss=0.1053  val_loss=0.1018  f1=0.671  time=66.68s\n",
      "model_saved at f1: 0.6708762385855838 from 0.6482872320937901\n",
      "Epoch 3/10  loss=0.0989  val_loss=0.1002  f1=0.676  time=66.51s\n",
      "model_saved at f1: 0.6757842445800497 from 0.6708762385855838\n",
      "Epoch 4/10  loss=0.0942  val_loss=0.0992  f1=0.680  time=66.18s\n",
      "model_saved at f1: 0.6798365122615804 from 0.6757842445800497\n",
      "Epoch 5/10  loss=0.0899  val_loss=0.1000  f1=0.681  time=66.63s\n",
      "model_saved at f1: 0.6808062178896062 from 0.6798365122615804\n",
      "Epoch 6/10  loss=0.0857  val_loss=0.0991  f1=0.684  time=66.64s\n",
      "model_saved at f1: 0.6839883935637034 from 0.6808062178896062\n",
      "Epoch 7/10  loss=0.0812  val_loss=0.1031  f1=0.678  time=66.04s\n",
      "Epoch 8/10  loss=0.0770  val_loss=0.1035  f1=0.680  time=66.98s\n",
      "Epoch 9/10  loss=0.0728  val_loss=0.1060  f1=0.679  time=66.49s\n",
      "Epoch 10/10  loss=0.0684  val_loss=0.1117  f1=0.672  time=66.10s\n",
      "{'threshold': 0.29999855160713196, 'f1': 0.6718978879374318}\n",
      "f1 score: 0.6974440712699949\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings, trial=4)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
