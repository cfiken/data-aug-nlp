{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# use only for tokenizer and padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1019):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# SEED = 1019\n",
    "# seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "class Config:\n",
    "    num_epochs = 15\n",
    "    batch_size = 512\n",
    "    test_batch_size = 512\n",
    "    vocab_size = 120000\n",
    "    max_length = 72\n",
    "    embedding_size = 300\n",
    "    hidden_size = 64\n",
    "    num_layers = 1\n",
    "    embedding_dropout = 0.3\n",
    "    layer_dropout = 0.1\n",
    "    dense_size = [hidden_size*2*4, int(hidden_size/4)] # depend on concat num\n",
    "    output_size = 1\n",
    "    num_cv_splits = 5\n",
    "    learning_rate = 0.001\n",
    "    clip_grad = 5.0\n",
    "    embeddings = ['glove', 'paragram', 'fasttext']\n",
    "    datadir = Path('./data/')\n",
    "    # datadir = Path('../input') # for kernel\n",
    "\n",
    "c = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [\n",
    "    ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '¬', '░', '¶', '↑', '±',  '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '₹', '´'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"'re\": \" are\",\n",
    "    \"tryin'\": \"trying\",\n",
    "    \"doesn'\": \"does not\",\n",
    "    'howdo': 'how do',\n",
    "    'whatare': 'what are',\n",
    "    'howcan': 'how can',\n",
    "    'howmuch': 'how much',\n",
    "    'howmany': 'how many',\n",
    "    'whydo': 'why do',\n",
    "    'doI': 'do I',\n",
    "    'theBest': 'the best',\n",
    "    'howdoes': 'how does',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spells = {\n",
    "    'colour': 'color',\n",
    "    'centre': 'center',\n",
    "    'favourite': 'favorite',\n",
    "    'travelling': 'traveling',\n",
    "    'counselling': 'counseling',\n",
    "    'theatre': 'theater',\n",
    "    'cancelled': 'canceled',\n",
    "    'labour': 'labor',\n",
    "    'organisation': 'organization',\n",
    "    'wwii': 'world war 2',\n",
    "    'citicise': 'criticize',\n",
    "    'youtu.be': 'youtube',\n",
    "    'youtu ': 'youtube ',\n",
    "    'qoura': 'quora',\n",
    "    'sallary': 'salary',\n",
    "    'Whta': 'what',\n",
    "    'whta': 'what',\n",
    "    'narcisist': 'narcissist',\n",
    "    'mastrubation': 'masturbation',\n",
    "    'mastrubate': 'masturbate',\n",
    "    \"mastrubating\": 'masturbating',\n",
    "    'pennis': 'penis',\n",
    "    'Etherium': 'ethereum',\n",
    "    'etherium': 'ethereum',\n",
    "    'narcissit': 'narcissist',\n",
    "    'bigdata': 'big data',\n",
    "    '2k17': '2017',\n",
    "    '2k18': '2018',\n",
    "    'qouta': 'quota',\n",
    "    'exboyfriend': 'ex boyfriend',\n",
    "    'exgirlfriend': 'ex girlfriend',\n",
    "    'airhostess': 'air hostess',\n",
    "    \"whst\": 'what',\n",
    "    'watsapp': 'whatsapp',\n",
    "    'demonitisation': 'demonetization',\n",
    "    'demonitization': 'demonetization',\n",
    "    'demonetisation': 'demonetization',\n",
    "    'quorans': 'quora user',\n",
    "    'quoran': 'quora user',\n",
    "    'pokémon': 'pokemon',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    train_df = pd.read_csv(datadir / 'train_local.csv')\n",
    "    test_df = pd.read_csv(datadir / 'test_local.csv')\n",
    "    print(\"Train shape : \", train_df.shape)\n",
    "    print(\"Test shape : \", test_df.shape)\n",
    "    return train_df, test_df\n",
    "\n",
    "def clean(df):\n",
    "    df = clean_lower(df)\n",
    "    df = clean_unicode(df)\n",
    "    df = clean_abbreviation(df, abbreviations)\n",
    "    df = clean_spells(df, spells)\n",
    "    df = clean_language(df)\n",
    "    df = clean_puncts(df, puncts)\n",
    "    df = clean_space(df)\n",
    "    return df\n",
    "\n",
    "def clean_unicode(df):\n",
    "    codes = ['\\x7f', '\\u200b', '\\xa0', '\\ufeff', '\\u200e', '\\u202a', '\\u202c', '\\u2060', '\\uf0d8', '\\ue019', '\\uf02d', '\\u200f', '\\u2061', '\\ue01b']\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: _clean_unicode(x, codes))\n",
    "    return df\n",
    "\n",
    "def _clean_unicode(x, codes):\n",
    "    for u in codes:\n",
    "        if u in x:\n",
    "            x = x.replace(u, '')\n",
    "    return x\n",
    "\n",
    "def clean_language(df):\n",
    "    langs1 = r'[\\p{Katakana}\\p{Hiragana}\\p{Han}]' # regex\n",
    "    langs2 = r'[ஆய்தஎழுத்துஆயுதஎழுத்துशुषछछशुषدوउसशुष북한내제តើបងប្អូនមានមធ្យបាយអ្វីខ្លះដើម្បីរកឃើញឯកសារអំពីប្រវត្តិស្ត្រនៃប្រាសាទអង្គរវट्टरौरआदસંઘરાજ્યपीतऊनअहএকটিবাড়িএকটিখামারএরঅধীনেপদেরবাছাইপরীক্ষাএরপ্রশ্নওউত্তরসহকোথায়পেতেপারিص、。Емелядуракلكلمقاممقال수능ί서로가를행복하게기乡국고등학교는몇시간업니《》싱관없어나이रचा키کپڤ」मिलगईकलेजेकोठंडकऋॠऌॡर]'\n",
    "    compiled_langs1 = regex.compile(langs1)\n",
    "    compiled_langs2 = re.compile(langs2)\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs1))\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs2))\n",
    "    return df\n",
    "\n",
    "def _clean_language(x, compiled_re):\n",
    "    return compiled_re.sub(' <lang> ', x)\n",
    "\n",
    "def clean_lower(df):\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "def clean_puncts(df, puncts):\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_puncts(x, puncts))\n",
    "    return df\n",
    "    \n",
    "def _clean_puncts(x, puncts):\n",
    "    x = str(x)\n",
    "    # added space around puncts after replace\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_spells(df, spells):\n",
    "    compiled_spells = re.compile('(%s)' % '|'.join(spells.keys()))\n",
    "    def replace(match):\n",
    "        return spells[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_spells(x, compiled_spells, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_spells(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_abbreviation(df, abbreviations):\n",
    "    compiled_abbreviation = re.compile('(%s)' % '|'.join(abbreviations.keys()))\n",
    "    def replace(match):\n",
    "        return abbreviations[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_abreviation(x, compiled_abbreviation, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_abreviation(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_space(df):\n",
    "    compiled_re = re.compile(r\"\\s+\")\n",
    "    df['question_text'] = df[\"question_text\"].apply(lambda x: _clean_space(x, compiled_re))\n",
    "    return df\n",
    "\n",
    "def _clean_space(x, compiled_re):\n",
    "    return compiled_re.sub(\" \", x)\n",
    "        \n",
    "def prepare_tokenizer(texts, max_words):\n",
    "    tokenizer = Tokenizer(num_words=max_words, filters='', oov_token='<unk>')\n",
    "    tokenizer.fit_on_texts(list(texts))\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_and_padding(texts, tokenizer, max_length):\n",
    "    texts = tokenizer.texts_to_sequences(texts)\n",
    "    texts = pad_sequences(texts, maxlen=max_length)\n",
    "    return texts\n",
    "\n",
    "def get_all_vocabs(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config, tokenizer, all_vocabs, embedding_weights = None):\n",
    "        super(Embeddings, self).__init__()\n",
    "        \n",
    "        self.embedding_map = {\n",
    "            'fasttext': self._load_fasttext,\n",
    "            'glove': self._load_glove,\n",
    "            'paragram': self._load_paragram\n",
    "        }\n",
    "        self.c = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.all_vocabs = all_vocabs\n",
    "        \n",
    "        if embedding_weights is None:\n",
    "            embedding_weights = self._load_embeddings(self.c.embeddings)\n",
    "            \n",
    "        self.original_embedding_weights = embedding_weights\n",
    "        self.embeddings = nn.Embedding(self.c.vocab_size + 1, self.c.embedding_size, padding_idx=0)\n",
    "        self.embeddings.weight = nn.Parameter(embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.embedding_dropout = nn.Dropout2d(self.c.embedding_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.embeddings(x)\n",
    "        return self.embedding_dropout(embedding)\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        self.embeddings.weight = nn.Parameter(self.original_embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "    \n",
    "    def _load_embeddings(self, embedding_list: list):\n",
    "        embedding_weights = np.zeros((self.c.vocab_size, self.c.embedding_size))\n",
    "        pool = Pool(num_cores)\n",
    "        embedding_weights = np.mean(pool.map(self._load_an_embedding, embedding_list), 0)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return torch.tensor(embedding_weights, dtype=torch.float32)\n",
    "\n",
    "    def _load_an_embedding(self, emb):\n",
    "        return self.embedding_map[emb](self.tokenizer.word_index)\n",
    "        \n",
    "    def _get_embeddings_pair(self, word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "        \n",
    "    def _make_embeddings(self, embeddings_index, word_index, emb_mean, emb_std):\n",
    "        nb_words = min(self.c.vocab_size, len(word_index))\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, self.c.embedding_size))\n",
    "        embedding_matrix[0] = np.zeros(self.c.embedding_size)\n",
    "        for word, i in word_index.items():\n",
    "            if i >= self.c.vocab_size:\n",
    "                continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return embedding_matrix\n",
    "    \n",
    "    def _load_glove(self, word_index):\n",
    "        print('loading glove')\n",
    "        filepath = self.c.datadir / 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "    \n",
    "    def _load_fasttext(self, word_index):    \n",
    "        print('loading fasttext')\n",
    "        filepath = self.c.datadir / 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0033469985, 0.109855495\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "\n",
    "    def _load_paragram(self, word_index):\n",
    "        print('loading paragram')\n",
    "        filepath = self.c.datadir / 'embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath, encoding=\"utf8\", errors='ignore')\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0053247833, 0.49346462\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 2\n",
    "def df_parallelize_run(df, func, num_cores=2):\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1175509, 3)\n",
      "Test shape :  (130613, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_data(c.datadir)\n",
    "train_df = df_parallelize_run(train_df, clean)\n",
    "test_df = df_parallelize_run(test_df, clean)\n",
    "train_x, train_y = train_df['question_text'].values, train_df['target'].values\n",
    "test_x = test_df['question_text'].values\n",
    "tokenizer = prepare_tokenizer(train_x, c.vocab_size)\n",
    "train_x = tokenize_and_padding(train_x, tokenizer, c.max_length)\n",
    "test_x = tokenize_and_padding(test_x, tokenizer, c.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_vocabs:  184279\n",
      "loading glove\n",
      "loading paragram\n",
      "loading fasttext\n",
      "49.65930151939392\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_vocabs = get_all_vocabs(train_df['question_text'])\n",
    "print('all_vocabs: ', len(all_vocabs))\n",
    "embeddings = Embeddings(c, tokenizer, all_vocabs)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(GRULayer, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          bias=False,\n",
    "                          bidirectional=True,\n",
    "                          batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_outputs, gru_state = self.gru(x)\n",
    "        return self.dropout(gru_outputs), gru_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=False,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_outputs, (lstm_states, _) = self.lstm(x)\n",
    "        return self.dropout(lstm_outputs), lstm_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, config: Config, embeddings):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.c = config\n",
    "        \n",
    "        self.embedding = embeddings\n",
    "        self.lstm1 = LSTMLayer(input_size=self.c.embedding_size,\n",
    "                              hidden_size=self.c.hidden_size,\n",
    "                              num_layers=self.c.num_layers,\n",
    "                              dropout_rate=self.c.layer_dropout)\n",
    "        self.lstm2 = LSTMLayer(input_size=self.c.hidden_size*2,\n",
    "                            hidden_size=self.c.hidden_size,\n",
    "                            num_layers=self.c.num_layers,\n",
    "                            dropout_rate=self.c.layer_dropout)\n",
    "        \n",
    "        self.cell_dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.linear = nn.Linear(self.c.dense_size[0], self.c.dense_size[1])\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(self.c.dense_size[1])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.out = nn.Linear(self.c.dense_size[1], self.c.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        o_lstm1, h_lstm1 = self.lstm1(h_embedding)\n",
    "        o_lstm2, h_lstm2 = self.lstm2(o_lstm1)\n",
    "        \n",
    "        avg_pool = torch.mean(o_lstm2, 1)\n",
    "        max_pool, _ = torch.max(o_lstm2, 1)\n",
    "        \n",
    "        h_lstm1 = self.cell_dropout(torch.cat(h_lstm1.split(1, 0), -1).squeeze(0))\n",
    "        h_lstm2 = self.cell_dropout(torch.cat(h_lstm2.split(1, 0), -1).squeeze(0))\n",
    "\n",
    "        concat = torch.cat([h_lstm1, h_lstm2, avg_pool, max_pool], 1)\n",
    "        concat = self.linear(concat)\n",
    "        concat = self.batch_norm(concat)\n",
    "        concat = self.relu(concat)\n",
    "        concat = self.dropout(concat)\n",
    "        out = self.out(concat)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_length(data, mask):\n",
    "    max_length = data.shape[1]\n",
    "    transposed = torch.transpose(data, 1, 0)\n",
    "    res = (transposed == mask).all(1)\n",
    "    for i, r in enumerate(res):\n",
    "        if r == 0:\n",
    "            break\n",
    "    data = data[:, -(max_length - i):]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_x, train_y, test_x, c, embeddings, trial=0):\n",
    "    splits = list(StratifiedKFold(n_splits=c.num_cv_splits, shuffle=True).split(train_x, train_y))\n",
    "    x_test_cuda = torch.tensor(test_x, dtype=torch.long).cuda(cuda_idx)\n",
    "    test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=c.test_batch_size, shuffle=False)\n",
    "    train_preds = np.zeros((len(train_x)))\n",
    "    test_preds = np.zeros((len(test_x)))\n",
    "\n",
    "    mask = torch.zeros((c.max_length, 1), dtype=torch.long).cuda(cuda_idx)\n",
    "    \n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        x_train_fold = torch.tensor(train_x[train_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "        x_val_fold = torch.tensor(train_x[valid_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "\n",
    "        model = SimpleRNN(c, embeddings)\n",
    "        model.cuda(cuda_idx)\n",
    "\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=c.learning_rate)\n",
    "\n",
    "        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "        valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=c.batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=c.test_batch_size, shuffle=False)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        print(f'Fold {i + 1}')\n",
    "\n",
    "        for epoch in range(c.num_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred = model(x_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), c.clip_grad)\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "            avg_val_loss = 0.\n",
    "\n",
    "            # validation prediction\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred = model(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            search_result = threshold_search(y_val_fold.cpu().numpy(), valid_preds_fold)\n",
    "            valid_pred_targets = valid_preds_fold > search_result['threshold']\n",
    "            val_f1 = f1_score(y_val_fold.cpu().numpy(), valid_pred_targets)\n",
    "\n",
    "            elapsed_time = time.time() - start_time \n",
    "            print('Epoch {}/{}  loss={:.4f}  val_loss={:.4f}  f1={:.3f}  time={:.2f}s'.format(\n",
    "                epoch + 1, c.num_epochs, avg_loss, avg_val_loss, val_f1, elapsed_time))\n",
    "            if best_f1 < val_f1:\n",
    "                print(f'model_saved at f1: {val_f1} from {best_f1}')\n",
    "                ckpt_path = Path(f'./ckpt/do_word/{trial}/')\n",
    "                if not ckpt_path.exists():\n",
    "                    ckpt_path.mkdir(parents=True)\n",
    "                torch.save(model.state_dict(), ckpt_path / f'{i}_model.pt')\n",
    "                best_f1 = val_f1\n",
    "                best_epoch = epoch\n",
    "\n",
    "        # test prediction\n",
    "        model.load_state_dict(torch.load(f'./ckpt/do_word/{trial}/{i}_model.pt'))  # load best model\n",
    "        test_preds_fold = np.zeros(len(test_x))\n",
    "        for i, (x_batch, ) in enumerate(test_loader):\n",
    "            x_batch = cut_length(x_batch, mask)\n",
    "            y_pred = model(x_batch).detach()\n",
    "            test_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "        train_preds[valid_idx] = valid_preds_fold\n",
    "        test_preds += test_preds_fold / len(splits)\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1631  val_loss=0.1271  f1=0.631  time=123.32s\n",
      "model_saved at f1: 0.6307863383637807 from 0.0\n",
      "Epoch 2/15  loss=0.1250  val_loss=0.1106  f1=0.648  time=123.96s\n",
      "model_saved at f1: 0.6483554169914264 from 0.6307863383637807\n",
      "Epoch 3/15  loss=0.1203  val_loss=0.1066  f1=0.662  time=123.73s\n",
      "model_saved at f1: 0.6619619810541039 from 0.6483554169914264\n",
      "Epoch 4/15  loss=0.1178  val_loss=0.1031  f1=0.667  time=123.12s\n",
      "model_saved at f1: 0.6672138910812944 from 0.6619619810541039\n",
      "Epoch 5/15  loss=0.1149  val_loss=0.1022  f1=0.671  time=121.13s\n",
      "model_saved at f1: 0.6709360243224012 from 0.6672138910812944\n",
      "Epoch 6/15  loss=0.1125  val_loss=0.1006  f1=0.675  time=122.90s\n",
      "model_saved at f1: 0.6753008813216667 from 0.6709360243224012\n",
      "Epoch 7/15  loss=0.1113  val_loss=0.0999  f1=0.677  time=122.65s\n",
      "model_saved at f1: 0.6774947941694698 from 0.6753008813216667\n",
      "Epoch 8/15  loss=0.1094  val_loss=0.1039  f1=0.673  time=123.10s\n",
      "Epoch 9/15  loss=0.1074  val_loss=0.1049  f1=0.673  time=122.93s\n",
      "Epoch 10/15  loss=0.1063  val_loss=0.0986  f1=0.680  time=122.47s\n",
      "model_saved at f1: 0.6804633606664063 from 0.6774947941694698\n",
      "Epoch 11/15  loss=0.1050  val_loss=0.1006  f1=0.676  time=123.09s\n",
      "Epoch 12/15  loss=0.1040  val_loss=0.0997  f1=0.676  time=123.13s\n",
      "Epoch 13/15  loss=0.1032  val_loss=0.1046  f1=0.677  time=122.00s\n",
      "Epoch 14/15  loss=0.1021  val_loss=0.1032  f1=0.676  time=123.21s\n",
      "Epoch 15/15  loss=0.1006  val_loss=0.1038  f1=0.674  time=122.91s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1538  val_loss=0.1257  f1=0.640  time=122.95s\n",
      "model_saved at f1: 0.6395378364652107 from 0.0\n",
      "Epoch 2/15  loss=0.1235  val_loss=0.1123  f1=0.653  time=123.84s\n",
      "model_saved at f1: 0.6534792906984188 from 0.6395378364652107\n",
      "Epoch 3/15  loss=0.1192  val_loss=0.1042  f1=0.663  time=122.98s\n",
      "model_saved at f1: 0.6629963080510395 from 0.6534792906984188\n",
      "Epoch 4/15  loss=0.1167  val_loss=0.1025  f1=0.667  time=122.22s\n",
      "model_saved at f1: 0.6673726174162203 from 0.6629963080510395\n",
      "Epoch 5/15  loss=0.1144  val_loss=0.1016  f1=0.675  time=121.99s\n",
      "model_saved at f1: 0.674543433380975 from 0.6673726174162203\n",
      "Epoch 6/15  loss=0.1123  val_loss=0.1006  f1=0.678  time=123.84s\n",
      "model_saved at f1: 0.6779404341241797 from 0.674543433380975\n",
      "Epoch 7/15  loss=0.1105  val_loss=0.1010  f1=0.678  time=122.72s\n",
      "model_saved at f1: 0.6779585040983607 from 0.6779404341241797\n",
      "Epoch 8/15  loss=0.1093  val_loss=0.0996  f1=0.677  time=124.47s\n",
      "Epoch 9/15  loss=0.1073  val_loss=0.0997  f1=0.680  time=123.37s\n",
      "model_saved at f1: 0.6797809249116893 from 0.6779585040983607\n",
      "Epoch 10/15  loss=0.1064  val_loss=0.0997  f1=0.681  time=121.93s\n",
      "model_saved at f1: 0.6813123561013047 from 0.6797809249116893\n",
      "Epoch 11/15  loss=0.1048  val_loss=0.1015  f1=0.681  time=123.40s\n",
      "Epoch 12/15  loss=0.1039  val_loss=0.1014  f1=0.680  time=122.60s\n",
      "Epoch 13/15  loss=0.1031  val_loss=0.0989  f1=0.680  time=122.96s\n",
      "Epoch 14/15  loss=0.1019  val_loss=0.0992  f1=0.676  time=122.98s\n",
      "Epoch 15/15  loss=0.1003  val_loss=0.0988  f1=0.682  time=122.94s\n",
      "model_saved at f1: 0.6817105886065962 from 0.6813123561013047\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1565  val_loss=0.1221  f1=0.637  time=123.20s\n",
      "model_saved at f1: 0.6374358176443131 from 0.0\n",
      "Epoch 2/15  loss=0.1244  val_loss=0.1088  f1=0.656  time=122.55s\n",
      "model_saved at f1: 0.6559206045470762 from 0.6374358176443131\n",
      "Epoch 3/15  loss=0.1197  val_loss=0.1044  f1=0.667  time=122.18s\n",
      "model_saved at f1: 0.6671573019326762 from 0.6559206045470762\n",
      "Epoch 4/15  loss=0.1169  val_loss=0.1021  f1=0.669  time=122.45s\n",
      "model_saved at f1: 0.6691294875438099 from 0.6671573019326762\n",
      "Epoch 5/15  loss=0.1147  val_loss=0.1023  f1=0.674  time=120.64s\n",
      "model_saved at f1: 0.6739851419474662 from 0.6691294875438099\n",
      "Epoch 6/15  loss=0.1130  val_loss=0.1003  f1=0.679  time=122.59s\n",
      "model_saved at f1: 0.6785018823834871 from 0.6739851419474662\n",
      "Epoch 7/15  loss=0.1109  val_loss=0.1004  f1=0.679  time=122.77s\n",
      "model_saved at f1: 0.6794976967249322 from 0.6785018823834871\n",
      "Epoch 8/15  loss=0.1096  val_loss=0.0991  f1=0.683  time=123.01s\n",
      "model_saved at f1: 0.6831066314381706 from 0.6794976967249322\n",
      "Epoch 9/15  loss=0.1077  val_loss=0.1016  f1=0.682  time=122.60s\n",
      "Epoch 10/15  loss=0.1066  val_loss=0.0993  f1=0.684  time=122.69s\n",
      "model_saved at f1: 0.6842022365726886 from 0.6831066314381706\n",
      "Epoch 11/15  loss=0.1054  val_loss=0.0987  f1=0.685  time=122.78s\n",
      "model_saved at f1: 0.6845391833131722 from 0.6842022365726886\n",
      "Epoch 12/15  loss=0.1044  val_loss=0.0989  f1=0.685  time=122.73s\n",
      "Epoch 13/15  loss=0.1032  val_loss=0.0989  f1=0.682  time=122.26s\n",
      "Epoch 14/15  loss=0.1025  val_loss=0.1002  f1=0.684  time=123.23s\n",
      "Epoch 15/15  loss=0.1010  val_loss=0.0984  f1=0.685  time=122.77s\n",
      "model_saved at f1: 0.6845532690804897 from 0.6845391833131722\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1509  val_loss=0.1214  f1=0.629  time=122.89s\n",
      "model_saved at f1: 0.6287735410008997 from 0.0\n",
      "Epoch 2/15  loss=0.1253  val_loss=0.1119  f1=0.647  time=123.50s\n",
      "model_saved at f1: 0.647069931390445 from 0.6287735410008997\n",
      "Epoch 3/15  loss=0.1206  val_loss=0.1068  f1=0.663  time=123.09s\n",
      "model_saved at f1: 0.6633656915759985 from 0.647069931390445\n",
      "Epoch 4/15  loss=0.1177  val_loss=0.1037  f1=0.672  time=123.14s\n",
      "model_saved at f1: 0.6718234189915204 from 0.6633656915759985\n",
      "Epoch 5/15  loss=0.1149  val_loss=0.1030  f1=0.676  time=122.08s\n",
      "model_saved at f1: 0.6756297128188673 from 0.6718234189915204\n",
      "Epoch 6/15  loss=0.1134  val_loss=0.1014  f1=0.675  time=122.62s\n",
      "Epoch 7/15  loss=0.1113  val_loss=0.1003  f1=0.679  time=123.32s\n",
      "model_saved at f1: 0.6785462921492098 from 0.6756297128188673\n",
      "Epoch 8/15  loss=0.1098  val_loss=0.0998  f1=0.681  time=122.82s\n",
      "model_saved at f1: 0.6813150935388032 from 0.6785462921492098\n",
      "Epoch 9/15  loss=0.1088  val_loss=0.0988  f1=0.681  time=122.00s\n",
      "Epoch 10/15  loss=0.1072  val_loss=0.0991  f1=0.681  time=123.05s\n",
      "model_saved at f1: 0.6813727055067836 from 0.6813150935388032\n",
      "Epoch 11/15  loss=0.1058  val_loss=0.0981  f1=0.683  time=123.06s\n",
      "model_saved at f1: 0.6828324519540823 from 0.6813727055067836\n",
      "Epoch 12/15  loss=0.1045  val_loss=0.1032  f1=0.683  time=122.62s\n",
      "model_saved at f1: 0.6828329739591972 from 0.6828324519540823\n",
      "Epoch 13/15  loss=0.1041  val_loss=0.1001  f1=0.682  time=122.64s\n",
      "Epoch 14/15  loss=0.1031  val_loss=0.0988  f1=0.681  time=122.54s\n",
      "Epoch 15/15  loss=0.1018  val_loss=0.0985  f1=0.682  time=122.54s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1597  val_loss=0.1156  f1=0.636  time=122.80s\n",
      "model_saved at f1: 0.6357127359513994 from 0.0\n",
      "Epoch 2/15  loss=0.1240  val_loss=0.1070  f1=0.657  time=122.77s\n",
      "model_saved at f1: 0.6571726520237241 from 0.6357127359513994\n",
      "Epoch 3/15  loss=0.1201  val_loss=0.1088  f1=0.662  time=123.25s\n",
      "model_saved at f1: 0.662160881786312 from 0.6571726520237241\n",
      "Epoch 4/15  loss=0.1167  val_loss=0.1027  f1=0.671  time=123.55s\n",
      "model_saved at f1: 0.6711483145341133 from 0.662160881786312\n",
      "Epoch 5/15  loss=0.1144  val_loss=0.1022  f1=0.678  time=121.21s\n",
      "model_saved at f1: 0.6780557914679557 from 0.6711483145341133\n",
      "Epoch 6/15  loss=0.1127  val_loss=0.1011  f1=0.678  time=123.13s\n",
      "Epoch 7/15  loss=0.1106  val_loss=0.1006  f1=0.681  time=122.74s\n",
      "model_saved at f1: 0.6809074350057955 from 0.6780557914679557\n",
      "Epoch 8/15  loss=0.1092  val_loss=0.1011  f1=0.680  time=123.18s\n",
      "Epoch 9/15  loss=0.1073  val_loss=0.0997  f1=0.682  time=123.86s\n",
      "model_saved at f1: 0.6818226359627634 from 0.6809074350057955\n",
      "Epoch 10/15  loss=0.1066  val_loss=0.0991  f1=0.682  time=122.87s\n",
      "model_saved at f1: 0.6820120462314829 from 0.6818226359627634\n",
      "Epoch 11/15  loss=0.1055  val_loss=0.1009  f1=0.685  time=123.07s\n",
      "model_saved at f1: 0.6845395921306754 from 0.6820120462314829\n",
      "Epoch 12/15  loss=0.1041  val_loss=0.0980  f1=0.683  time=123.67s\n",
      "Epoch 13/15  loss=0.1026  val_loss=0.0991  f1=0.686  time=123.75s\n",
      "model_saved at f1: 0.6856516976998905 from 0.6845395921306754\n",
      "Epoch 14/15  loss=0.1018  val_loss=0.1012  f1=0.683  time=123.43s\n",
      "Epoch 15/15  loss=0.1010  val_loss=0.1018  f1=0.681  time=123.02s\n",
      "{'threshold': 0.2643725574016571, 'f1': 0.679380245153751}\n",
      "f1 score: 0.6955124526569494\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1589  val_loss=0.1165  f1=0.636  time=122.28s\n",
      "model_saved at f1: 0.6360558210183221 from 0.0\n",
      "Epoch 2/15  loss=0.1255  val_loss=0.1104  f1=0.651  time=122.16s\n",
      "model_saved at f1: 0.6509472622387542 from 0.6360558210183221\n",
      "Epoch 3/15  loss=0.1207  val_loss=0.1039  f1=0.664  time=122.83s\n",
      "model_saved at f1: 0.6640859382560728 from 0.6509472622387542\n",
      "Epoch 4/15  loss=0.1177  val_loss=0.1024  f1=0.668  time=122.66s\n",
      "model_saved at f1: 0.6681037256289813 from 0.6640859382560728\n",
      "Epoch 5/15  loss=0.1151  val_loss=0.1027  f1=0.670  time=120.88s\n",
      "model_saved at f1: 0.6701239564887427 from 0.6681037256289813\n",
      "Epoch 6/15  loss=0.1132  val_loss=0.1013  f1=0.673  time=122.73s\n",
      "model_saved at f1: 0.673383208645054 from 0.6701239564887427\n",
      "Epoch 7/15  loss=0.1112  val_loss=0.1007  f1=0.677  time=122.60s\n",
      "model_saved at f1: 0.6774318064848173 from 0.673383208645054\n",
      "Epoch 8/15  loss=0.1097  val_loss=0.1006  f1=0.679  time=122.01s\n",
      "model_saved at f1: 0.6786930330015716 from 0.6774318064848173\n",
      "Epoch 9/15  loss=0.1082  val_loss=0.1001  f1=0.680  time=122.29s\n",
      "model_saved at f1: 0.6795583719866729 from 0.6786930330015716\n",
      "Epoch 10/15  loss=0.1065  val_loss=0.0993  f1=0.681  time=122.16s\n",
      "model_saved at f1: 0.6806671182941385 from 0.6795583719866729\n",
      "Epoch 11/15  loss=0.1053  val_loss=0.0989  f1=0.679  time=122.49s\n",
      "Epoch 12/15  loss=0.1042  val_loss=0.0983  f1=0.681  time=122.79s\n",
      "model_saved at f1: 0.6813545763615759 from 0.6806671182941385\n",
      "Epoch 13/15  loss=0.1031  val_loss=0.0986  f1=0.679  time=122.84s\n",
      "Epoch 14/15  loss=0.1024  val_loss=0.0992  f1=0.679  time=122.68s\n",
      "Epoch 15/15  loss=0.1012  val_loss=0.0992  f1=0.679  time=123.27s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1670  val_loss=0.1214  f1=0.636  time=123.49s\n",
      "model_saved at f1: 0.6360447198452509 from 0.0\n",
      "Epoch 2/15  loss=0.1242  val_loss=0.1106  f1=0.656  time=123.57s\n",
      "model_saved at f1: 0.6556850393700787 from 0.6360447198452509\n",
      "Epoch 3/15  loss=0.1197  val_loss=0.1054  f1=0.667  time=123.96s\n",
      "model_saved at f1: 0.666981607457798 from 0.6556850393700787\n",
      "Epoch 4/15  loss=0.1173  val_loss=0.1039  f1=0.668  time=123.64s\n",
      "model_saved at f1: 0.667616764915463 from 0.666981607457798\n",
      "Epoch 5/15  loss=0.1147  val_loss=0.1022  f1=0.674  time=121.66s\n",
      "model_saved at f1: 0.6744800051516517 from 0.667616764915463\n",
      "Epoch 6/15  loss=0.1124  val_loss=0.1021  f1=0.675  time=123.75s\n",
      "model_saved at f1: 0.6752092723760463 from 0.6744800051516517\n",
      "Epoch 7/15  loss=0.1110  val_loss=0.1012  f1=0.679  time=123.62s\n",
      "model_saved at f1: 0.6785888473418956 from 0.6752092723760463\n",
      "Epoch 8/15  loss=0.1096  val_loss=0.0991  f1=0.682  time=123.56s\n",
      "model_saved at f1: 0.6824818694601129 from 0.6785888473418956\n",
      "Epoch 9/15  loss=0.1079  val_loss=0.1015  f1=0.682  time=123.26s\n",
      "Epoch 10/15  loss=0.1066  val_loss=0.0999  f1=0.685  time=122.49s\n",
      "model_saved at f1: 0.6845120569671469 from 0.6824818694601129\n",
      "Epoch 11/15  loss=0.1052  val_loss=0.1011  f1=0.680  time=123.03s\n",
      "Epoch 12/15  loss=0.1046  val_loss=0.1026  f1=0.681  time=123.36s\n",
      "Epoch 13/15  loss=0.1028  val_loss=0.0992  f1=0.683  time=122.70s\n",
      "Epoch 14/15  loss=0.1018  val_loss=0.1018  f1=0.681  time=122.83s\n",
      "Epoch 15/15  loss=0.1006  val_loss=0.1002  f1=0.683  time=123.16s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1579  val_loss=0.1172  f1=0.638  time=121.98s\n",
      "model_saved at f1: 0.6381738064473379 from 0.0\n",
      "Epoch 2/15  loss=0.1245  val_loss=0.1089  f1=0.655  time=122.41s\n",
      "model_saved at f1: 0.6547653698133954 from 0.6381738064473379\n",
      "Epoch 3/15  loss=0.1204  val_loss=0.1049  f1=0.667  time=122.72s\n",
      "model_saved at f1: 0.6668994856816306 from 0.6547653698133954\n",
      "Epoch 4/15  loss=0.1171  val_loss=0.1030  f1=0.672  time=122.58s\n",
      "model_saved at f1: 0.6718211942044051 from 0.6668994856816306\n",
      "Epoch 5/15  loss=0.1147  val_loss=0.1036  f1=0.673  time=120.33s\n",
      "model_saved at f1: 0.6730732021928411 from 0.6718211942044051\n",
      "Epoch 6/15  loss=0.1130  val_loss=0.1008  f1=0.677  time=122.51s\n",
      "model_saved at f1: 0.6774298973789136 from 0.6730732021928411\n",
      "Epoch 7/15  loss=0.1111  val_loss=0.1019  f1=0.674  time=122.42s\n",
      "Epoch 8/15  loss=0.1097  val_loss=0.0997  f1=0.685  time=122.84s\n",
      "model_saved at f1: 0.6849697836116707 from 0.6774298973789136\n",
      "Epoch 9/15  loss=0.1081  val_loss=0.0995  f1=0.682  time=122.42s\n",
      "Epoch 10/15  loss=0.1066  val_loss=0.0993  f1=0.683  time=122.54s\n",
      "Epoch 11/15  loss=0.1054  val_loss=0.0998  f1=0.687  time=122.67s\n",
      "model_saved at f1: 0.6865887773572487 from 0.6849697836116707\n",
      "Epoch 12/15  loss=0.1043  val_loss=0.1007  f1=0.681  time=121.94s\n",
      "Epoch 13/15  loss=0.1033  val_loss=0.0993  f1=0.680  time=122.89s\n",
      "Epoch 14/15  loss=0.1021  val_loss=0.1011  f1=0.684  time=122.35s\n",
      "Epoch 15/15  loss=0.1009  val_loss=0.1003  f1=0.682  time=122.29s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1686  val_loss=0.1190  f1=0.634  time=123.40s\n",
      "model_saved at f1: 0.633995755354042 from 0.0\n",
      "Epoch 2/15  loss=0.1256  val_loss=0.1077  f1=0.657  time=123.43s\n",
      "model_saved at f1: 0.657333716879215 from 0.633995755354042\n",
      "Epoch 3/15  loss=0.1205  val_loss=0.1052  f1=0.666  time=122.93s\n",
      "model_saved at f1: 0.6659604519774013 from 0.657333716879215\n",
      "Epoch 4/15  loss=0.1174  val_loss=0.1047  f1=0.673  time=122.96s\n",
      "model_saved at f1: 0.6725317065339541 from 0.6659604519774013\n",
      "Epoch 5/15  loss=0.1149  val_loss=0.1009  f1=0.680  time=121.38s\n",
      "model_saved at f1: 0.6799076813054644 from 0.6725317065339541\n",
      "Epoch 6/15  loss=0.1130  val_loss=0.1006  f1=0.680  time=123.14s\n",
      "model_saved at f1: 0.6799260244638397 from 0.6799076813054644\n",
      "Epoch 7/15  loss=0.1111  val_loss=0.1010  f1=0.679  time=123.74s\n",
      "Epoch 8/15  loss=0.1094  val_loss=0.0991  f1=0.686  time=124.26s\n",
      "model_saved at f1: 0.6863313477526837 from 0.6799260244638397\n",
      "Epoch 9/15  loss=0.1077  val_loss=0.1001  f1=0.683  time=123.37s\n",
      "Epoch 10/15  loss=0.1068  val_loss=0.0985  f1=0.684  time=123.42s\n",
      "Epoch 11/15  loss=0.1058  val_loss=0.0989  f1=0.687  time=123.11s\n",
      "model_saved at f1: 0.6865891547355947 from 0.6863313477526837\n",
      "Epoch 12/15  loss=0.1043  val_loss=0.0979  f1=0.685  time=123.59s\n",
      "Epoch 13/15  loss=0.1034  val_loss=0.0978  f1=0.687  time=123.26s\n",
      "model_saved at f1: 0.6866028708133972 from 0.6865891547355947\n",
      "Epoch 14/15  loss=0.1020  val_loss=0.0975  f1=0.686  time=123.69s\n",
      "Epoch 15/15  loss=0.1010  val_loss=0.1006  f1=0.685  time=123.56s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1656  val_loss=0.1219  f1=0.630  time=123.03s\n",
      "model_saved at f1: 0.6301991751654464 from 0.0\n",
      "Epoch 2/15  loss=0.1248  val_loss=0.1145  f1=0.640  time=122.64s\n",
      "model_saved at f1: 0.6403219672962353 from 0.6301991751654464\n",
      "Epoch 3/15  loss=0.1201  val_loss=0.1047  f1=0.660  time=122.93s\n",
      "model_saved at f1: 0.6602804474554907 from 0.6403219672962353\n",
      "Epoch 4/15  loss=0.1173  val_loss=0.1039  f1=0.667  time=122.89s\n",
      "model_saved at f1: 0.6674694977024244 from 0.6602804474554907\n",
      "Epoch 5/15  loss=0.1144  val_loss=0.1019  f1=0.670  time=121.05s\n",
      "model_saved at f1: 0.6700707515685489 from 0.6674694977024244\n",
      "Epoch 6/15  loss=0.1125  val_loss=0.1037  f1=0.676  time=123.12s\n",
      "model_saved at f1: 0.6755278555075044 from 0.6700707515685489\n",
      "Epoch 7/15  loss=0.1108  val_loss=0.1021  f1=0.677  time=122.84s\n",
      "model_saved at f1: 0.6768124317729404 from 0.6755278555075044\n",
      "Epoch 8/15  loss=0.1093  val_loss=0.1021  f1=0.679  time=121.71s\n",
      "model_saved at f1: 0.6787724950372613 from 0.6768124317729404\n",
      "Epoch 9/15  loss=0.1081  val_loss=0.1007  f1=0.681  time=122.90s\n",
      "model_saved at f1: 0.6811036252807187 from 0.6787724950372613\n",
      "Epoch 10/15  loss=0.1071  val_loss=0.1004  f1=0.681  time=122.57s\n",
      "Epoch 11/15  loss=0.1054  val_loss=0.1005  f1=0.680  time=122.76s\n",
      "Epoch 12/15  loss=0.1046  val_loss=0.1002  f1=0.681  time=123.60s\n",
      "model_saved at f1: 0.681134078561169 from 0.6811036252807187\n",
      "Epoch 13/15  loss=0.1029  val_loss=0.1011  f1=0.683  time=123.03s\n",
      "model_saved at f1: 0.6834887547997806 from 0.681134078561169\n",
      "Epoch 14/15  loss=0.1021  val_loss=0.1028  f1=0.681  time=122.59s\n",
      "Epoch 15/15  loss=0.1016  val_loss=0.0998  f1=0.681  time=122.99s\n",
      "{'threshold': 0.2750913202762604, 'f1': 0.6810534016093636}\n",
      "f1 score: 0.6978160313835705\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1872  val_loss=0.1211  f1=0.629  time=123.67s\n",
      "model_saved at f1: 0.6285639378645868 from 0.0\n",
      "Epoch 2/15  loss=0.1263  val_loss=0.1111  f1=0.650  time=122.44s\n",
      "model_saved at f1: 0.6501215623125256 from 0.6285639378645868\n",
      "Epoch 3/15  loss=0.1208  val_loss=0.1068  f1=0.656  time=122.92s\n",
      "model_saved at f1: 0.6558681262729125 from 0.6501215623125256\n",
      "Epoch 4/15  loss=0.1178  val_loss=0.1036  f1=0.669  time=123.55s\n",
      "model_saved at f1: 0.6691466649414506 from 0.6558681262729125\n",
      "Epoch 5/15  loss=0.1155  val_loss=0.1039  f1=0.669  time=120.68s\n",
      "Epoch 6/15  loss=0.1134  val_loss=0.1006  f1=0.677  time=122.70s\n",
      "model_saved at f1: 0.6765601965601966 from 0.6691466649414506\n",
      "Epoch 7/15  loss=0.1120  val_loss=0.0993  f1=0.680  time=123.08s\n",
      "model_saved at f1: 0.6798235582421173 from 0.6765601965601966\n",
      "Epoch 8/15  loss=0.1100  val_loss=0.1013  f1=0.680  time=123.12s\n",
      "model_saved at f1: 0.6798852074093399 from 0.6798235582421173\n",
      "Epoch 9/15  loss=0.1088  val_loss=0.1023  f1=0.680  time=123.44s\n",
      "model_saved at f1: 0.6804508814470148 from 0.6798852074093399\n",
      "Epoch 10/15  loss=0.1073  val_loss=0.1001  f1=0.682  time=123.36s\n",
      "model_saved at f1: 0.68184016248106 from 0.6804508814470148\n",
      "Epoch 11/15  loss=0.1063  val_loss=0.0988  f1=0.683  time=122.97s\n",
      "model_saved at f1: 0.682578486875965 from 0.68184016248106\n",
      "Epoch 12/15  loss=0.1044  val_loss=0.0985  f1=0.680  time=122.99s\n",
      "Epoch 13/15  loss=0.1038  val_loss=0.1003  f1=0.679  time=123.40s\n",
      "Epoch 14/15  loss=0.1025  val_loss=0.0995  f1=0.681  time=122.63s\n",
      "Epoch 15/15  loss=0.1016  val_loss=0.0999  f1=0.681  time=122.98s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1528  val_loss=0.1431  f1=0.637  time=123.19s\n",
      "model_saved at f1: 0.6365639469902602 from 0.0\n",
      "Epoch 2/15  loss=0.1247  val_loss=0.1086  f1=0.657  time=123.07s\n",
      "model_saved at f1: 0.6573925112772468 from 0.6365639469902602\n",
      "Epoch 3/15  loss=0.1200  val_loss=0.1057  f1=0.662  time=123.39s\n",
      "model_saved at f1: 0.6623730964467004 from 0.6573925112772468\n",
      "Epoch 4/15  loss=0.1170  val_loss=0.1017  f1=0.668  time=123.25s\n",
      "model_saved at f1: 0.6677502194194325 from 0.6623730964467004\n",
      "Epoch 5/15  loss=0.1145  val_loss=0.1026  f1=0.675  time=120.33s\n",
      "model_saved at f1: 0.6745738544299695 from 0.6677502194194325\n",
      "Epoch 6/15  loss=0.1124  val_loss=0.1017  f1=0.675  time=122.43s\n",
      "model_saved at f1: 0.6751659206667695 from 0.6745738544299695\n",
      "Epoch 7/15  loss=0.1111  val_loss=0.1004  f1=0.679  time=122.93s\n",
      "model_saved at f1: 0.6790917443234021 from 0.6751659206667695\n",
      "Epoch 8/15  loss=0.1094  val_loss=0.0995  f1=0.678  time=123.37s\n",
      "Epoch 9/15  loss=0.1080  val_loss=0.1004  f1=0.680  time=123.26s\n",
      "model_saved at f1: 0.68014381228712 from 0.6790917443234021\n",
      "Epoch 10/15  loss=0.1067  val_loss=0.1007  f1=0.675  time=123.24s\n",
      "Epoch 11/15  loss=0.1051  val_loss=0.1000  f1=0.680  time=122.70s\n",
      "Epoch 12/15  loss=0.1043  val_loss=0.0994  f1=0.681  time=123.47s\n",
      "model_saved at f1: 0.6809989142236699 from 0.68014381228712\n",
      "Epoch 13/15  loss=0.1031  val_loss=0.1026  f1=0.679  time=123.04s\n",
      "Epoch 14/15  loss=0.1019  val_loss=0.0992  f1=0.681  time=122.55s\n",
      "model_saved at f1: 0.6812249644656934 from 0.6809989142236699\n",
      "Epoch 15/15  loss=0.1009  val_loss=0.0993  f1=0.678  time=123.25s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1675  val_loss=0.1205  f1=0.636  time=122.93s\n",
      "model_saved at f1: 0.6364263322884013 from 0.0\n",
      "Epoch 2/15  loss=0.1240  val_loss=0.1088  f1=0.656  time=122.45s\n",
      "model_saved at f1: 0.656290104972194 from 0.6364263322884013\n",
      "Epoch 3/15  loss=0.1197  val_loss=0.1058  f1=0.664  time=123.11s\n",
      "model_saved at f1: 0.6644605795797726 from 0.656290104972194\n",
      "Epoch 4/15  loss=0.1165  val_loss=0.1031  f1=0.667  time=122.80s\n",
      "model_saved at f1: 0.6673053586255349 from 0.6644605795797726\n",
      "Epoch 5/15  loss=0.1143  val_loss=0.1049  f1=0.673  time=121.70s\n",
      "model_saved at f1: 0.672873174095668 from 0.6673053586255349\n",
      "Epoch 6/15  loss=0.1123  val_loss=0.0998  f1=0.675  time=122.88s\n",
      "model_saved at f1: 0.6747123561780891 from 0.672873174095668\n",
      "Epoch 7/15  loss=0.1112  val_loss=0.1001  f1=0.678  time=122.98s\n",
      "model_saved at f1: 0.6782079999999999 from 0.6747123561780891\n",
      "Epoch 8/15  loss=0.1092  val_loss=0.0996  f1=0.677  time=122.64s\n",
      "Epoch 9/15  loss=0.1079  val_loss=0.0991  f1=0.683  time=122.88s\n",
      "model_saved at f1: 0.6828398599172237 from 0.6782079999999999\n",
      "Epoch 10/15  loss=0.1063  val_loss=0.1018  f1=0.681  time=122.69s\n",
      "Epoch 11/15  loss=0.1054  val_loss=0.1000  f1=0.683  time=122.85s\n",
      "model_saved at f1: 0.6833150737900368 from 0.6828398599172237\n",
      "Epoch 12/15  loss=0.1045  val_loss=0.0987  f1=0.684  time=122.88s\n",
      "model_saved at f1: 0.6842830146375836 from 0.6833150737900368\n",
      "Epoch 13/15  loss=0.1036  val_loss=0.1015  f1=0.682  time=122.38s\n",
      "Epoch 14/15  loss=0.1023  val_loss=0.0985  f1=0.685  time=122.36s\n",
      "model_saved at f1: 0.685302947157752 from 0.6842830146375836\n",
      "Epoch 15/15  loss=0.1010  val_loss=0.0991  f1=0.677  time=122.68s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1781  val_loss=0.1200  f1=0.632  time=122.21s\n",
      "model_saved at f1: 0.6319009022786359 from 0.0\n",
      "Epoch 2/15  loss=0.1248  val_loss=0.1148  f1=0.662  time=121.97s\n",
      "model_saved at f1: 0.6621995250951436 from 0.6319009022786359\n",
      "Epoch 3/15  loss=0.1199  val_loss=0.1046  f1=0.663  time=122.73s\n",
      "model_saved at f1: 0.663394242644963 from 0.6621995250951436\n",
      "Epoch 4/15  loss=0.1169  val_loss=0.1016  f1=0.673  time=121.98s\n",
      "model_saved at f1: 0.6730320932460483 from 0.663394242644963\n",
      "Epoch 5/15  loss=0.1144  val_loss=0.1041  f1=0.680  time=122.69s\n",
      "model_saved at f1: 0.6801908533220812 from 0.6730320932460483\n",
      "Epoch 6/15  loss=0.1128  val_loss=0.1007  f1=0.679  time=123.29s\n",
      "Epoch 7/15  loss=0.1108  val_loss=0.1007  f1=0.679  time=122.61s\n",
      "Epoch 8/15  loss=0.1097  val_loss=0.1002  f1=0.680  time=122.72s\n",
      "Epoch 9/15  loss=0.1081  val_loss=0.1000  f1=0.681  time=123.65s\n",
      "model_saved at f1: 0.6809054049739808 from 0.6801908533220812\n",
      "Epoch 10/15  loss=0.1067  val_loss=0.0989  f1=0.683  time=122.45s\n",
      "model_saved at f1: 0.6833799376466428 from 0.6809054049739808\n",
      "Epoch 11/15  loss=0.1054  val_loss=0.1025  f1=0.681  time=122.90s\n",
      "Epoch 12/15  loss=0.1045  val_loss=0.0993  f1=0.681  time=123.01s\n",
      "Epoch 13/15  loss=0.1029  val_loss=0.0999  f1=0.682  time=122.65s\n",
      "Epoch 14/15  loss=0.1024  val_loss=0.0995  f1=0.682  time=122.55s\n",
      "Epoch 15/15  loss=0.1014  val_loss=0.1015  f1=0.681  time=123.33s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1783  val_loss=0.1200  f1=0.629  time=123.09s\n",
      "model_saved at f1: 0.6293433867420672 from 0.0\n",
      "Epoch 2/15  loss=0.1247  val_loss=0.1094  f1=0.653  time=122.35s\n",
      "model_saved at f1: 0.6527983173945906 from 0.6293433867420672\n",
      "Epoch 3/15  loss=0.1207  val_loss=0.1024  f1=0.669  time=122.83s\n",
      "model_saved at f1: 0.669010450264482 from 0.6527983173945906\n",
      "Epoch 4/15  loss=0.1171  val_loss=0.1033  f1=0.671  time=122.71s\n",
      "model_saved at f1: 0.6712141882673943 from 0.669010450264482\n",
      "Epoch 5/15  loss=0.1151  val_loss=0.1021  f1=0.672  time=123.02s\n",
      "model_saved at f1: 0.6717667058977377 from 0.6712141882673943\n",
      "Epoch 6/15  loss=0.1130  val_loss=0.1015  f1=0.681  time=122.34s\n",
      "model_saved at f1: 0.6814553614361276 from 0.6717667058977377\n",
      "Epoch 7/15  loss=0.1112  val_loss=0.0997  f1=0.682  time=122.94s\n",
      "model_saved at f1: 0.6818082414320973 from 0.6814553614361276\n",
      "Epoch 8/15  loss=0.1095  val_loss=0.0997  f1=0.681  time=122.44s\n",
      "Epoch 9/15  loss=0.1086  val_loss=0.0989  f1=0.685  time=123.58s\n",
      "model_saved at f1: 0.6846482355585525 from 0.6818082414320973\n",
      "Epoch 10/15  loss=0.1072  val_loss=0.1009  f1=0.682  time=123.03s\n",
      "Epoch 11/15  loss=0.1061  val_loss=0.1016  f1=0.682  time=122.39s\n",
      "Epoch 12/15  loss=0.1045  val_loss=0.0985  f1=0.685  time=122.92s\n",
      "model_saved at f1: 0.6852108812804544 from 0.6846482355585525\n",
      "Epoch 13/15  loss=0.1036  val_loss=0.1000  f1=0.685  time=123.06s\n",
      "Epoch 14/15  loss=0.1022  val_loss=0.1006  f1=0.685  time=122.97s\n",
      "Epoch 15/15  loss=0.1014  val_loss=0.1009  f1=0.683  time=122.81s\n",
      "{'threshold': 0.2617061138153076, 'f1': 0.6795447187501983}\n",
      "f1 score: 0.6959024720623096\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1586  val_loss=0.1177  f1=0.641  time=122.23s\n",
      "model_saved at f1: 0.641182179643718 from 0.0\n",
      "Epoch 2/15  loss=0.1245  val_loss=0.1082  f1=0.658  time=121.86s\n",
      "model_saved at f1: 0.6581174984207202 from 0.641182179643718\n",
      "Epoch 3/15  loss=0.1199  val_loss=0.1055  f1=0.671  time=122.79s\n",
      "model_saved at f1: 0.6706529690531472 from 0.6581174984207202\n",
      "Epoch 4/15  loss=0.1168  val_loss=0.1023  f1=0.677  time=121.33s\n",
      "model_saved at f1: 0.6768540215848707 from 0.6706529690531472\n",
      "Epoch 5/15  loss=0.1148  val_loss=0.1009  f1=0.680  time=122.72s\n",
      "model_saved at f1: 0.6796249267435045 from 0.6768540215848707\n",
      "Epoch 6/15  loss=0.1125  val_loss=0.1012  f1=0.678  time=123.12s\n",
      "Epoch 7/15  loss=0.1107  val_loss=0.0998  f1=0.683  time=122.58s\n",
      "model_saved at f1: 0.682889146388567 from 0.6796249267435045\n",
      "Epoch 8/15  loss=0.1097  val_loss=0.1000  f1=0.680  time=122.46s\n",
      "Epoch 9/15  loss=0.1080  val_loss=0.1007  f1=0.681  time=123.00s\n",
      "Epoch 10/15  loss=0.1068  val_loss=0.1006  f1=0.686  time=122.80s\n",
      "model_saved at f1: 0.6859943092001265 from 0.682889146388567\n",
      "Epoch 11/15  loss=0.1053  val_loss=0.0994  f1=0.686  time=122.24s\n",
      "Epoch 12/15  loss=0.1041  val_loss=0.1002  f1=0.684  time=123.42s\n",
      "Epoch 13/15  loss=0.1035  val_loss=0.0996  f1=0.685  time=122.95s\n",
      "Epoch 14/15  loss=0.1023  val_loss=0.1001  f1=0.686  time=122.76s\n",
      "Epoch 15/15  loss=0.1014  val_loss=0.1015  f1=0.683  time=123.68s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1576  val_loss=0.1178  f1=0.641  time=123.47s\n",
      "model_saved at f1: 0.6414830804393372 from 0.0\n",
      "Epoch 2/15  loss=0.1242  val_loss=0.1071  f1=0.661  time=123.58s\n",
      "model_saved at f1: 0.6605498721227621 from 0.6414830804393372\n",
      "Epoch 3/15  loss=0.1195  val_loss=0.1052  f1=0.668  time=123.19s\n",
      "model_saved at f1: 0.6680848320763231 from 0.6605498721227621\n",
      "Epoch 4/15  loss=0.1168  val_loss=0.1017  f1=0.674  time=122.41s\n",
      "model_saved at f1: 0.6736307534070456 from 0.6680848320763231\n",
      "Epoch 5/15  loss=0.1142  val_loss=0.1011  f1=0.673  time=123.02s\n",
      "Epoch 6/15  loss=0.1128  val_loss=0.1003  f1=0.679  time=122.98s\n",
      "model_saved at f1: 0.6788244638602065 from 0.6736307534070456\n",
      "Epoch 7/15  loss=0.1107  val_loss=0.0994  f1=0.679  time=122.70s\n",
      "model_saved at f1: 0.6791440514068251 from 0.6788244638602065\n",
      "Epoch 8/15  loss=0.1093  val_loss=0.1006  f1=0.679  time=122.75s\n",
      "Epoch 9/15  loss=0.1082  val_loss=0.1002  f1=0.684  time=122.59s\n",
      "model_saved at f1: 0.6835132165708008 from 0.6791440514068251\n",
      "Epoch 10/15  loss=0.1065  val_loss=0.0996  f1=0.682  time=123.10s\n",
      "Epoch 11/15  loss=0.1051  val_loss=0.0990  f1=0.684  time=123.16s\n",
      "model_saved at f1: 0.6843642500162264 from 0.6835132165708008\n",
      "Epoch 12/15  loss=0.1047  val_loss=0.1000  f1=0.681  time=122.68s\n",
      "Epoch 13/15  loss=0.1028  val_loss=0.0984  f1=0.680  time=123.31s\n",
      "Epoch 14/15  loss=0.1019  val_loss=0.1015  f1=0.682  time=122.66s\n",
      "Epoch 15/15  loss=0.1010  val_loss=0.1005  f1=0.681  time=122.88s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1631  val_loss=0.1232  f1=0.635  time=123.79s\n",
      "model_saved at f1: 0.6349887729494123 from 0.0\n",
      "Epoch 2/15  loss=0.1236  val_loss=0.1081  f1=0.657  time=123.18s\n",
      "model_saved at f1: 0.6574814169750879 from 0.6349887729494123\n",
      "Epoch 3/15  loss=0.1195  val_loss=0.1066  f1=0.665  time=123.06s\n",
      "model_saved at f1: 0.6648323190470025 from 0.6574814169750879\n",
      "Epoch 4/15  loss=0.1167  val_loss=0.1034  f1=0.671  time=121.36s\n",
      "model_saved at f1: 0.6709606775949245 from 0.6648323190470025\n",
      "Epoch 5/15  loss=0.1147  val_loss=0.1021  f1=0.675  time=122.88s\n",
      "model_saved at f1: 0.6748781280467988 from 0.6709606775949245\n",
      "Epoch 6/15  loss=0.1125  val_loss=0.1003  f1=0.677  time=123.45s\n",
      "model_saved at f1: 0.676797385620915 from 0.6748781280467988\n",
      "Epoch 7/15  loss=0.1106  val_loss=0.0995  f1=0.678  time=123.04s\n",
      "model_saved at f1: 0.6784773152998699 from 0.676797385620915\n",
      "Epoch 8/15  loss=0.1089  val_loss=0.0992  f1=0.679  time=122.60s\n",
      "model_saved at f1: 0.6787573141951961 from 0.6784773152998699\n",
      "Epoch 9/15  loss=0.1078  val_loss=0.0995  f1=0.681  time=123.01s\n",
      "model_saved at f1: 0.6810056772100567 from 0.6787573141951961\n",
      "Epoch 10/15  loss=0.1063  val_loss=0.1008  f1=0.681  time=122.78s\n",
      "Epoch 11/15  loss=0.1049  val_loss=0.0992  f1=0.683  time=122.85s\n",
      "model_saved at f1: 0.6829237315044135 from 0.6810056772100567\n",
      "Epoch 12/15  loss=0.1043  val_loss=0.1007  f1=0.682  time=123.02s\n",
      "Epoch 13/15  loss=0.1024  val_loss=0.0998  f1=0.683  time=123.00s\n",
      "model_saved at f1: 0.683067461080146 from 0.6829237315044135\n",
      "Epoch 14/15  loss=0.1016  val_loss=0.1072  f1=0.678  time=122.52s\n",
      "Epoch 15/15  loss=0.1006  val_loss=0.1015  f1=0.681  time=122.89s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1654  val_loss=0.1370  f1=0.618  time=123.51s\n",
      "model_saved at f1: 0.6176433282597666 from 0.0\n",
      "Epoch 2/15  loss=0.1248  val_loss=0.1120  f1=0.649  time=123.45s\n",
      "model_saved at f1: 0.6494939601697681 from 0.6176433282597666\n",
      "Epoch 3/15  loss=0.1202  val_loss=0.1064  f1=0.658  time=123.53s\n",
      "model_saved at f1: 0.6579994178713495 from 0.6494939601697681\n",
      "Epoch 4/15  loss=0.1169  val_loss=0.1045  f1=0.668  time=121.47s\n",
      "model_saved at f1: 0.6683731328775556 from 0.6579994178713495\n",
      "Epoch 5/15  loss=0.1146  val_loss=0.1043  f1=0.674  time=122.54s\n",
      "model_saved at f1: 0.6744292836525846 from 0.6683731328775556\n",
      "Epoch 6/15  loss=0.1122  val_loss=0.1024  f1=0.670  time=123.40s\n",
      "Epoch 7/15  loss=0.1109  val_loss=0.1021  f1=0.675  time=123.06s\n",
      "model_saved at f1: 0.6754109138724522 from 0.6744292836525846\n",
      "Epoch 8/15  loss=0.1090  val_loss=0.1011  f1=0.674  time=122.63s\n",
      "Epoch 9/15  loss=0.1080  val_loss=0.1000  f1=0.677  time=123.11s\n",
      "model_saved at f1: 0.6767160509801383 from 0.6754109138724522\n",
      "Epoch 10/15  loss=0.1063  val_loss=0.1016  f1=0.678  time=123.07s\n",
      "model_saved at f1: 0.6783301237066293 from 0.6767160509801383\n",
      "Epoch 11/15  loss=0.1055  val_loss=0.1024  f1=0.676  time=123.31s\n",
      "Epoch 12/15  loss=0.1042  val_loss=0.0997  f1=0.680  time=123.45s\n",
      "model_saved at f1: 0.679715417055661 from 0.6783301237066293\n",
      "Epoch 13/15  loss=0.1032  val_loss=0.0992  f1=0.679  time=123.29s\n",
      "Epoch 14/15  loss=0.1022  val_loss=0.1008  f1=0.679  time=122.92s\n",
      "Epoch 15/15  loss=0.1016  val_loss=0.1018  f1=0.678  time=123.72s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1499  val_loss=0.1166  f1=0.638  time=123.77s\n",
      "model_saved at f1: 0.6383794491660809 from 0.0\n",
      "Epoch 2/15  loss=0.1241  val_loss=0.1103  f1=0.647  time=124.25s\n",
      "model_saved at f1: 0.6468846591833415 from 0.6383794491660809\n",
      "Epoch 3/15  loss=0.1193  val_loss=0.1063  f1=0.663  time=124.70s\n",
      "model_saved at f1: 0.6631196470282897 from 0.6468846591833415\n",
      "Epoch 4/15  loss=0.1164  val_loss=0.1034  f1=0.668  time=121.37s\n",
      "model_saved at f1: 0.6684312459651388 from 0.6631196470282897\n",
      "Epoch 5/15  loss=0.1145  val_loss=0.1028  f1=0.673  time=124.19s\n",
      "model_saved at f1: 0.6731795749823749 from 0.6684312459651388\n",
      "Epoch 6/15  loss=0.1122  val_loss=0.1020  f1=0.674  time=123.76s\n",
      "model_saved at f1: 0.6741983195392894 from 0.6731795749823749\n",
      "Epoch 7/15  loss=0.1109  val_loss=0.1006  f1=0.678  time=122.73s\n",
      "model_saved at f1: 0.6784184190731596 from 0.6741983195392894\n",
      "Epoch 8/15  loss=0.1093  val_loss=0.0994  f1=0.678  time=123.74s\n",
      "Epoch 9/15  loss=0.1078  val_loss=0.1043  f1=0.681  time=123.13s\n",
      "model_saved at f1: 0.6812111851208289 from 0.6784184190731596\n",
      "Epoch 10/15  loss=0.1066  val_loss=0.0988  f1=0.681  time=122.80s\n",
      "Epoch 11/15  loss=0.1054  val_loss=0.0988  f1=0.681  time=124.01s\n",
      "Epoch 12/15  loss=0.1041  val_loss=0.0988  f1=0.685  time=123.38s\n",
      "model_saved at f1: 0.6851585954836183 from 0.6812111851208289\n",
      "Epoch 13/15  loss=0.1030  val_loss=0.0998  f1=0.680  time=123.14s\n",
      "Epoch 14/15  loss=0.1020  val_loss=0.0999  f1=0.681  time=124.37s\n",
      "Epoch 15/15  loss=0.1011  val_loss=0.1006  f1=0.682  time=123.70s\n",
      "{'threshold': 0.24698999524116516, 'f1': 0.6801108176019587}\n",
      "f1 score: 0.695230578419213\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1580  val_loss=0.1236  f1=0.627  time=123.34s\n",
      "model_saved at f1: 0.6270746230837452 from 0.0\n",
      "Epoch 2/15  loss=0.1242  val_loss=0.1107  f1=0.651  time=123.67s\n",
      "model_saved at f1: 0.6506047349459598 from 0.6270746230837452\n",
      "Epoch 3/15  loss=0.1199  val_loss=0.1047  f1=0.661  time=122.82s\n",
      "model_saved at f1: 0.6606785793633161 from 0.6506047349459598\n",
      "Epoch 4/15  loss=0.1172  val_loss=0.1047  f1=0.662  time=120.63s\n",
      "model_saved at f1: 0.6619771385504334 from 0.6606785793633161\n",
      "Epoch 5/15  loss=0.1146  val_loss=0.1017  f1=0.674  time=123.29s\n",
      "model_saved at f1: 0.6737895851381268 from 0.6619771385504334\n",
      "Epoch 6/15  loss=0.1124  val_loss=0.1007  f1=0.676  time=122.60s\n",
      "model_saved at f1: 0.6756808106396454 from 0.6737895851381268\n",
      "Epoch 7/15  loss=0.1111  val_loss=0.1010  f1=0.675  time=123.06s\n",
      "Epoch 8/15  loss=0.1087  val_loss=0.1030  f1=0.676  time=123.00s\n",
      "model_saved at f1: 0.675913507147764 from 0.6756808106396454\n",
      "Epoch 9/15  loss=0.1078  val_loss=0.0999  f1=0.678  time=122.49s\n",
      "model_saved at f1: 0.6784798769546271 from 0.675913507147764\n",
      "Epoch 10/15  loss=0.1066  val_loss=0.1013  f1=0.678  time=123.50s\n",
      "Epoch 11/15  loss=0.1049  val_loss=0.1004  f1=0.680  time=122.97s\n",
      "model_saved at f1: 0.6795642984880508 from 0.6784798769546271\n",
      "Epoch 12/15  loss=0.1038  val_loss=0.0992  f1=0.678  time=122.97s\n",
      "Epoch 13/15  loss=0.1028  val_loss=0.1012  f1=0.682  time=123.46s\n",
      "model_saved at f1: 0.6816435880055517 from 0.6795642984880508\n",
      "Epoch 14/15  loss=0.1019  val_loss=0.0999  f1=0.678  time=123.35s\n",
      "Epoch 15/15  loss=0.1009  val_loss=0.1020  f1=0.678  time=122.60s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1935  val_loss=0.1242  f1=0.622  time=124.07s\n",
      "model_saved at f1: 0.6223580180804484 from 0.0\n",
      "Epoch 2/15  loss=0.1265  val_loss=0.1110  f1=0.642  time=124.69s\n",
      "model_saved at f1: 0.6422769553255601 from 0.6223580180804484\n",
      "Epoch 3/15  loss=0.1211  val_loss=0.1062  f1=0.659  time=123.41s\n",
      "model_saved at f1: 0.6592261904761904 from 0.6422769553255601\n",
      "Epoch 4/15  loss=0.1181  val_loss=0.1039  f1=0.669  time=121.90s\n",
      "model_saved at f1: 0.6692398481610259 from 0.6592261904761904\n",
      "Epoch 5/15  loss=0.1156  val_loss=0.1018  f1=0.673  time=123.93s\n",
      "model_saved at f1: 0.6733590233476896 from 0.6692398481610259\n",
      "Epoch 6/15  loss=0.1131  val_loss=0.1016  f1=0.675  time=123.79s\n",
      "model_saved at f1: 0.6752994901629601 from 0.6733590233476896\n",
      "Epoch 7/15  loss=0.1119  val_loss=0.1017  f1=0.677  time=122.43s\n",
      "model_saved at f1: 0.677440248720772 from 0.6752994901629601\n",
      "Epoch 8/15  loss=0.1104  val_loss=0.1015  f1=0.674  time=123.83s\n",
      "Epoch 9/15  loss=0.1086  val_loss=0.1006  f1=0.678  time=123.68s\n",
      "model_saved at f1: 0.6782923206097009 from 0.677440248720772\n",
      "Epoch 10/15  loss=0.1073  val_loss=0.1011  f1=0.683  time=123.60s\n",
      "model_saved at f1: 0.6826316956507966 from 0.6782923206097009\n",
      "Epoch 11/15  loss=0.1062  val_loss=0.1001  f1=0.682  time=123.83s\n",
      "Epoch 12/15  loss=0.1048  val_loss=0.0997  f1=0.681  time=123.45s\n",
      "Epoch 13/15  loss=0.1038  val_loss=0.1011  f1=0.681  time=123.64s\n",
      "Epoch 14/15  loss=0.1028  val_loss=0.1024  f1=0.682  time=123.83s\n",
      "Epoch 15/15  loss=0.1017  val_loss=0.0983  f1=0.683  time=123.81s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1699  val_loss=0.1232  f1=0.632  time=123.36s\n",
      "model_saved at f1: 0.6316130285566276 from 0.0\n",
      "Epoch 2/15  loss=0.1253  val_loss=0.1114  f1=0.661  time=123.67s\n",
      "model_saved at f1: 0.6605540509674683 from 0.6316130285566276\n",
      "Epoch 3/15  loss=0.1203  val_loss=0.1062  f1=0.670  time=123.98s\n",
      "model_saved at f1: 0.6698140852407776 from 0.6605540509674683\n",
      "Epoch 4/15  loss=0.1172  val_loss=0.1020  f1=0.673  time=121.95s\n",
      "model_saved at f1: 0.6726567550096961 from 0.6698140852407776\n",
      "Epoch 5/15  loss=0.1148  val_loss=0.1019  f1=0.672  time=65.85s\n",
      "Epoch 6/15  loss=0.1133  val_loss=0.1001  f1=0.683  time=66.07s\n",
      "model_saved at f1: 0.6831577241043928 from 0.6726567550096961\n",
      "Epoch 7/15  loss=0.1111  val_loss=0.0999  f1=0.680  time=65.72s\n",
      "Epoch 8/15  loss=0.1098  val_loss=0.0979  f1=0.684  time=65.61s\n",
      "model_saved at f1: 0.6842888572537854 from 0.6831577241043928\n",
      "Epoch 9/15  loss=0.1080  val_loss=0.0988  f1=0.686  time=66.35s\n",
      "model_saved at f1: 0.6863734679163662 from 0.6842888572537854\n",
      "Epoch 10/15  loss=0.1072  val_loss=0.0987  f1=0.686  time=65.61s\n",
      "Epoch 11/15  loss=0.1062  val_loss=0.0999  f1=0.682  time=65.70s\n",
      "Epoch 12/15  loss=0.1045  val_loss=0.0984  f1=0.689  time=66.33s\n",
      "model_saved at f1: 0.6886244645406948 from 0.6863734679163662\n",
      "Epoch 13/15  loss=0.1029  val_loss=0.1011  f1=0.685  time=65.72s\n",
      "Epoch 14/15  loss=0.1021  val_loss=0.1000  f1=0.686  time=65.83s\n",
      "Epoch 15/15  loss=0.1012  val_loss=0.0992  f1=0.689  time=66.47s\n",
      "model_saved at f1: 0.6887274859134286 from 0.6886244645406948\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1710  val_loss=0.1232  f1=0.629  time=66.12s\n",
      "model_saved at f1: 0.629360193993658 from 0.0\n",
      "Epoch 2/15  loss=0.1249  val_loss=0.1089  f1=0.655  time=66.19s\n",
      "model_saved at f1: 0.6550193417464646 from 0.629360193993658\n",
      "Epoch 3/15  loss=0.1200  val_loss=0.1051  f1=0.669  time=66.09s\n",
      "model_saved at f1: 0.6693684347036701 from 0.6550193417464646\n",
      "Epoch 4/15  loss=0.1167  val_loss=0.1020  f1=0.669  time=66.24s\n",
      "model_saved at f1: 0.6693778992184025 from 0.6693684347036701\n",
      "Epoch 5/15  loss=0.1147  val_loss=0.1017  f1=0.677  time=66.15s\n",
      "model_saved at f1: 0.6766610542675819 from 0.6693778992184025\n",
      "Epoch 6/15  loss=0.1129  val_loss=0.1002  f1=0.677  time=66.29s\n",
      "model_saved at f1: 0.6768957082499445 from 0.6766610542675819\n",
      "Epoch 7/15  loss=0.1111  val_loss=0.1000  f1=0.681  time=66.03s\n",
      "model_saved at f1: 0.6814875203812868 from 0.6768957082499445\n",
      "Epoch 8/15  loss=0.1092  val_loss=0.1004  f1=0.684  time=66.17s\n",
      "model_saved at f1: 0.6835177100088632 from 0.6814875203812868\n",
      "Epoch 9/15  loss=0.1078  val_loss=0.1011  f1=0.683  time=66.25s\n",
      "Epoch 10/15  loss=0.1067  val_loss=0.0992  f1=0.685  time=66.63s\n",
      "model_saved at f1: 0.6850254802038416 from 0.6835177100088632\n",
      "Epoch 11/15  loss=0.1052  val_loss=0.1000  f1=0.683  time=66.12s\n",
      "Epoch 12/15  loss=0.1041  val_loss=0.1007  f1=0.682  time=66.32s\n",
      "Epoch 13/15  loss=0.1034  val_loss=0.1009  f1=0.686  time=66.67s\n",
      "model_saved at f1: 0.6856604509128711 from 0.6850254802038416\n",
      "Epoch 14/15  loss=0.1016  val_loss=0.0996  f1=0.683  time=66.12s\n",
      "Epoch 15/15  loss=0.1008  val_loss=0.1014  f1=0.684  time=66.48s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1676  val_loss=0.1127  f1=0.640  time=66.52s\n",
      "model_saved at f1: 0.6396581932191928 from 0.0\n",
      "Epoch 2/15  loss=0.1246  val_loss=0.1082  f1=0.657  time=66.72s\n",
      "model_saved at f1: 0.6572263429984267 from 0.6396581932191928\n",
      "Epoch 3/15  loss=0.1202  val_loss=0.1020  f1=0.672  time=66.40s\n",
      "model_saved at f1: 0.6719085060757684 from 0.6572263429984267\n",
      "Epoch 4/15  loss=0.1171  val_loss=0.1032  f1=0.667  time=66.53s\n",
      "Epoch 5/15  loss=0.1146  val_loss=0.1022  f1=0.677  time=66.71s\n",
      "model_saved at f1: 0.6769309725429817 from 0.6719085060757684\n",
      "Epoch 6/15  loss=0.1124  val_loss=0.1026  f1=0.682  time=66.59s\n",
      "model_saved at f1: 0.6815638823835545 from 0.6769309725429817\n",
      "Epoch 7/15  loss=0.1105  val_loss=0.1013  f1=0.681  time=66.55s\n",
      "Epoch 8/15  loss=0.1098  val_loss=0.1038  f1=0.681  time=66.86s\n",
      "Epoch 9/15  loss=0.1084  val_loss=0.0999  f1=0.683  time=66.65s\n",
      "model_saved at f1: 0.682888845629745 from 0.6815638823835545\n",
      "Epoch 10/15  loss=0.1070  val_loss=0.1057  f1=0.682  time=66.41s\n",
      "Epoch 11/15  loss=0.1059  val_loss=0.1008  f1=0.683  time=66.81s\n",
      "model_saved at f1: 0.6829004329004329 from 0.682888845629745\n",
      "Epoch 12/15  loss=0.1048  val_loss=0.1011  f1=0.682  time=66.32s\n",
      "Epoch 13/15  loss=0.1033  val_loss=0.1008  f1=0.683  time=66.46s\n",
      "model_saved at f1: 0.6830794776000256 from 0.6829004329004329\n",
      "Epoch 14/15  loss=0.1019  val_loss=0.1018  f1=0.686  time=66.67s\n",
      "model_saved at f1: 0.6855818972357105 from 0.6830794776000256\n",
      "Epoch 15/15  loss=0.1012  val_loss=0.1000  f1=0.683  time=66.46s\n",
      "{'threshold': 0.26576894521713257, 'f1': 0.6812956620637807}\n",
      "f1 score: 0.6969426068299659\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
