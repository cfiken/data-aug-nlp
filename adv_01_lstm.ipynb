{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# use only for tokenizer and padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1019):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# SEED = 1019\n",
    "# seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "class Config:\n",
    "    num_epochs = 15\n",
    "    batch_size = 512\n",
    "    test_batch_size = 512\n",
    "    vocab_size = 120000\n",
    "    max_length = 72\n",
    "    embedding_size = 300\n",
    "    hidden_size = 64\n",
    "    num_layers = 1\n",
    "    embedding_noise_var = 0.1\n",
    "    embedding_dropout = 0.3\n",
    "    layer_dropout = 0.1\n",
    "    dense_size = [hidden_size*2*4, int(hidden_size/4)] # depend on concat num\n",
    "    output_size = 1\n",
    "    num_cv_splits = 5\n",
    "    learning_rate = 0.001\n",
    "    clip_grad = 5.0\n",
    "    embeddings = ['glove', 'paragram', 'fasttext']\n",
    "    datadir = Path('./data/')\n",
    "    # datadir = Path('../input') # for kernel\n",
    "\n",
    "c = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [\n",
    "    ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '¬', '░', '¶', '↑', '±',  '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '₹', '´'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"'re\": \" are\",\n",
    "    \"tryin'\": \"trying\",\n",
    "    \"doesn'\": \"does not\",\n",
    "    'howdo': 'how do',\n",
    "    'whatare': 'what are',\n",
    "    'howcan': 'how can',\n",
    "    'howmuch': 'how much',\n",
    "    'howmany': 'how many',\n",
    "    'whydo': 'why do',\n",
    "    'doI': 'do I',\n",
    "    'theBest': 'the best',\n",
    "    'howdoes': 'how does',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spells = {\n",
    "    'colour': 'color',\n",
    "    'centre': 'center',\n",
    "    'favourite': 'favorite',\n",
    "    'travelling': 'traveling',\n",
    "    'counselling': 'counseling',\n",
    "    'theatre': 'theater',\n",
    "    'cancelled': 'canceled',\n",
    "    'labour': 'labor',\n",
    "    'organisation': 'organization',\n",
    "    'wwii': 'world war 2',\n",
    "    'citicise': 'criticize',\n",
    "    'youtu.be': 'youtube',\n",
    "    'youtu ': 'youtube ',\n",
    "    'qoura': 'quora',\n",
    "    'sallary': 'salary',\n",
    "    'Whta': 'what',\n",
    "    'whta': 'what',\n",
    "    'narcisist': 'narcissist',\n",
    "    'mastrubation': 'masturbation',\n",
    "    'mastrubate': 'masturbate',\n",
    "    \"mastrubating\": 'masturbating',\n",
    "    'pennis': 'penis',\n",
    "    'Etherium': 'ethereum',\n",
    "    'etherium': 'ethereum',\n",
    "    'narcissit': 'narcissist',\n",
    "    'bigdata': 'big data',\n",
    "    '2k17': '2017',\n",
    "    '2k18': '2018',\n",
    "    'qouta': 'quota',\n",
    "    'exboyfriend': 'ex boyfriend',\n",
    "    'exgirlfriend': 'ex girlfriend',\n",
    "    'airhostess': 'air hostess',\n",
    "    \"whst\": 'what',\n",
    "    'watsapp': 'whatsapp',\n",
    "    'demonitisation': 'demonetization',\n",
    "    'demonitization': 'demonetization',\n",
    "    'demonetisation': 'demonetization',\n",
    "    'quorans': 'quora user',\n",
    "    'quoran': 'quora user',\n",
    "    'pokémon': 'pokemon',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    train_df = pd.read_csv(datadir / 'train_local.csv')\n",
    "    test_df = pd.read_csv(datadir / 'test_local.csv')\n",
    "    print(\"Train shape : \", train_df.shape)\n",
    "    print(\"Test shape : \", test_df.shape)\n",
    "    return train_df, test_df\n",
    "\n",
    "def clean(df):\n",
    "    df = clean_lower(df)\n",
    "    df = clean_unicode(df)\n",
    "    df = clean_abbreviation(df, abbreviations)\n",
    "    df = clean_spells(df, spells)\n",
    "    df = clean_language(df)\n",
    "    df = clean_puncts(df, puncts)\n",
    "    df = clean_space(df)\n",
    "    return df\n",
    "\n",
    "def clean_unicode(df):\n",
    "    codes = ['\\x7f', '\\u200b', '\\xa0', '\\ufeff', '\\u200e', '\\u202a', '\\u202c', '\\u2060', '\\uf0d8', '\\ue019', '\\uf02d', '\\u200f', '\\u2061', '\\ue01b']\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: _clean_unicode(x, codes))\n",
    "    return df\n",
    "\n",
    "def _clean_unicode(x, codes):\n",
    "    for u in codes:\n",
    "        if u in x:\n",
    "            x = x.replace(u, '')\n",
    "    return x\n",
    "\n",
    "def clean_language(df):\n",
    "    langs1 = r'[\\p{Katakana}\\p{Hiragana}\\p{Han}]' # regex\n",
    "    langs2 = r'[ஆய்தஎழுத்துஆயுதஎழுத்துशुषछछशुषدوउसशुष북한내제តើបងប្អូនមានមធ្យបាយអ្វីខ្លះដើម្បីរកឃើញឯកសារអំពីប្រវត្តិស្ត្រនៃប្រាសាទអង្គរវट्टरौरआदસંઘરાજ્યपीतऊनअहএকটিবাড়িএকটিখামারএরঅধীনেপদেরবাছাইপরীক্ষাএরপ্রশ্নওউত্তরসহকোথায়পেতেপারিص、。Емелядуракلكلمقاممقال수능ί서로가를행복하게기乡국고등학교는몇시간업니《》싱관없어나이रचा키کپڤ」मिलगईकलेजेकोठंडकऋॠऌॡर]'\n",
    "    compiled_langs1 = regex.compile(langs1)\n",
    "    compiled_langs2 = re.compile(langs2)\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs1))\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs2))\n",
    "    return df\n",
    "\n",
    "def _clean_language(x, compiled_re):\n",
    "    return compiled_re.sub(' <lang> ', x)\n",
    "\n",
    "def clean_lower(df):\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "def clean_puncts(df, puncts):\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_puncts(x, puncts))\n",
    "    return df\n",
    "    \n",
    "def _clean_puncts(x, puncts):\n",
    "    x = str(x)\n",
    "    # added space around puncts after replace\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_spells(df, spells):\n",
    "    compiled_spells = re.compile('(%s)' % '|'.join(spells.keys()))\n",
    "    def replace(match):\n",
    "        return spells[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_spells(x, compiled_spells, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_spells(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_abbreviation(df, abbreviations):\n",
    "    compiled_abbreviation = re.compile('(%s)' % '|'.join(abbreviations.keys()))\n",
    "    def replace(match):\n",
    "        return abbreviations[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_abreviation(x, compiled_abbreviation, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_abreviation(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_space(df):\n",
    "    compiled_re = re.compile(r\"\\s+\")\n",
    "    df['question_text'] = df[\"question_text\"].apply(lambda x: _clean_space(x, compiled_re))\n",
    "    return df\n",
    "\n",
    "def _clean_space(x, compiled_re):\n",
    "    return compiled_re.sub(\" \", x)\n",
    "        \n",
    "def prepare_tokenizer(texts, max_words):\n",
    "    tokenizer = Tokenizer(num_words=max_words, filters='', oov_token='<unk>')\n",
    "    tokenizer.fit_on_texts(list(texts))\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_and_padding(texts, tokenizer, max_length):\n",
    "    texts = tokenizer.texts_to_sequences(texts)\n",
    "    texts = pad_sequences(texts, maxlen=max_length)\n",
    "    return texts\n",
    "\n",
    "def get_all_vocabs(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config, tokenizer, all_vocabs, embedding_weights = None):\n",
    "        super(Embeddings, self).__init__()\n",
    "        \n",
    "        self.embedding_map = {\n",
    "            'fasttext': self._load_fasttext,\n",
    "            'glove': self._load_glove,\n",
    "            'paragram': self._load_paragram\n",
    "        }\n",
    "        self.c = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.all_vocabs = all_vocabs\n",
    "        \n",
    "        if embedding_weights is None:\n",
    "            embedding_weights = self._load_embeddings(self.c.embeddings)\n",
    "            \n",
    "        self.original_embedding_weights = embedding_weights\n",
    "        self.embeddings = nn.Embedding(self.c.vocab_size + 1, self.c.embedding_size, padding_idx=0)\n",
    "        self.embeddings.weight = nn.Parameter(embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.embedding_dropout = nn.Dropout2d(self.c.embedding_dropout)\n",
    "        \n",
    "    def forward(self, x, perturb):\n",
    "        embedding = self.embeddings(x)\n",
    "        embedding.requires_grad = True\n",
    "        if perturb is not None:\n",
    "            return embedding + perturb\n",
    "        return embedding\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        self.embeddings.weight = nn.Parameter(self.original_embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "    \n",
    "    def _load_embeddings(self, embedding_list: list):\n",
    "        embedding_weights = np.zeros((self.c.vocab_size, self.c.embedding_size))\n",
    "        pool = Pool(num_cores)\n",
    "        embedding_weights = np.mean(pool.map(self._load_an_embedding, embedding_list), 0)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return torch.tensor(embedding_weights, dtype=torch.float32)\n",
    "\n",
    "    def _load_an_embedding(self, emb):\n",
    "        return self.embedding_map[emb](self.tokenizer.word_index)\n",
    "        \n",
    "    def _get_embeddings_pair(self, word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "        \n",
    "    def _make_embeddings(self, embeddings_index, word_index, emb_mean, emb_std):\n",
    "        nb_words = min(self.c.vocab_size, len(word_index))\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, self.c.embedding_size))\n",
    "        embedding_matrix[0] = np.zeros(self.c.embedding_size)\n",
    "        for word, i in word_index.items():\n",
    "            if i >= self.c.vocab_size:\n",
    "                continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return embedding_matrix\n",
    "    \n",
    "    def _load_glove(self, word_index):\n",
    "        print('loading glove')\n",
    "        filepath = self.c.datadir / 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "    \n",
    "    def _load_fasttext(self, word_index):    \n",
    "        print('loading fasttext')\n",
    "        filepath = self.c.datadir / 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0033469985, 0.109855495\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "\n",
    "    def _load_paragram(self, word_index):\n",
    "        print('loading paragram')\n",
    "        filepath = self.c.datadir / 'embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath, encoding=\"utf8\", errors='ignore')\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0053247833, 0.49346462\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 2\n",
    "def df_parallelize_run(df, func, num_cores=2):\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1175509, 3)\n",
      "Test shape :  (130613, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_data(c.datadir)\n",
    "train_df = df_parallelize_run(train_df, clean)\n",
    "test_df = df_parallelize_run(test_df, clean)\n",
    "train_x, train_y = train_df['question_text'].values, train_df['target'].values\n",
    "test_x = test_df['question_text'].values\n",
    "tokenizer = prepare_tokenizer(train_x, c.vocab_size)\n",
    "train_x = tokenize_and_padding(train_x, tokenizer, c.max_length)\n",
    "test_x = tokenize_and_padding(test_x, tokenizer, c.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_vocabs:  184279\n",
      "loading glove\n",
      "loading paragram\n",
      "loading fasttext\n",
      "48.364673376083374\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_vocabs = get_all_vocabs(train_df['question_text'])\n",
    "print('all_vocabs: ', len(all_vocabs))\n",
    "embeddings = Embeddings(c, tokenizer, all_vocabs)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(GRULayer, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          bias=False,\n",
    "                          bidirectional=True,\n",
    "                          batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_outputs, gru_state = self.gru(x)\n",
    "        return self.dropout(gru_outputs), gru_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=False,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_outputs, (lstm_states, _) = self.lstm(x)\n",
    "        return self.dropout(lstm_outputs), lstm_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, config: Config, embeddings):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.c = config\n",
    "        \n",
    "        self.embedding = embeddings\n",
    "        self.lstm1 = LSTMLayer(input_size=self.c.embedding_size,\n",
    "                              hidden_size=self.c.hidden_size,\n",
    "                              num_layers=self.c.num_layers,\n",
    "                              dropout_rate=self.c.layer_dropout)\n",
    "        self.lstm2 = LSTMLayer(input_size=self.c.hidden_size*2,\n",
    "                            hidden_size=self.c.hidden_size,\n",
    "                            num_layers=self.c.num_layers,\n",
    "                            dropout_rate=self.c.layer_dropout)\n",
    "        \n",
    "        self.cell_dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.linear = nn.Linear(self.c.dense_size[0], self.c.dense_size[1])\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(self.c.dense_size[1])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.out = nn.Linear(self.c.dense_size[1], self.c.output_size)\n",
    "        \n",
    "    def forward(self, x, perturb=None):\n",
    "        h_embedding = self.embedding(x, perturb)\n",
    "        o_lstm1, h_lstm1 = self.lstm1(h_embedding)\n",
    "        o_lstm2, h_lstm2 = self.lstm2(o_lstm1)\n",
    "        \n",
    "        avg_pool = torch.mean(o_lstm2, 1)\n",
    "        max_pool, _ = torch.max(o_lstm2, 1)\n",
    "        \n",
    "        h_lstm1 = self.cell_dropout(torch.cat(h_lstm1.split(1, 0), -1).squeeze(0))\n",
    "        h_lstm2 = self.cell_dropout(torch.cat(h_lstm2.split(1, 0), -1).squeeze(0))\n",
    "\n",
    "        concat = torch.cat([h_lstm1, h_lstm2, avg_pool, max_pool], 1)\n",
    "        concat = self.linear(concat)\n",
    "        concat = self.batch_norm(concat)\n",
    "        concat = self.relu(concat)\n",
    "        concat = self.dropout(concat)\n",
    "        out = self.out(concat)\n",
    "        \n",
    "        return out, h_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_length(data, mask):\n",
    "    max_length = data.shape[1]\n",
    "    transposed = torch.transpose(data, 1, 0)\n",
    "    res = (transposed == mask).all(1)\n",
    "    for i, r in enumerate(res):\n",
    "        if r == 0:\n",
    "            break\n",
    "    data = data[:, -(max_length - i):]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_x, train_y, test_x, c, embeddings, trial=0):\n",
    "    splits = list(StratifiedKFold(n_splits=c.num_cv_splits, shuffle=True).split(train_x, train_y))\n",
    "    x_test_cuda = torch.tensor(test_x, dtype=torch.long).cuda(cuda_idx)\n",
    "    test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=c.test_batch_size, shuffle=False)\n",
    "    train_preds = np.zeros((len(train_x)))\n",
    "    test_preds = np.zeros((len(test_x)))\n",
    "\n",
    "    mask = torch.zeros((c.max_length, 1), dtype=torch.long).cuda(cuda_idx)\n",
    "    \n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        x_train_fold = torch.tensor(train_x[train_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "        x_val_fold = torch.tensor(train_x[valid_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "\n",
    "        model = SimpleRNN(c, embeddings)\n",
    "        model.cuda(cuda_idx)\n",
    "\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=c.learning_rate)\n",
    "\n",
    "        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "        valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=c.batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=c.test_batch_size, shuffle=False)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        print(f'Fold {i + 1}')\n",
    "\n",
    "        for epoch in range(c.num_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred, embedding = model(x_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                perturb = embedding.grad\n",
    "                perturb = 0.1 * perturb / torch.norm(perturb, p=2)\n",
    "                perturb = perturb.detach()\n",
    "\n",
    "                y_pred, embedding = model(x_batch, perturb)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), c.clip_grad)\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "            avg_val_loss = 0.\n",
    "\n",
    "            # validation prediction\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred, _ = model(x_batch)\n",
    "                y_pred = y_pred.detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            search_result = threshold_search(y_val_fold.cpu().numpy(), valid_preds_fold)\n",
    "            valid_pred_targets = valid_preds_fold > search_result['threshold']\n",
    "            val_f1 = f1_score(y_val_fold.cpu().numpy(), valid_pred_targets)\n",
    "\n",
    "            elapsed_time = time.time() - start_time \n",
    "            print('Epoch {}/{}  loss={:.4f}  val_loss={:.4f}  f1={:.3f}  time={:.2f}s'.format(\n",
    "                epoch + 1, c.num_epochs, avg_loss, avg_val_loss, val_f1, elapsed_time))\n",
    "            if best_f1 < val_f1:\n",
    "                print(f'model_saved at f1: {val_f1} from {best_f1}')\n",
    "                ckpt_path = Path(f'./ckpt/adv/{trial}/')\n",
    "                if not ckpt_path.exists():\n",
    "                    ckpt_path.mkdir(parents=True)\n",
    "                torch.save(model.state_dict(),ckpt_path / f'{i}_model.pt')\n",
    "                best_f1 = val_f1\n",
    "                best_epoch = epoch\n",
    "\n",
    "        # test prediction\n",
    "        model.load_state_dict(torch.load(f'./ckpt/adv/{trial}/{i}_model.pt'))  # load best model\n",
    "        test_preds_fold = np.zeros(len(test_x))\n",
    "        for i, (x_batch, ) in enumerate(test_loader):\n",
    "            x_batch = cut_length(x_batch, mask)\n",
    "            y_pred, _ = model(x_batch)\n",
    "            y_pred = y_pred.detach()\n",
    "            test_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "        train_preds[valid_idx] = valid_preds_fold\n",
    "        test_preds += test_preds_fold / len(splits)\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1656  val_loss=0.1072  f1=0.652  time=114.61s\n",
      "model_saved at f1: 0.6515805060194424 from 0.0\n",
      "Epoch 2/15  loss=0.1084  val_loss=0.1016  f1=0.672  time=118.32s\n",
      "model_saved at f1: 0.6719098261478699 from 0.6515805060194424\n",
      "Epoch 3/15  loss=0.0652  val_loss=0.1873  f1=0.656  time=117.83s\n",
      "Epoch 4/15  loss=0.0497  val_loss=0.1847  f1=0.662  time=117.38s\n",
      "Epoch 5/15  loss=0.0450  val_loss=0.2206  f1=0.664  time=118.85s\n",
      "Epoch 6/15  loss=0.0414  val_loss=0.2514  f1=0.660  time=118.76s\n",
      "Epoch 7/15  loss=0.0350  val_loss=0.2790  f1=0.665  time=119.94s\n",
      "Epoch 8/15  loss=0.0436  val_loss=0.3160  f1=0.652  time=117.67s\n",
      "Epoch 9/15  loss=0.0229  val_loss=0.2999  f1=0.652  time=119.24s\n",
      "Epoch 10/15  loss=0.0244  val_loss=0.3825  f1=0.657  time=120.14s\n",
      "Epoch 11/15  loss=0.0235  val_loss=0.4184  f1=0.659  time=119.74s\n",
      "Epoch 12/15  loss=0.0237  val_loss=0.4399  f1=0.654  time=118.29s\n",
      "Epoch 13/15  loss=0.0253  val_loss=0.3375  f1=0.661  time=119.77s\n",
      "Epoch 14/15  loss=0.0268  val_loss=0.3641  f1=0.666  time=119.75s\n",
      "Epoch 15/15  loss=0.0251  val_loss=0.3745  f1=0.665  time=119.90s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1392  val_loss=0.1073  f1=0.652  time=120.56s\n",
      "model_saved at f1: 0.6520626073430765 from 0.0\n",
      "Epoch 2/15  loss=0.1070  val_loss=0.1018  f1=0.670  time=120.11s\n",
      "model_saved at f1: 0.6700398456770602 from 0.6520626073430765\n",
      "Epoch 3/15  loss=0.1013  val_loss=0.0988  f1=0.678  time=119.59s\n",
      "model_saved at f1: 0.6776933108559433 from 0.6700398456770602\n",
      "Epoch 4/15  loss=0.0964  val_loss=0.0970  f1=0.684  time=120.29s\n",
      "model_saved at f1: 0.6841245958318661 from 0.6776933108559433\n",
      "Epoch 5/15  loss=0.0924  val_loss=0.0970  f1=0.686  time=119.82s\n",
      "model_saved at f1: 0.6862177550176533 from 0.6841245958318661\n",
      "Epoch 6/15  loss=0.0885  val_loss=0.0969  f1=0.686  time=120.07s\n",
      "model_saved at f1: 0.6863778298204528 from 0.6862177550176533\n",
      "Epoch 7/15  loss=0.0844  val_loss=0.0977  f1=0.685  time=119.81s\n",
      "Epoch 8/15  loss=0.0803  val_loss=0.0986  f1=0.687  time=120.56s\n",
      "model_saved at f1: 0.6870772749550726 from 0.6863778298204528\n",
      "Epoch 9/15  loss=0.0763  val_loss=0.1027  f1=0.684  time=120.57s\n",
      "Epoch 10/15  loss=0.0723  val_loss=0.1048  f1=0.679  time=119.12s\n",
      "Epoch 11/15  loss=0.0688  val_loss=0.1094  f1=0.672  time=120.60s\n",
      "Epoch 12/15  loss=0.0652  val_loss=0.1135  f1=0.673  time=121.08s\n",
      "Epoch 13/15  loss=0.0618  val_loss=0.1188  f1=0.667  time=120.16s\n",
      "Epoch 14/15  loss=0.0590  val_loss=0.1193  f1=0.668  time=118.64s\n",
      "Epoch 15/15  loss=0.0557  val_loss=0.1275  f1=0.668  time=120.85s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1425  val_loss=0.1089  f1=0.656  time=121.09s\n",
      "model_saved at f1: 0.6559544520006326 from 0.0\n",
      "Epoch 2/15  loss=0.1068  val_loss=0.1027  f1=0.673  time=120.38s\n",
      "model_saved at f1: 0.6734128262971317 from 0.6559544520006326\n",
      "Epoch 3/15  loss=0.1007  val_loss=0.0991  f1=0.683  time=121.05s\n",
      "model_saved at f1: 0.6825877626502567 from 0.6734128262971317\n",
      "Epoch 4/15  loss=0.0961  val_loss=0.0979  f1=0.684  time=121.19s\n",
      "model_saved at f1: 0.6842406512337827 from 0.6825877626502567\n",
      "Epoch 5/15  loss=0.0920  val_loss=0.0978  f1=0.686  time=120.90s\n",
      "model_saved at f1: 0.6859311708608428 from 0.6842406512337827\n",
      "Epoch 6/15  loss=0.0880  val_loss=0.0974  f1=0.687  time=119.40s\n",
      "model_saved at f1: 0.6871336691695973 from 0.6859311708608428\n",
      "Epoch 7/15  loss=0.0839  val_loss=0.0990  f1=0.686  time=119.89s\n",
      "Epoch 8/15  loss=0.0800  val_loss=0.1011  f1=0.686  time=121.29s\n",
      "Epoch 9/15  loss=0.0760  val_loss=0.1026  f1=0.682  time=120.67s\n",
      "Epoch 10/15  loss=0.0721  val_loss=0.1107  f1=0.673  time=121.26s\n",
      "Epoch 11/15  loss=0.0680  val_loss=0.1100  f1=0.674  time=120.75s\n",
      "Epoch 12/15  loss=0.0645  val_loss=0.1144  f1=0.671  time=120.20s\n",
      "Epoch 13/15  loss=0.0613  val_loss=0.1169  f1=0.673  time=121.40s\n",
      "Epoch 14/15  loss=0.0577  val_loss=0.1230  f1=0.667  time=119.92s\n",
      "Epoch 15/15  loss=0.0551  val_loss=0.1299  f1=0.666  time=119.71s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1535  val_loss=0.1087  f1=0.649  time=120.79s\n",
      "model_saved at f1: 0.6490024346286402 from 0.0\n",
      "Epoch 2/15  loss=0.1072  val_loss=0.1033  f1=0.668  time=119.67s\n",
      "model_saved at f1: 0.668415160722853 from 0.6490024346286402\n",
      "Epoch 3/15  loss=0.1012  val_loss=0.0993  f1=0.680  time=120.81s\n",
      "model_saved at f1: 0.6798473812001387 from 0.668415160722853\n",
      "Epoch 4/15  loss=0.0879  val_loss=0.1178  f1=0.671  time=120.80s\n",
      "Epoch 5/15  loss=0.0496  val_loss=0.1622  f1=0.671  time=120.76s\n",
      "Epoch 6/15  loss=0.0471  val_loss=0.2221  f1=0.669  time=120.35s\n",
      "Epoch 7/15  loss=0.0353  val_loss=0.2623  f1=0.665  time=120.93s\n",
      "Epoch 8/15  loss=0.0295  val_loss=0.3113  f1=0.663  time=120.43s\n",
      "Epoch 9/15  loss=0.0267  val_loss=0.3661  f1=0.655  time=119.81s\n",
      "Epoch 10/15  loss=0.0251  val_loss=0.3708  f1=0.663  time=120.93s\n",
      "Epoch 11/15  loss=0.0249  val_loss=0.3011  f1=0.664  time=120.42s\n",
      "Epoch 12/15  loss=0.0290  val_loss=0.3504  f1=0.669  time=120.81s\n",
      "Epoch 13/15  loss=0.0256  val_loss=0.2892  f1=0.662  time=120.35s\n",
      "Epoch 14/15  loss=0.0231  val_loss=0.4127  f1=0.662  time=120.91s\n",
      "Epoch 15/15  loss=0.0219  val_loss=0.3976  f1=0.655  time=119.58s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1458  val_loss=0.1061  f1=0.655  time=120.22s\n",
      "model_saved at f1: 0.6548979329824112 from 0.0\n",
      "Epoch 2/15  loss=0.1070  val_loss=0.1020  f1=0.674  time=119.64s\n",
      "model_saved at f1: 0.673828125 from 0.6548979329824112\n",
      "Epoch 3/15  loss=0.1010  val_loss=0.0990  f1=0.680  time=120.73s\n",
      "model_saved at f1: 0.6804533704513821 from 0.673828125\n",
      "Epoch 4/15  loss=0.0964  val_loss=0.0977  f1=0.684  time=120.69s\n",
      "model_saved at f1: 0.6838272543059778 from 0.6804533704513821\n",
      "Epoch 5/15  loss=0.0924  val_loss=0.0982  f1=0.684  time=118.79s\n",
      "model_saved at f1: 0.6839902676399026 from 0.6838272543059778\n",
      "Epoch 6/15  loss=0.0888  val_loss=0.0968  f1=0.687  time=121.06s\n",
      "model_saved at f1: 0.687433491341782 from 0.6839902676399026\n",
      "Epoch 7/15  loss=0.0850  val_loss=0.0987  f1=0.686  time=120.54s\n",
      "Epoch 8/15  loss=0.0810  val_loss=0.0984  f1=0.687  time=120.21s\n",
      "Epoch 9/15  loss=0.0770  val_loss=0.1020  f1=0.686  time=119.02s\n",
      "Epoch 10/15  loss=0.0731  val_loss=0.1045  f1=0.680  time=121.01s\n",
      "Epoch 11/15  loss=0.0695  val_loss=0.1097  f1=0.678  time=120.50s\n",
      "Epoch 12/15  loss=0.0656  val_loss=0.1155  f1=0.672  time=119.08s\n",
      "Epoch 13/15  loss=0.0623  val_loss=0.1167  f1=0.671  time=121.41s\n",
      "Epoch 14/15  loss=0.0589  val_loss=0.1250  f1=0.667  time=121.45s\n",
      "Epoch 15/15  loss=0.0560  val_loss=0.1261  f1=0.669  time=120.46s\n",
      "{'threshold': 0.2801567316055298, 'f1': 0.6630460388506524}\n",
      "f1 score: 0.6901546504029624\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1794  val_loss=0.1108  f1=0.652  time=116.96s\n",
      "model_saved at f1: 0.6521035084934116 from 0.0\n",
      "Epoch 2/15  loss=0.1094  val_loss=0.1022  f1=0.671  time=121.21s\n",
      "model_saved at f1: 0.6707423580786027 from 0.6521035084934116\n",
      "Epoch 3/15  loss=0.1027  val_loss=0.0991  f1=0.681  time=119.22s\n",
      "model_saved at f1: 0.6813201063346949 from 0.6707423580786027\n",
      "Epoch 4/15  loss=0.0603  val_loss=0.1457  f1=0.674  time=120.67s\n",
      "Epoch 5/15  loss=0.0469  val_loss=0.1636  f1=0.675  time=121.16s\n",
      "Epoch 6/15  loss=0.0449  val_loss=0.1779  f1=0.676  time=121.49s\n",
      "Epoch 7/15  loss=0.0442  val_loss=0.1846  f1=0.677  time=120.45s\n",
      "Epoch 8/15  loss=0.0372  val_loss=0.2198  f1=0.668  time=119.40s\n",
      "Epoch 9/15  loss=0.0269  val_loss=0.2929  f1=0.662  time=120.55s\n",
      "Epoch 10/15  loss=0.0251  val_loss=0.3227  f1=0.664  time=120.97s\n",
      "Epoch 11/15  loss=0.0256  val_loss=0.3184  f1=0.651  time=120.40s\n",
      "Epoch 12/15  loss=0.0243  val_loss=0.3307  f1=0.667  time=121.42s\n",
      "Epoch 13/15  loss=0.0250  val_loss=0.2972  f1=0.662  time=120.04s\n",
      "Epoch 14/15  loss=0.0251  val_loss=0.3421  f1=0.662  time=121.63s\n",
      "Epoch 15/15  loss=0.0234  val_loss=0.3693  f1=0.669  time=121.11s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1512  val_loss=0.1092  f1=0.650  time=119.98s\n",
      "model_saved at f1: 0.6502847635502655 from 0.0\n",
      "Epoch 2/15  loss=0.1075  val_loss=0.1053  f1=0.661  time=118.74s\n",
      "model_saved at f1: 0.6610993792794524 from 0.6502847635502655\n",
      "Epoch 3/15  loss=0.1010  val_loss=0.0999  f1=0.679  time=120.97s\n",
      "model_saved at f1: 0.6790155440414508 from 0.6610993792794524\n",
      "Epoch 4/15  loss=0.0963  val_loss=0.0982  f1=0.685  time=120.45s\n",
      "model_saved at f1: 0.685009976185879 from 0.6790155440414508\n",
      "Epoch 5/15  loss=0.0921  val_loss=0.0990  f1=0.682  time=121.16s\n",
      "Epoch 6/15  loss=0.0881  val_loss=0.0983  f1=0.688  time=120.34s\n",
      "model_saved at f1: 0.6875039908051849 from 0.685009976185879\n",
      "Epoch 7/15  loss=0.0841  val_loss=0.0988  f1=0.686  time=120.10s\n",
      "Epoch 8/15  loss=0.0801  val_loss=0.1011  f1=0.682  time=120.60s\n",
      "Epoch 9/15  loss=0.0759  val_loss=0.1042  f1=0.680  time=120.65s\n",
      "Epoch 10/15  loss=0.0720  val_loss=0.1094  f1=0.675  time=120.14s\n",
      "Epoch 11/15  loss=0.0682  val_loss=0.1096  f1=0.675  time=121.18s\n",
      "Epoch 12/15  loss=0.0645  val_loss=0.1170  f1=0.669  time=120.47s\n",
      "Epoch 13/15  loss=0.0613  val_loss=0.1218  f1=0.670  time=120.99s\n",
      "Epoch 14/15  loss=0.0582  val_loss=0.1290  f1=0.663  time=119.68s\n",
      "Epoch 15/15  loss=0.0547  val_loss=0.1337  f1=0.663  time=120.84s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1546  val_loss=0.1091  f1=0.648  time=119.95s\n",
      "model_saved at f1: 0.6481161311368456 from 0.0\n",
      "Epoch 2/15  loss=0.1074  val_loss=0.1018  f1=0.672  time=120.14s\n",
      "model_saved at f1: 0.6716413191260632 from 0.6481161311368456\n",
      "Epoch 3/15  loss=0.0905  val_loss=0.1537  f1=0.665  time=120.83s\n",
      "Epoch 4/15  loss=0.0483  val_loss=0.1438  f1=0.673  time=121.29s\n",
      "model_saved at f1: 0.6734139558947164 from 0.6716413191260632\n",
      "Epoch 5/15  loss=0.0440  val_loss=0.1726  f1=0.667  time=120.57s\n",
      "Epoch 6/15  loss=0.0424  val_loss=0.2319  f1=0.668  time=119.95s\n",
      "Epoch 7/15  loss=0.0306  val_loss=0.2558  f1=0.668  time=120.68s\n",
      "Epoch 8/15  loss=0.0283  val_loss=0.2214  f1=0.660  time=120.35s\n",
      "Epoch 9/15  loss=0.0272  val_loss=0.2896  f1=0.666  time=119.85s\n",
      "Epoch 10/15  loss=0.0262  val_loss=0.3241  f1=0.669  time=121.28s\n",
      "Epoch 11/15  loss=0.0222  val_loss=0.3311  f1=0.665  time=120.51s\n",
      "Epoch 12/15  loss=0.0209  val_loss=0.3817  f1=0.665  time=121.26s\n",
      "Epoch 13/15  loss=0.0199  val_loss=0.4077  f1=0.662  time=119.56s\n",
      "Epoch 14/15  loss=0.0196  val_loss=0.3912  f1=0.663  time=120.65s\n",
      "Epoch 15/15  loss=0.0211  val_loss=0.4061  f1=0.649  time=120.28s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1508  val_loss=0.1088  f1=0.651  time=120.30s\n",
      "model_saved at f1: 0.6514518853791478 from 0.0\n",
      "Epoch 2/15  loss=0.1074  val_loss=0.1012  f1=0.671  time=119.08s\n",
      "model_saved at f1: 0.6707262641076325 from 0.6514518853791478\n",
      "Epoch 3/15  loss=0.1007  val_loss=0.0986  f1=0.680  time=119.92s\n",
      "model_saved at f1: 0.6800910412846937 from 0.6707262641076325\n",
      "Epoch 4/15  loss=0.0962  val_loss=0.0964  f1=0.689  time=120.58s\n",
      "model_saved at f1: 0.68891514706352 from 0.6800910412846937\n",
      "Epoch 5/15  loss=0.0917  val_loss=0.0972  f1=0.690  time=119.91s\n",
      "model_saved at f1: 0.689668478086112 from 0.68891514706352\n",
      "Epoch 6/15  loss=0.0879  val_loss=0.0964  f1=0.690  time=120.05s\n",
      "model_saved at f1: 0.6904777049805992 from 0.689668478086112\n",
      "Epoch 7/15  loss=0.0840  val_loss=0.0977  f1=0.690  time=120.50s\n",
      "Epoch 8/15  loss=0.0799  val_loss=0.1006  f1=0.684  time=119.76s\n",
      "Epoch 9/15  loss=0.0759  val_loss=0.1012  f1=0.685  time=119.43s\n",
      "Epoch 10/15  loss=0.0720  val_loss=0.1046  f1=0.682  time=119.59s\n",
      "Epoch 11/15  loss=0.0684  val_loss=0.1104  f1=0.677  time=120.90s\n",
      "Epoch 12/15  loss=0.0619  val_loss=0.1299  f1=0.677  time=119.62s\n",
      "Epoch 13/15  loss=0.0444  val_loss=0.1838  f1=0.674  time=120.64s\n",
      "Epoch 14/15  loss=0.0356  val_loss=0.2167  f1=0.672  time=119.98s\n",
      "Epoch 15/15  loss=0.0309  val_loss=0.2419  f1=0.668  time=120.56s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1473  val_loss=0.1082  f1=0.649  time=118.54s\n",
      "model_saved at f1: 0.649194174455796 from 0.0\n",
      "Epoch 2/15  loss=0.1070  val_loss=0.1015  f1=0.672  time=120.52s\n",
      "model_saved at f1: 0.6720713681057499 from 0.649194174455796\n",
      "Epoch 3/15  loss=0.1009  val_loss=0.0984  f1=0.678  time=119.58s\n",
      "model_saved at f1: 0.6775213404995258 from 0.6720713681057499\n",
      "Epoch 4/15  loss=0.0961  val_loss=0.0994  f1=0.684  time=120.42s\n",
      "model_saved at f1: 0.6843535654199561 from 0.6775213404995258\n",
      "Epoch 5/15  loss=0.0920  val_loss=0.0964  f1=0.689  time=119.91s\n",
      "model_saved at f1: 0.6887985401927201 from 0.6843535654199561\n",
      "Epoch 6/15  loss=0.0881  val_loss=0.0978  f1=0.684  time=120.64s\n",
      "Epoch 7/15  loss=0.0840  val_loss=0.0997  f1=0.688  time=120.50s\n",
      "Epoch 8/15  loss=0.0799  val_loss=0.1030  f1=0.686  time=119.20s\n",
      "Epoch 9/15  loss=0.0760  val_loss=0.1044  f1=0.680  time=119.87s\n",
      "Epoch 10/15  loss=0.0721  val_loss=0.1045  f1=0.680  time=119.81s\n",
      "Epoch 11/15  loss=0.0681  val_loss=0.1101  f1=0.676  time=120.01s\n",
      "Epoch 12/15  loss=0.0645  val_loss=0.1128  f1=0.669  time=120.65s\n",
      "Epoch 13/15  loss=0.0612  val_loss=0.1170  f1=0.669  time=119.82s\n",
      "Epoch 14/15  loss=0.0581  val_loss=0.1218  f1=0.669  time=121.03s\n",
      "Epoch 15/15  loss=0.0549  val_loss=0.1262  f1=0.664  time=118.72s\n",
      "{'threshold': 0.26787614822387695, 'f1': 0.6598773889271216}\n",
      "f1 score: 0.6891524317266893\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1605  val_loss=0.1106  f1=0.646  time=121.34s\n",
      "model_saved at f1: 0.645739046701974 from 0.0\n",
      "Epoch 2/15  loss=0.0762  val_loss=0.1456  f1=0.648  time=120.73s\n",
      "model_saved at f1: 0.648353814812381 from 0.645739046701974\n",
      "Epoch 3/15  loss=0.0467  val_loss=0.1621  f1=0.653  time=121.33s\n",
      "model_saved at f1: 0.6530475776522117 from 0.648353814812381\n",
      "Epoch 4/15  loss=0.0470  val_loss=0.1985  f1=0.654  time=121.14s\n",
      "model_saved at f1: 0.6543226411303824 from 0.6530475776522117\n",
      "Epoch 5/15  loss=0.0465  val_loss=0.1852  f1=0.663  time=120.16s\n",
      "model_saved at f1: 0.6626822060630209 from 0.6543226411303824\n",
      "Epoch 6/15  loss=0.0462  val_loss=0.2015  f1=0.665  time=121.59s\n",
      "model_saved at f1: 0.6648136365158569 from 0.6626822060630209\n",
      "Epoch 7/15  loss=0.0464  val_loss=0.1989  f1=0.668  time=121.41s\n",
      "model_saved at f1: 0.6675383257607949 from 0.6648136365158569\n",
      "Epoch 8/15  loss=0.0406  val_loss=0.2598  f1=0.666  time=121.37s\n",
      "Epoch 9/15  loss=0.0336  val_loss=0.3019  f1=0.661  time=120.76s\n",
      "Epoch 10/15  loss=0.0247  val_loss=0.3500  f1=0.657  time=121.39s\n",
      "Epoch 11/15  loss=0.0252  val_loss=0.3232  f1=0.663  time=121.11s\n",
      "Epoch 12/15  loss=0.0263  val_loss=0.2844  f1=0.664  time=120.32s\n",
      "Epoch 13/15  loss=0.0258  val_loss=0.3319  f1=0.662  time=121.63s\n",
      "Epoch 14/15  loss=0.0258  val_loss=0.2965  f1=0.655  time=121.27s\n",
      "Epoch 15/15  loss=0.0240  val_loss=0.2929  f1=0.667  time=121.89s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1394  val_loss=0.1047  f1=0.660  time=120.71s\n",
      "model_saved at f1: 0.66004276191674 from 0.0\n",
      "Epoch 2/15  loss=0.1034  val_loss=0.1140  f1=0.653  time=120.36s\n",
      "Epoch 3/15  loss=0.0556  val_loss=0.1725  f1=0.657  time=120.25s\n",
      "Epoch 4/15  loss=0.0471  val_loss=0.2196  f1=0.662  time=120.67s\n",
      "model_saved at f1: 0.6624663072776281 from 0.66004276191674\n",
      "Epoch 5/15  loss=0.0388  val_loss=0.2112  f1=0.663  time=121.56s\n",
      "model_saved at f1: 0.662883015017696 from 0.6624663072776281\n",
      "Epoch 6/15  loss=0.0328  val_loss=0.2788  f1=0.656  time=120.41s\n",
      "Epoch 7/15  loss=0.0293  val_loss=0.3318  f1=0.653  time=120.32s\n",
      "Epoch 8/15  loss=0.0277  val_loss=0.3227  f1=0.663  time=120.03s\n",
      "Epoch 9/15  loss=0.0291  val_loss=0.3025  f1=0.667  time=121.03s\n",
      "model_saved at f1: 0.6668599780913719 from 0.662883015017696\n",
      "Epoch 10/15  loss=0.0306  val_loss=0.2871  f1=0.666  time=120.36s\n",
      "Epoch 11/15  loss=0.0277  val_loss=0.2897  f1=0.666  time=120.31s\n",
      "Epoch 12/15  loss=0.0349  val_loss=0.2602  f1=0.671  time=119.39s\n",
      "model_saved at f1: 0.6709914729978266 from 0.6668599780913719\n",
      "Epoch 13/15  loss=0.0304  val_loss=0.2738  f1=0.668  time=120.42s\n",
      "Epoch 14/15  loss=0.0243  val_loss=0.3101  f1=0.668  time=119.67s\n",
      "Epoch 15/15  loss=0.0232  val_loss=0.3717  f1=0.656  time=119.57s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1548  val_loss=0.1068  f1=0.654  time=120.39s\n",
      "model_saved at f1: 0.6541395594597271 from 0.0\n",
      "Epoch 2/15  loss=0.1075  val_loss=0.1019  f1=0.670  time=118.79s\n",
      "model_saved at f1: 0.6699360880845409 from 0.6541395594597271\n",
      "Epoch 3/15  loss=0.1013  val_loss=0.0983  f1=0.680  time=121.11s\n",
      "model_saved at f1: 0.6796404227995653 from 0.6699360880845409\n",
      "Epoch 4/15  loss=0.0966  val_loss=0.0976  f1=0.682  time=121.15s\n",
      "model_saved at f1: 0.6823352017569737 from 0.6796404227995653\n",
      "Epoch 5/15  loss=0.0925  val_loss=0.0979  f1=0.682  time=121.12s\n",
      "Epoch 6/15  loss=0.0886  val_loss=0.0977  f1=0.687  time=120.58s\n",
      "model_saved at f1: 0.6866664534493234 from 0.6823352017569737\n",
      "Epoch 7/15  loss=0.0847  val_loss=0.0976  f1=0.686  time=120.60s\n",
      "Epoch 8/15  loss=0.0805  val_loss=0.0987  f1=0.685  time=120.28s\n",
      "Epoch 9/15  loss=0.0766  val_loss=0.1017  f1=0.682  time=119.03s\n",
      "Epoch 10/15  loss=0.0725  val_loss=0.1036  f1=0.681  time=121.15s\n",
      "Epoch 11/15  loss=0.0689  val_loss=0.1080  f1=0.676  time=121.18s\n",
      "Epoch 12/15  loss=0.0652  val_loss=0.1154  f1=0.672  time=121.15s\n",
      "Epoch 13/15  loss=0.0617  val_loss=0.1141  f1=0.672  time=120.29s\n",
      "Epoch 14/15  loss=0.0585  val_loss=0.1183  f1=0.665  time=120.80s\n",
      "Epoch 15/15  loss=0.0555  val_loss=0.1262  f1=0.664  time=120.07s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1791  val_loss=0.1146  f1=0.648  time=119.34s\n",
      "model_saved at f1: 0.648034378481617 from 0.0\n",
      "Epoch 2/15  loss=0.1087  val_loss=0.1027  f1=0.677  time=120.17s\n",
      "model_saved at f1: 0.6768753634425277 from 0.648034378481617\n",
      "Epoch 3/15  loss=0.0991  val_loss=0.1068  f1=0.671  time=121.22s\n",
      "Epoch 4/15  loss=0.0539  val_loss=0.1419  f1=0.675  time=119.55s\n",
      "Epoch 5/15  loss=0.0461  val_loss=0.1592  f1=0.676  time=120.16s\n",
      "Epoch 6/15  loss=0.0414  val_loss=0.1998  f1=0.670  time=120.03s\n",
      "Epoch 7/15  loss=0.0418  val_loss=0.1653  f1=0.683  time=120.64s\n",
      "model_saved at f1: 0.6826558445878518 from 0.6768753634425277\n",
      "Epoch 8/15  loss=0.0385  val_loss=0.2094  f1=0.676  time=119.17s\n",
      "Epoch 9/15  loss=0.0310  val_loss=0.2368  f1=0.674  time=120.38s\n",
      "Epoch 10/15  loss=0.0284  val_loss=0.2545  f1=0.675  time=121.06s\n",
      "Epoch 11/15  loss=0.0276  val_loss=0.2771  f1=0.668  time=119.49s\n",
      "Epoch 12/15  loss=0.0226  val_loss=0.3165  f1=0.670  time=120.31s\n",
      "Epoch 13/15  loss=0.0228  val_loss=0.2946  f1=0.676  time=119.93s\n",
      "Epoch 14/15  loss=0.0225  val_loss=0.3075  f1=0.678  time=120.54s\n",
      "Epoch 15/15  loss=0.0217  val_loss=0.3376  f1=0.669  time=120.66s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1485  val_loss=0.1068  f1=0.653  time=120.12s\n",
      "model_saved at f1: 0.6534258229173483 from 0.0\n",
      "Epoch 2/15  loss=0.1068  val_loss=0.1005  f1=0.677  time=120.51s\n",
      "model_saved at f1: 0.6765135529946438 from 0.6534258229173483\n",
      "Epoch 3/15  loss=0.0982  val_loss=0.1253  f1=0.666  time=120.60s\n",
      "Epoch 4/15  loss=0.0547  val_loss=0.1409  f1=0.669  time=119.75s\n",
      "Epoch 5/15  loss=0.0452  val_loss=0.1971  f1=0.658  time=120.37s\n",
      "Epoch 6/15  loss=0.0334  val_loss=0.2759  f1=0.651  time=120.60s\n",
      "Epoch 7/15  loss=0.0299  val_loss=0.2404  f1=0.649  time=120.64s\n",
      "Epoch 8/15  loss=0.0285  val_loss=0.2783  f1=0.620  time=119.91s\n",
      "Epoch 9/15  loss=0.0295  val_loss=0.3021  f1=0.648  time=120.38s\n",
      "Epoch 10/15  loss=0.0319  val_loss=0.2638  f1=0.656  time=120.63s\n",
      "Epoch 11/15  loss=0.0314  val_loss=0.2704  f1=0.626  time=120.39s\n",
      "Epoch 12/15  loss=0.0271  val_loss=0.2903  f1=0.623  time=119.93s\n",
      "Epoch 13/15  loss=0.0292  val_loss=0.2888  f1=0.649  time=120.59s\n",
      "Epoch 14/15  loss=0.0277  val_loss=0.3571  f1=0.640  time=120.81s\n",
      "Epoch 15/15  loss=0.0250  val_loss=0.2880  f1=0.639  time=119.46s\n",
      "{'threshold': 0.2928018867969513, 'f1': 0.6528794298921418}\n",
      "f1 score: 0.6849723032938233\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1584  val_loss=0.1087  f1=0.647  time=121.60s\n",
      "model_saved at f1: 0.6469422883310058 from 0.0\n",
      "Epoch 2/15  loss=0.1076  val_loss=0.1175  f1=0.650  time=120.97s\n",
      "model_saved at f1: 0.6499587798845836 from 0.6469422883310058\n",
      "Epoch 3/15  loss=0.0492  val_loss=0.1819  f1=0.655  time=119.05s\n",
      "model_saved at f1: 0.6547097117045303 from 0.6499587798845836\n",
      "Epoch 4/15  loss=0.0417  val_loss=0.2198  f1=0.642  time=120.93s\n",
      "Epoch 5/15  loss=0.0304  val_loss=0.2617  f1=0.653  time=120.52s\n",
      "Epoch 6/15  loss=0.0288  val_loss=0.2881  f1=0.645  time=120.80s\n",
      "Epoch 7/15  loss=0.0291  val_loss=0.2621  f1=0.655  time=121.29s\n",
      "model_saved at f1: 0.6547711404189294 from 0.6547097117045303\n",
      "Epoch 8/15  loss=0.0273  val_loss=0.3203  f1=0.654  time=121.35s\n",
      "Epoch 9/15  loss=0.0282  val_loss=0.3145  f1=0.641  time=121.25s\n",
      "Epoch 10/15  loss=0.0299  val_loss=0.2802  f1=0.651  time=119.10s\n",
      "Epoch 11/15  loss=0.0358  val_loss=0.3050  f1=0.656  time=121.02s\n",
      "model_saved at f1: 0.6562231475973297 from 0.6547711404189294\n",
      "Epoch 12/15  loss=0.0289  val_loss=0.3250  f1=0.653  time=120.59s\n",
      "Epoch 13/15  loss=0.0262  val_loss=0.2846  f1=0.649  time=121.24s\n",
      "Epoch 14/15  loss=0.0281  val_loss=0.3509  f1=0.663  time=121.23s\n",
      "model_saved at f1: 0.66250615864674 from 0.6562231475973297\n",
      "Epoch 15/15  loss=0.0293  val_loss=0.3081  f1=0.656  time=121.49s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1389  val_loss=0.1059  f1=0.655  time=120.47s\n",
      "model_saved at f1: 0.6554846938775509 from 0.0\n",
      "Epoch 2/15  loss=0.1062  val_loss=0.1001  f1=0.675  time=121.03s\n",
      "model_saved at f1: 0.6746432913174227 from 0.6554846938775509\n",
      "Epoch 3/15  loss=0.1002  val_loss=0.0989  f1=0.683  time=119.85s\n",
      "model_saved at f1: 0.6828346254885629 from 0.6746432913174227\n",
      "Epoch 4/15  loss=0.0959  val_loss=0.0989  f1=0.684  time=119.97s\n",
      "model_saved at f1: 0.6836904351099538 from 0.6828346254885629\n",
      "Epoch 5/15  loss=0.0917  val_loss=0.0979  f1=0.691  time=120.74s\n",
      "model_saved at f1: 0.6908593600915973 from 0.6836904351099538\n",
      "Epoch 6/15  loss=0.0877  val_loss=0.0969  f1=0.691  time=119.62s\n",
      "model_saved at f1: 0.6909649581589958 from 0.6908593600915973\n",
      "Epoch 7/15  loss=0.0837  val_loss=0.0983  f1=0.688  time=119.22s\n",
      "Epoch 8/15  loss=0.0800  val_loss=0.1003  f1=0.690  time=120.22s\n",
      "Epoch 9/15  loss=0.0760  val_loss=0.1023  f1=0.686  time=120.53s\n",
      "Epoch 10/15  loss=0.0707  val_loss=0.1169  f1=0.682  time=119.88s\n",
      "Epoch 11/15  loss=0.0507  val_loss=0.1736  f1=0.672  time=120.19s\n",
      "Epoch 12/15  loss=0.0477  val_loss=0.1614  f1=0.674  time=120.74s\n",
      "Epoch 13/15  loss=0.0396  val_loss=0.2077  f1=0.675  time=120.64s\n",
      "Epoch 14/15  loss=0.0330  val_loss=0.2273  f1=0.676  time=120.36s\n",
      "Epoch 15/15  loss=0.0303  val_loss=0.2421  f1=0.676  time=119.64s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1668  val_loss=0.1113  f1=0.645  time=120.36s\n",
      "model_saved at f1: 0.645126479829879 from 0.0\n",
      "Epoch 2/15  loss=0.1095  val_loss=0.1036  f1=0.665  time=119.71s\n",
      "model_saved at f1: 0.6654565141237542 from 0.645126479829879\n",
      "Epoch 3/15  loss=0.0903  val_loss=0.1330  f1=0.659  time=120.86s\n",
      "Epoch 4/15  loss=0.0486  val_loss=0.1697  f1=0.654  time=120.20s\n",
      "Epoch 5/15  loss=0.0369  val_loss=0.2553  f1=0.646  time=120.13s\n",
      "Epoch 6/15  loss=0.0312  val_loss=0.2372  f1=0.659  time=120.44s\n",
      "Epoch 7/15  loss=0.0314  val_loss=0.2198  f1=0.666  time=120.29s\n",
      "model_saved at f1: 0.6659932447362606 from 0.6654565141237542\n",
      "Epoch 8/15  loss=0.0297  val_loss=0.2791  f1=0.665  time=120.33s\n",
      "Epoch 9/15  loss=0.0281  val_loss=0.2985  f1=0.662  time=119.70s\n",
      "Epoch 10/15  loss=0.0300  val_loss=0.2785  f1=0.666  time=120.73s\n",
      "Epoch 11/15  loss=0.0307  val_loss=0.2536  f1=0.665  time=120.15s\n",
      "Epoch 12/15  loss=0.0308  val_loss=0.2710  f1=0.669  time=120.33s\n",
      "model_saved at f1: 0.6688971987141639 from 0.6659932447362606\n",
      "Epoch 13/15  loss=0.0288  val_loss=0.2899  f1=0.662  time=120.14s\n",
      "Epoch 14/15  loss=0.0259  val_loss=0.3191  f1=0.657  time=120.76s\n",
      "Epoch 15/15  loss=0.0297  val_loss=0.2695  f1=0.673  time=120.76s\n",
      "model_saved at f1: 0.6733014603050704 from 0.6688971987141639\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1352  val_loss=0.1743  f1=0.584  time=120.37s\n",
      "model_saved at f1: 0.5840051705956391 from 0.0\n",
      "Epoch 2/15  loss=0.0605  val_loss=0.2244  f1=0.603  time=120.81s\n",
      "model_saved at f1: 0.6027819651911744 from 0.5840051705956391\n",
      "Epoch 3/15  loss=0.0434  val_loss=0.2064  f1=0.635  time=120.71s\n",
      "model_saved at f1: 0.6347301666252175 from 0.6027819651911744\n",
      "Epoch 4/15  loss=0.0383  val_loss=0.2401  f1=0.643  time=120.38s\n",
      "model_saved at f1: 0.6428134359601945 from 0.6347301666252175\n",
      "Epoch 5/15  loss=0.0364  val_loss=0.2626  f1=0.642  time=120.15s\n",
      "Epoch 6/15  loss=0.0318  val_loss=0.2892  f1=0.620  time=121.30s\n",
      "Epoch 7/15  loss=0.0309  val_loss=0.1635  f1=0.648  time=120.51s\n",
      "model_saved at f1: 0.6482892188508714 from 0.6428134359601945\n",
      "Epoch 8/15  loss=0.0546  val_loss=0.1825  f1=0.660  time=120.61s\n",
      "model_saved at f1: 0.659791148039184 from 0.6482892188508714\n",
      "Epoch 9/15  loss=0.0481  val_loss=0.2052  f1=0.662  time=120.15s\n",
      "model_saved at f1: 0.6624763044996508 from 0.659791148039184\n",
      "Epoch 10/15  loss=0.0521  val_loss=0.2533  f1=0.667  time=121.34s\n",
      "model_saved at f1: 0.6673757686554761 from 0.6624763044996508\n",
      "Epoch 11/15  loss=0.0448  val_loss=0.2496  f1=0.664  time=119.52s\n",
      "Epoch 12/15  loss=0.0410  val_loss=0.2625  f1=0.667  time=121.11s\n",
      "model_saved at f1: 0.6674438939084815 from 0.6673757686554761\n",
      "Epoch 13/15  loss=0.0335  val_loss=0.2510  f1=0.664  time=120.47s\n",
      "Epoch 14/15  loss=0.0301  val_loss=0.3131  f1=0.664  time=121.19s\n",
      "Epoch 15/15  loss=0.0304  val_loss=0.3233  f1=0.664  time=120.28s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1590  val_loss=0.1102  f1=0.648  time=120.91s\n",
      "model_saved at f1: 0.6483277384060673 from 0.0\n",
      "Epoch 2/15  loss=0.1077  val_loss=0.1034  f1=0.665  time=120.93s\n",
      "model_saved at f1: 0.6647931303669009 from 0.6483277384060673\n",
      "Epoch 3/15  loss=0.0778  val_loss=0.1516  f1=0.655  time=119.68s\n",
      "Epoch 4/15  loss=0.0414  val_loss=0.2120  f1=0.654  time=121.03s\n",
      "Epoch 5/15  loss=0.0319  val_loss=0.2615  f1=0.653  time=120.64s\n",
      "Epoch 6/15  loss=0.0280  val_loss=0.3431  f1=0.640  time=120.08s\n",
      "Epoch 7/15  loss=0.0235  val_loss=0.3463  f1=0.642  time=119.89s\n",
      "Epoch 8/15  loss=0.0246  val_loss=0.3751  f1=0.653  time=120.54s\n",
      "Epoch 9/15  loss=0.0237  val_loss=0.3509  f1=0.639  time=119.98s\n",
      "Epoch 10/15  loss=0.0228  val_loss=0.4016  f1=0.649  time=121.26s\n",
      "Epoch 11/15  loss=0.0217  val_loss=0.4024  f1=0.650  time=120.98s\n",
      "Epoch 12/15  loss=0.0199  val_loss=0.4258  f1=0.641  time=121.10s\n",
      "Epoch 13/15  loss=0.0187  val_loss=0.4552  f1=0.648  time=121.24s\n",
      "Epoch 14/15  loss=0.0170  val_loss=0.4818  f1=0.642  time=120.94s\n",
      "Epoch 15/15  loss=0.0197  val_loss=0.4300  f1=0.651  time=120.64s\n",
      "{'threshold': 0.1563814878463745, 'f1': 0.6624136838770986}\n",
      "f1 score: 0.6727610948711136\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1539  val_loss=0.1057  f1=0.656  time=120.80s\n",
      "model_saved at f1: 0.6561942391027276 from 0.0\n",
      "Epoch 2/15  loss=0.1077  val_loss=0.0998  f1=0.676  time=120.31s\n",
      "model_saved at f1: 0.675806874836001 from 0.6561942391027276\n",
      "Epoch 3/15  loss=0.1014  val_loss=0.0975  f1=0.683  time=121.17s\n",
      "model_saved at f1: 0.6834212897412795 from 0.675806874836001\n",
      "Epoch 4/15  loss=0.0967  val_loss=0.0959  f1=0.689  time=121.48s\n",
      "model_saved at f1: 0.6888723603790059 from 0.6834212897412795\n",
      "Epoch 5/15  loss=0.0925  val_loss=0.0971  f1=0.690  time=121.60s\n",
      "model_saved at f1: 0.6898936345374914 from 0.6888723603790059\n",
      "Epoch 6/15  loss=0.0886  val_loss=0.0965  f1=0.696  time=120.81s\n",
      "model_saved at f1: 0.6963061123074376 from 0.6898936345374914\n",
      "Epoch 7/15  loss=0.0847  val_loss=0.0971  f1=0.693  time=120.72s\n",
      "Epoch 8/15  loss=0.0806  val_loss=0.0968  f1=0.692  time=120.33s\n",
      "Epoch 9/15  loss=0.0769  val_loss=0.1025  f1=0.691  time=121.32s\n",
      "Epoch 10/15  loss=0.0727  val_loss=0.1022  f1=0.687  time=120.06s\n",
      "Epoch 11/15  loss=0.0690  val_loss=0.1059  f1=0.684  time=121.74s\n",
      "Epoch 12/15  loss=0.0653  val_loss=0.1115  f1=0.675  time=120.94s\n",
      "Epoch 13/15  loss=0.0614  val_loss=0.1218  f1=0.683  time=121.19s\n",
      "Epoch 14/15  loss=0.0472  val_loss=0.1655  f1=0.675  time=120.37s\n",
      "Epoch 15/15  loss=0.0387  val_loss=0.1986  f1=0.673  time=120.60s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1547  val_loss=0.1088  f1=0.649  time=120.18s\n",
      "model_saved at f1: 0.6486100769421693 from 0.0\n",
      "Epoch 2/15  loss=0.1094  val_loss=0.1023  f1=0.671  time=119.45s\n",
      "model_saved at f1: 0.6709273262252033 from 0.6486100769421693\n",
      "Epoch 3/15  loss=0.1027  val_loss=0.1029  f1=0.675  time=119.97s\n",
      "model_saved at f1: 0.6745409429280397 from 0.6709273262252033\n",
      "Epoch 4/15  loss=0.0979  val_loss=0.0980  f1=0.682  time=121.15s\n",
      "model_saved at f1: 0.6822767942128308 from 0.6745409429280397\n",
      "Epoch 5/15  loss=0.0936  val_loss=0.0985  f1=0.684  time=120.04s\n",
      "model_saved at f1: 0.6838482471586361 from 0.6822767942128308\n",
      "Epoch 6/15  loss=0.0896  val_loss=0.0980  f1=0.688  time=120.13s\n",
      "model_saved at f1: 0.6877088266779122 from 0.6838482471586361\n",
      "Epoch 7/15  loss=0.0858  val_loss=0.0967  f1=0.686  time=121.22s\n",
      "Epoch 8/15  loss=0.0818  val_loss=0.0988  f1=0.687  time=120.92s\n",
      "Epoch 9/15  loss=0.0777  val_loss=0.1019  f1=0.682  time=119.31s\n",
      "Epoch 10/15  loss=0.0738  val_loss=0.1053  f1=0.681  time=120.49s\n",
      "Epoch 11/15  loss=0.0698  val_loss=0.1073  f1=0.678  time=120.69s\n",
      "Epoch 12/15  loss=0.0660  val_loss=0.1174  f1=0.670  time=120.49s\n",
      "Epoch 13/15  loss=0.0624  val_loss=0.1167  f1=0.670  time=120.07s\n",
      "Epoch 14/15  loss=0.0589  val_loss=0.1190  f1=0.667  time=121.01s\n",
      "Epoch 15/15  loss=0.0560  val_loss=0.1274  f1=0.662  time=120.21s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1609  val_loss=0.1113  f1=0.643  time=120.43s\n",
      "model_saved at f1: 0.643330681516839 from 0.0\n",
      "Epoch 2/15  loss=0.0621  val_loss=0.1745  f1=0.646  time=120.75s\n",
      "model_saved at f1: 0.6458527792976922 from 0.643330681516839\n",
      "Epoch 3/15  loss=0.0443  val_loss=0.1769  f1=0.654  time=121.10s\n",
      "model_saved at f1: 0.6538538672371148 from 0.6458527792976922\n",
      "Epoch 4/15  loss=0.0430  val_loss=0.1824  f1=0.655  time=119.52s\n",
      "model_saved at f1: 0.6548435670068596 from 0.6538538672371148\n",
      "Epoch 5/15  loss=0.0414  val_loss=0.2076  f1=0.662  time=120.84s\n",
      "model_saved at f1: 0.6616127255645187 from 0.6548435670068596\n",
      "Epoch 6/15  loss=0.0336  val_loss=0.2463  f1=0.656  time=119.70s\n",
      "Epoch 7/15  loss=0.0284  val_loss=0.3058  f1=0.642  time=121.18s\n",
      "Epoch 8/15  loss=0.0246  val_loss=0.2882  f1=0.655  time=120.76s\n",
      "Epoch 9/15  loss=0.0271  val_loss=0.2926  f1=0.651  time=120.73s\n",
      "Epoch 10/15  loss=0.0331  val_loss=0.2986  f1=0.633  time=121.11s\n",
      "Epoch 11/15  loss=0.0262  val_loss=0.3366  f1=0.653  time=119.71s\n",
      "Epoch 12/15  loss=0.0226  val_loss=0.3032  f1=0.658  time=120.83s\n",
      "Epoch 13/15  loss=0.0249  val_loss=0.2660  f1=0.660  time=119.62s\n",
      "Epoch 14/15  loss=0.0282  val_loss=0.3295  f1=0.660  time=121.18s\n",
      "Epoch 15/15  loss=0.0289  val_loss=0.3013  f1=0.659  time=120.52s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1642  val_loss=0.1080  f1=0.648  time=120.29s\n",
      "model_saved at f1: 0.6484678259223026 from 0.0\n",
      "Epoch 2/15  loss=0.0756  val_loss=0.1783  f1=0.639  time=120.08s\n",
      "Epoch 3/15  loss=0.0457  val_loss=0.1866  f1=0.649  time=121.62s\n",
      "model_saved at f1: 0.6487333333333334 from 0.6484678259223026\n",
      "Epoch 4/15  loss=0.0403  val_loss=0.2164  f1=0.650  time=119.60s\n",
      "model_saved at f1: 0.6499855086465076 from 0.6487333333333334\n",
      "Epoch 5/15  loss=0.0342  val_loss=0.2424  f1=0.653  time=120.26s\n",
      "model_saved at f1: 0.6533696174163599 from 0.6499855086465076\n",
      "Epoch 6/15  loss=0.0314  val_loss=0.2621  f1=0.654  time=120.99s\n",
      "model_saved at f1: 0.6544996446798889 from 0.6533696174163599\n",
      "Epoch 7/15  loss=0.0270  val_loss=0.2954  f1=0.652  time=121.48s\n",
      "Epoch 8/15  loss=0.0250  val_loss=0.3032  f1=0.655  time=120.30s\n",
      "model_saved at f1: 0.6548826988948491 from 0.6544996446798889\n",
      "Epoch 9/15  loss=0.0250  val_loss=0.3316  f1=0.655  time=120.51s\n",
      "model_saved at f1: 0.6549552535631422 from 0.6548826988948491\n",
      "Epoch 10/15  loss=0.0227  val_loss=0.3799  f1=0.656  time=121.05s\n",
      "model_saved at f1: 0.6563361344537815 from 0.6549552535631422\n",
      "Epoch 11/15  loss=0.0228  val_loss=0.3908  f1=0.652  time=120.25s\n",
      "Epoch 12/15  loss=0.0228  val_loss=0.3470  f1=0.656  time=120.35s\n",
      "Epoch 13/15  loss=0.0257  val_loss=0.3076  f1=0.661  time=120.81s\n",
      "model_saved at f1: 0.6608746205025515 from 0.6563361344537815\n",
      "Epoch 14/15  loss=0.0267  val_loss=0.3237  f1=0.656  time=121.58s\n",
      "Epoch 15/15  loss=0.0281  val_loss=0.3158  f1=0.662  time=120.46s\n",
      "model_saved at f1: 0.6618306767204352 from 0.6608746205025515\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1518  val_loss=0.1098  f1=0.647  time=119.96s\n",
      "model_saved at f1: 0.6473765432098766 from 0.0\n",
      "Epoch 2/15  loss=0.1070  val_loss=0.1018  f1=0.669  time=120.58s\n",
      "model_saved at f1: 0.6685222672064777 from 0.6473765432098766\n",
      "Epoch 3/15  loss=0.1007  val_loss=0.1003  f1=0.675  time=120.56s\n",
      "model_saved at f1: 0.6754920187191488 from 0.6685222672064777\n",
      "Epoch 4/15  loss=0.0962  val_loss=0.0976  f1=0.684  time=119.19s\n",
      "model_saved at f1: 0.6839126919967664 from 0.6754920187191488\n",
      "Epoch 5/15  loss=0.0919  val_loss=0.0989  f1=0.684  time=121.05s\n",
      "Epoch 6/15  loss=0.0880  val_loss=0.0973  f1=0.688  time=120.44s\n",
      "model_saved at f1: 0.6876158940397351 from 0.6839126919967664\n",
      "Epoch 7/15  loss=0.0841  val_loss=0.0990  f1=0.685  time=119.53s\n",
      "Epoch 8/15  loss=0.0801  val_loss=0.1001  f1=0.685  time=119.86s\n",
      "Epoch 9/15  loss=0.0762  val_loss=0.1029  f1=0.682  time=121.56s\n",
      "Epoch 10/15  loss=0.0725  val_loss=0.1053  f1=0.677  time=120.01s\n",
      "Epoch 11/15  loss=0.0685  val_loss=0.1071  f1=0.676  time=119.47s\n",
      "Epoch 12/15  loss=0.0649  val_loss=0.1107  f1=0.673  time=120.43s\n",
      "Epoch 13/15  loss=0.0616  val_loss=0.1157  f1=0.672  time=121.04s\n",
      "Epoch 14/15  loss=0.0582  val_loss=0.1207  f1=0.666  time=119.39s\n",
      "Epoch 15/15  loss=0.0551  val_loss=0.1301  f1=0.666  time=119.91s\n",
      "{'threshold': 0.19383549690246582, 'f1': 0.6631682602204505}\n",
      "f1 score: 0.6852761729928191\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
