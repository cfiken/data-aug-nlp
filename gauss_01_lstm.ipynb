{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# use only for tokenizer and padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1019):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# SEED = 1019\n",
    "# seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "class Config:\n",
    "    num_epochs = 15\n",
    "    batch_size = 512\n",
    "    test_batch_size = 512\n",
    "    vocab_size = 120000\n",
    "    max_length = 72\n",
    "    embedding_size = 300\n",
    "    hidden_size = 64\n",
    "    num_layers = 1\n",
    "    embedding_noise_var = 0.1\n",
    "    embedding_dropout = 0.3\n",
    "    layer_dropout = 0.1\n",
    "    dense_size = [hidden_size*2*4, int(hidden_size/4)] # depend on concat num\n",
    "    output_size = 1\n",
    "    num_cv_splits = 5\n",
    "    learning_rate = 0.001\n",
    "    clip_grad = 5.0\n",
    "    embeddings = ['glove', 'paragram', 'fasttext']\n",
    "    datadir = Path('./data/')\n",
    "    # datadir = Path('../input') # for kernel\n",
    "\n",
    "c = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [\n",
    "    ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '¬', '░', '¶', '↑', '±',  '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '₹', '´'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"'re\": \" are\",\n",
    "    \"tryin'\": \"trying\",\n",
    "    \"doesn'\": \"does not\",\n",
    "    'howdo': 'how do',\n",
    "    'whatare': 'what are',\n",
    "    'howcan': 'how can',\n",
    "    'howmuch': 'how much',\n",
    "    'howmany': 'how many',\n",
    "    'whydo': 'why do',\n",
    "    'doI': 'do I',\n",
    "    'theBest': 'the best',\n",
    "    'howdoes': 'how does',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spells = {\n",
    "    'colour': 'color',\n",
    "    'centre': 'center',\n",
    "    'favourite': 'favorite',\n",
    "    'travelling': 'traveling',\n",
    "    'counselling': 'counseling',\n",
    "    'theatre': 'theater',\n",
    "    'cancelled': 'canceled',\n",
    "    'labour': 'labor',\n",
    "    'organisation': 'organization',\n",
    "    'wwii': 'world war 2',\n",
    "    'citicise': 'criticize',\n",
    "    'youtu.be': 'youtube',\n",
    "    'youtu ': 'youtube ',\n",
    "    'qoura': 'quora',\n",
    "    'sallary': 'salary',\n",
    "    'Whta': 'what',\n",
    "    'whta': 'what',\n",
    "    'narcisist': 'narcissist',\n",
    "    'mastrubation': 'masturbation',\n",
    "    'mastrubate': 'masturbate',\n",
    "    \"mastrubating\": 'masturbating',\n",
    "    'pennis': 'penis',\n",
    "    'Etherium': 'ethereum',\n",
    "    'etherium': 'ethereum',\n",
    "    'narcissit': 'narcissist',\n",
    "    'bigdata': 'big data',\n",
    "    '2k17': '2017',\n",
    "    '2k18': '2018',\n",
    "    'qouta': 'quota',\n",
    "    'exboyfriend': 'ex boyfriend',\n",
    "    'exgirlfriend': 'ex girlfriend',\n",
    "    'airhostess': 'air hostess',\n",
    "    \"whst\": 'what',\n",
    "    'watsapp': 'whatsapp',\n",
    "    'demonitisation': 'demonetization',\n",
    "    'demonitization': 'demonetization',\n",
    "    'demonetisation': 'demonetization',\n",
    "    'quorans': 'quora user',\n",
    "    'quoran': 'quora user',\n",
    "    'pokémon': 'pokemon',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    train_df = pd.read_csv(datadir / 'train_local.csv')\n",
    "    test_df = pd.read_csv(datadir / 'test_local.csv')\n",
    "    print(\"Train shape : \", train_df.shape)\n",
    "    print(\"Test shape : \", test_df.shape)\n",
    "    return train_df, test_df\n",
    "\n",
    "def clean(df):\n",
    "    df = clean_lower(df)\n",
    "    df = clean_unicode(df)\n",
    "    df = clean_abbreviation(df, abbreviations)\n",
    "    df = clean_spells(df, spells)\n",
    "    df = clean_language(df)\n",
    "    df = clean_puncts(df, puncts)\n",
    "    df = clean_space(df)\n",
    "    return df\n",
    "\n",
    "def clean_unicode(df):\n",
    "    codes = ['\\x7f', '\\u200b', '\\xa0', '\\ufeff', '\\u200e', '\\u202a', '\\u202c', '\\u2060', '\\uf0d8', '\\ue019', '\\uf02d', '\\u200f', '\\u2061', '\\ue01b']\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: _clean_unicode(x, codes))\n",
    "    return df\n",
    "\n",
    "def _clean_unicode(x, codes):\n",
    "    for u in codes:\n",
    "        if u in x:\n",
    "            x = x.replace(u, '')\n",
    "    return x\n",
    "\n",
    "def clean_language(df):\n",
    "    langs1 = r'[\\p{Katakana}\\p{Hiragana}\\p{Han}]' # regex\n",
    "    langs2 = r'[ஆய்தஎழுத்துஆயுதஎழுத்துशुषछछशुषدوउसशुष북한내제តើបងប្អូនមានមធ្យបាយអ្វីខ្លះដើម្បីរកឃើញឯកសារអំពីប្រវត្តិស្ត្រនៃប្រាសាទអង្គរវट्टरौरआदસંઘરાજ્યपीतऊनअहএকটিবাড়িএকটিখামারএরঅধীনেপদেরবাছাইপরীক্ষাএরপ্রশ্নওউত্তরসহকোথায়পেতেপারিص、。Емелядуракلكلمقاممقال수능ί서로가를행복하게기乡국고등학교는몇시간업니《》싱관없어나이रचा키کپڤ」मिलगईकलेजेकोठंडकऋॠऌॡर]'\n",
    "    compiled_langs1 = regex.compile(langs1)\n",
    "    compiled_langs2 = re.compile(langs2)\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs1))\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs2))\n",
    "    return df\n",
    "\n",
    "def _clean_language(x, compiled_re):\n",
    "    return compiled_re.sub(' <lang> ', x)\n",
    "\n",
    "def clean_lower(df):\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "def clean_puncts(df, puncts):\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_puncts(x, puncts))\n",
    "    return df\n",
    "    \n",
    "def _clean_puncts(x, puncts):\n",
    "    x = str(x)\n",
    "    # added space around puncts after replace\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_spells(df, spells):\n",
    "    compiled_spells = re.compile('(%s)' % '|'.join(spells.keys()))\n",
    "    def replace(match):\n",
    "        return spells[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_spells(x, compiled_spells, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_spells(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_abbreviation(df, abbreviations):\n",
    "    compiled_abbreviation = re.compile('(%s)' % '|'.join(abbreviations.keys()))\n",
    "    def replace(match):\n",
    "        return abbreviations[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_abreviation(x, compiled_abbreviation, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_abreviation(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_space(df):\n",
    "    compiled_re = re.compile(r\"\\s+\")\n",
    "    df['question_text'] = df[\"question_text\"].apply(lambda x: _clean_space(x, compiled_re))\n",
    "    return df\n",
    "\n",
    "def _clean_space(x, compiled_re):\n",
    "    return compiled_re.sub(\" \", x)\n",
    "        \n",
    "def prepare_tokenizer(texts, max_words):\n",
    "    tokenizer = Tokenizer(num_words=max_words, filters='', oov_token='<unk>')\n",
    "    tokenizer.fit_on_texts(list(texts))\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_and_padding(texts, tokenizer, max_length):\n",
    "    texts = tokenizer.texts_to_sequences(texts)\n",
    "    texts = pad_sequences(texts, maxlen=max_length)\n",
    "    return texts\n",
    "\n",
    "def get_all_vocabs(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config, tokenizer, all_vocabs, embedding_weights = None):\n",
    "        super(Embeddings, self).__init__()\n",
    "        \n",
    "        self.embedding_map = {\n",
    "            'fasttext': self._load_fasttext,\n",
    "            'glove': self._load_glove,\n",
    "            'paragram': self._load_paragram\n",
    "        }\n",
    "        self.c = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.all_vocabs = all_vocabs\n",
    "        \n",
    "        if embedding_weights is None:\n",
    "            embedding_weights = self._load_embeddings(self.c.embeddings)\n",
    "            \n",
    "        self.original_embedding_weights = embedding_weights\n",
    "        self.embeddings = nn.Embedding(self.c.vocab_size + 1, self.c.embedding_size, padding_idx=0)\n",
    "        self.embeddings.weight = nn.Parameter(embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.embedding_dropout = nn.Dropout2d(self.c.embedding_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.embeddings(x)\n",
    "        if self.training:\n",
    "            embedding += torch.randn_like(embedding) * self.c.embedding_noise_var\n",
    "        return embedding\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        self.embeddings.weight = nn.Parameter(self.original_embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "    \n",
    "    def _load_embeddings(self, embedding_list: list):\n",
    "        embedding_weights = np.zeros((self.c.vocab_size, self.c.embedding_size))\n",
    "        pool = Pool(num_cores)\n",
    "        embedding_weights = np.mean(pool.map(self._load_an_embedding, embedding_list), 0)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return torch.tensor(embedding_weights, dtype=torch.float32)\n",
    "\n",
    "    def _load_an_embedding(self, emb):\n",
    "        return self.embedding_map[emb](self.tokenizer.word_index)\n",
    "        \n",
    "    def _get_embeddings_pair(self, word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "        \n",
    "    def _make_embeddings(self, embeddings_index, word_index, emb_mean, emb_std):\n",
    "        nb_words = min(self.c.vocab_size, len(word_index))\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, self.c.embedding_size))\n",
    "        embedding_matrix[0] = np.zeros(self.c.embedding_size)\n",
    "        for word, i in word_index.items():\n",
    "            if i >= self.c.vocab_size:\n",
    "                continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return embedding_matrix\n",
    "    \n",
    "    def _load_glove(self, word_index):\n",
    "        print('loading glove')\n",
    "        filepath = self.c.datadir / 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "    \n",
    "    def _load_fasttext(self, word_index):    \n",
    "        print('loading fasttext')\n",
    "        filepath = self.c.datadir / 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0033469985, 0.109855495\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "\n",
    "    def _load_paragram(self, word_index):\n",
    "        print('loading paragram')\n",
    "        filepath = self.c.datadir / 'embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath, encoding=\"utf8\", errors='ignore')\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0053247833, 0.49346462\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 2\n",
    "def df_parallelize_run(df, func, num_cores=2):\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1175509, 3)\n",
      "Test shape :  (130613, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_data(c.datadir)\n",
    "train_df = df_parallelize_run(train_df, clean)\n",
    "test_df = df_parallelize_run(test_df, clean)\n",
    "train_x, train_y = train_df['question_text'].values, train_df['target'].values\n",
    "test_x = test_df['question_text'].values\n",
    "tokenizer = prepare_tokenizer(train_x, c.vocab_size)\n",
    "train_x = tokenize_and_padding(train_x, tokenizer, c.max_length)\n",
    "test_x = tokenize_and_padding(test_x, tokenizer, c.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_vocabs:  184279\n",
      "loading glove\n",
      "loading paragram\n",
      "loading fasttext\n",
      "48.77651238441467\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_vocabs = get_all_vocabs(train_df['question_text'])\n",
    "print('all_vocabs: ', len(all_vocabs))\n",
    "embeddings = Embeddings(c, tokenizer, all_vocabs)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(GRULayer, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          bias=False,\n",
    "                          bidirectional=True,\n",
    "                          batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_outputs, gru_state = self.gru(x)\n",
    "        return self.dropout(gru_outputs), gru_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=False,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_outputs, (lstm_states, _) = self.lstm(x)\n",
    "        return self.dropout(lstm_outputs), lstm_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, config: Config, embeddings):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.c = config\n",
    "        \n",
    "        self.embedding = embeddings\n",
    "        self.lstm1 = LSTMLayer(input_size=self.c.embedding_size,\n",
    "                              hidden_size=self.c.hidden_size,\n",
    "                              num_layers=self.c.num_layers,\n",
    "                              dropout_rate=self.c.layer_dropout)\n",
    "        self.lstm2 = LSTMLayer(input_size=self.c.hidden_size*2,\n",
    "                            hidden_size=self.c.hidden_size,\n",
    "                            num_layers=self.c.num_layers,\n",
    "                            dropout_rate=self.c.layer_dropout)\n",
    "        \n",
    "        self.cell_dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.linear = nn.Linear(self.c.dense_size[0], self.c.dense_size[1])\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(self.c.dense_size[1])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.out = nn.Linear(self.c.dense_size[1], self.c.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        o_lstm1, h_lstm1 = self.lstm1(h_embedding)\n",
    "        o_lstm2, h_lstm2 = self.lstm2(o_lstm1)\n",
    "        \n",
    "        avg_pool = torch.mean(o_lstm2, 1)\n",
    "        max_pool, _ = torch.max(o_lstm2, 1)\n",
    "        \n",
    "        h_lstm1 = self.cell_dropout(torch.cat(h_lstm1.split(1, 0), -1).squeeze(0))\n",
    "        h_lstm2 = self.cell_dropout(torch.cat(h_lstm2.split(1, 0), -1).squeeze(0))\n",
    "\n",
    "        concat = torch.cat([h_lstm1, h_lstm2, avg_pool, max_pool], 1)\n",
    "        concat = self.linear(concat)\n",
    "        concat = self.batch_norm(concat)\n",
    "        concat = self.relu(concat)\n",
    "        concat = self.dropout(concat)\n",
    "        out = self.out(concat)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_length(data, mask):\n",
    "    max_length = data.shape[1]\n",
    "    transposed = torch.transpose(data, 1, 0)\n",
    "    res = (transposed == mask).all(1)\n",
    "    for i, r in enumerate(res):\n",
    "        if r == 0:\n",
    "            break\n",
    "    data = data[:, -(max_length - i):]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_x, train_y, test_x, c, embeddings, trial=0):\n",
    "    splits = list(StratifiedKFold(n_splits=c.num_cv_splits, shuffle=True).split(train_x, train_y))\n",
    "    x_test_cuda = torch.tensor(test_x, dtype=torch.long).cuda(cuda_idx)\n",
    "    test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=c.test_batch_size, shuffle=False)\n",
    "    train_preds = np.zeros((len(train_x)))\n",
    "    test_preds = np.zeros((len(test_x)))\n",
    "\n",
    "    mask = torch.zeros((c.max_length, 1), dtype=torch.long).cuda(cuda_idx)\n",
    "    \n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        x_train_fold = torch.tensor(train_x[train_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "        x_val_fold = torch.tensor(train_x[valid_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "\n",
    "        model = SimpleRNN(c, embeddings)\n",
    "        model.cuda(cuda_idx)\n",
    "\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=c.learning_rate)\n",
    "\n",
    "        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "        valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=c.batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=c.test_batch_size, shuffle=False)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        print(f'Fold {i + 1}')\n",
    "\n",
    "        for epoch in range(c.num_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred = model(x_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), c.clip_grad)\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "            avg_val_loss = 0.\n",
    "\n",
    "            # validation prediction\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred = model(x_batch).detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            search_result = threshold_search(y_val_fold.cpu().numpy(), valid_preds_fold)\n",
    "            valid_pred_targets = valid_preds_fold > search_result['threshold']\n",
    "            val_f1 = f1_score(y_val_fold.cpu().numpy(), valid_pred_targets)\n",
    "\n",
    "            elapsed_time = time.time() - start_time \n",
    "            print('Epoch {}/{}  loss={:.4f}  val_loss={:.4f}  f1={:.3f}  time={:.2f}s'.format(\n",
    "                epoch + 1, c.num_epochs, avg_loss, avg_val_loss, val_f1, elapsed_time))\n",
    "            if best_f1 < val_f1:\n",
    "                print(f'model_saved at f1: {val_f1} from {best_f1}')\n",
    "                ckpt_path = Path(f'./ckpt/gauss/{trial}/')\n",
    "                if not ckpt_path.exists():\n",
    "                    ckpt_path.mkdir(parents=True)\n",
    "                torch.save(model.state_dict(),ckpt_path / f'{i}_model.pt')\n",
    "                best_f1 = val_f1\n",
    "                best_epoch = epoch\n",
    "\n",
    "        # test prediction\n",
    "        model.load_state_dict(torch.load(f'./ckpt/gauss/{trial}/{i}_model.pt'))  # load best model\n",
    "        test_preds_fold = np.zeros(len(test_x))\n",
    "        for i, (x_batch, ) in enumerate(test_loader):\n",
    "            x_batch = cut_length(x_batch, mask)\n",
    "            y_pred = model(x_batch).detach()\n",
    "            test_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "        train_preds[valid_idx] = valid_preds_fold\n",
    "        test_preds += test_preds_fold / len(splits)\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1557  val_loss=0.1100  f1=0.641  time=65.89s\n",
      "model_saved at f1: 0.640981626295378 from 0.0\n",
      "Epoch 2/15  loss=0.1083  val_loss=0.1031  f1=0.665  time=124.11s\n",
      "model_saved at f1: 0.6650526179869586 from 0.640981626295378\n",
      "Epoch 3/15  loss=0.1027  val_loss=0.1025  f1=0.676  time=125.97s\n",
      "model_saved at f1: 0.6756417498274022 from 0.6650526179869586\n",
      "Epoch 4/15  loss=0.0987  val_loss=0.0999  f1=0.681  time=125.51s\n",
      "model_saved at f1: 0.681150240689131 from 0.6756417498274022\n",
      "Epoch 5/15  loss=0.0952  val_loss=0.1003  f1=0.686  time=126.21s\n",
      "model_saved at f1: 0.6857984754339889 from 0.681150240689131\n",
      "Epoch 6/15  loss=0.0921  val_loss=0.0994  f1=0.685  time=125.68s\n",
      "Epoch 7/15  loss=0.0893  val_loss=0.0999  f1=0.687  time=125.59s\n",
      "model_saved at f1: 0.6865080632284848 from 0.6857984754339889\n",
      "Epoch 8/15  loss=0.0870  val_loss=0.1008  f1=0.685  time=125.79s\n",
      "Epoch 9/15  loss=0.0841  val_loss=0.1016  f1=0.682  time=126.31s\n",
      "Epoch 10/15  loss=0.0815  val_loss=0.1002  f1=0.685  time=125.81s\n",
      "Epoch 11/15  loss=0.0799  val_loss=0.1009  f1=0.686  time=125.80s\n",
      "Epoch 12/15  loss=0.0775  val_loss=0.1037  f1=0.685  time=126.00s\n",
      "Epoch 13/15  loss=0.0758  val_loss=0.1075  f1=0.683  time=125.44s\n",
      "Epoch 14/15  loss=0.0736  val_loss=0.1127  f1=0.679  time=126.14s\n",
      "Epoch 15/15  loss=0.0721  val_loss=0.1137  f1=0.682  time=125.98s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1444  val_loss=0.1072  f1=0.650  time=124.86s\n",
      "model_saved at f1: 0.6499350930333189 from 0.0\n",
      "Epoch 2/15  loss=0.1068  val_loss=0.1043  f1=0.670  time=125.86s\n",
      "model_saved at f1: 0.6701460713490118 from 0.6499350930333189\n",
      "Epoch 3/15  loss=0.1013  val_loss=0.0988  f1=0.680  time=126.71s\n",
      "model_saved at f1: 0.6797131081037224 from 0.6701460713490118\n",
      "Epoch 4/15  loss=0.0976  val_loss=0.1019  f1=0.685  time=126.24s\n",
      "model_saved at f1: 0.6846506784679964 from 0.6797131081037224\n",
      "Epoch 5/15  loss=0.0942  val_loss=0.0987  f1=0.687  time=125.94s\n",
      "model_saved at f1: 0.6871906553157792 from 0.6846506784679964\n",
      "Epoch 6/15  loss=0.0912  val_loss=0.1027  f1=0.690  time=126.46s\n",
      "model_saved at f1: 0.6901533309255672 from 0.6871906553157792\n",
      "Epoch 7/15  loss=0.0884  val_loss=0.1028  f1=0.686  time=125.73s\n",
      "Epoch 8/15  loss=0.0858  val_loss=0.1044  f1=0.687  time=125.78s\n",
      "Epoch 9/15  loss=0.0833  val_loss=0.1051  f1=0.688  time=126.42s\n",
      "Epoch 10/15  loss=0.0812  val_loss=0.1072  f1=0.687  time=126.36s\n",
      "Epoch 11/15  loss=0.0787  val_loss=0.1040  f1=0.687  time=126.21s\n",
      "Epoch 12/15  loss=0.0763  val_loss=0.1110  f1=0.684  time=126.74s\n",
      "Epoch 13/15  loss=0.0746  val_loss=0.1112  f1=0.684  time=126.66s\n",
      "Epoch 14/15  loss=0.0728  val_loss=0.1113  f1=0.684  time=126.34s\n",
      "Epoch 15/15  loss=0.0713  val_loss=0.1173  f1=0.679  time=126.39s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1510  val_loss=0.1107  f1=0.647  time=123.75s\n",
      "model_saved at f1: 0.6465161923454367 from 0.0\n",
      "Epoch 2/15  loss=0.1076  val_loss=0.1055  f1=0.664  time=126.02s\n",
      "model_saved at f1: 0.6636471543699632 from 0.6465161923454367\n",
      "Epoch 3/15  loss=0.1017  val_loss=0.1065  f1=0.672  time=125.85s\n",
      "model_saved at f1: 0.6716925915322709 from 0.6636471543699632\n",
      "Epoch 4/15  loss=0.0976  val_loss=0.1061  f1=0.679  time=125.30s\n",
      "model_saved at f1: 0.6792096057483218 from 0.6716925915322709\n",
      "Epoch 5/15  loss=0.0940  val_loss=0.1034  f1=0.683  time=125.88s\n",
      "model_saved at f1: 0.6830211028334527 from 0.6792096057483218\n",
      "Epoch 6/15  loss=0.0911  val_loss=0.1012  f1=0.685  time=125.61s\n",
      "model_saved at f1: 0.6852480000000001 from 0.6830211028334527\n",
      "Epoch 7/15  loss=0.0881  val_loss=0.1043  f1=0.683  time=125.36s\n",
      "Epoch 8/15  loss=0.0856  val_loss=0.1046  f1=0.681  time=125.90s\n",
      "Epoch 9/15  loss=0.0830  val_loss=0.1086  f1=0.682  time=125.84s\n",
      "Epoch 10/15  loss=0.0808  val_loss=0.1050  f1=0.683  time=125.49s\n",
      "Epoch 11/15  loss=0.0786  val_loss=0.1084  f1=0.680  time=125.64s\n",
      "Epoch 12/15  loss=0.0765  val_loss=0.1128  f1=0.679  time=125.86s\n",
      "Epoch 13/15  loss=0.0747  val_loss=0.1095  f1=0.679  time=125.43s\n",
      "Epoch 14/15  loss=0.0728  val_loss=0.1104  f1=0.678  time=125.99s\n",
      "Epoch 15/15  loss=0.0710  val_loss=0.1149  f1=0.677  time=126.11s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1615  val_loss=0.1135  f1=0.636  time=124.16s\n",
      "model_saved at f1: 0.6360323350119268 from 0.0\n",
      "Epoch 2/15  loss=0.1081  val_loss=0.1039  f1=0.665  time=125.81s\n",
      "model_saved at f1: 0.6651279092133102 from 0.6360323350119268\n",
      "Epoch 3/15  loss=0.1024  val_loss=0.1010  f1=0.676  time=126.03s\n",
      "model_saved at f1: 0.675567942444178 from 0.6651279092133102\n",
      "Epoch 4/15  loss=0.0986  val_loss=0.1031  f1=0.682  time=125.89s\n",
      "model_saved at f1: 0.6817737716284787 from 0.675567942444178\n",
      "Epoch 5/15  loss=0.0955  val_loss=0.1030  f1=0.681  time=125.60s\n",
      "Epoch 6/15  loss=0.0929  val_loss=0.0999  f1=0.684  time=125.94s\n",
      "model_saved at f1: 0.6838266838266838 from 0.6817737716284787\n",
      "Epoch 7/15  loss=0.0900  val_loss=0.0988  f1=0.686  time=125.77s\n",
      "model_saved at f1: 0.6863202274106486 from 0.6838266838266838\n",
      "Epoch 8/15  loss=0.0875  val_loss=0.1061  f1=0.684  time=125.91s\n",
      "Epoch 9/15  loss=0.0854  val_loss=0.1045  f1=0.684  time=126.19s\n",
      "Epoch 10/15  loss=0.0829  val_loss=0.1035  f1=0.684  time=125.90s\n",
      "Epoch 11/15  loss=0.0809  val_loss=0.1072  f1=0.681  time=125.83s\n",
      "Epoch 12/15  loss=0.0790  val_loss=0.1063  f1=0.681  time=126.27s\n",
      "Epoch 13/15  loss=0.0768  val_loss=0.1040  f1=0.680  time=126.02s\n",
      "Epoch 14/15  loss=0.0751  val_loss=0.1086  f1=0.679  time=125.39s\n",
      "Epoch 15/15  loss=0.0732  val_loss=0.1170  f1=0.677  time=126.06s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1556  val_loss=0.1085  f1=0.647  time=124.22s\n",
      "model_saved at f1: 0.6466155810983397 from 0.0\n",
      "Epoch 2/15  loss=0.1082  val_loss=0.1029  f1=0.667  time=126.08s\n",
      "model_saved at f1: 0.6673903032695216 from 0.6466155810983397\n",
      "Epoch 3/15  loss=0.1025  val_loss=0.1020  f1=0.680  time=125.69s\n",
      "model_saved at f1: 0.6795166858457997 from 0.6673903032695216\n",
      "Epoch 4/15  loss=0.0986  val_loss=0.1014  f1=0.679  time=125.52s\n",
      "Epoch 5/15  loss=0.0952  val_loss=0.0997  f1=0.686  time=126.23s\n",
      "model_saved at f1: 0.686494167410198 from 0.6795166858457997\n",
      "Epoch 6/15  loss=0.0924  val_loss=0.1020  f1=0.685  time=125.70s\n",
      "Epoch 7/15  loss=0.0897  val_loss=0.0995  f1=0.689  time=125.27s\n",
      "model_saved at f1: 0.6894572025052192 from 0.686494167410198\n",
      "Epoch 8/15  loss=0.0868  val_loss=0.1009  f1=0.689  time=125.65s\n",
      "Epoch 9/15  loss=0.0843  val_loss=0.1007  f1=0.691  time=125.89s\n",
      "model_saved at f1: 0.690711882126172 from 0.6894572025052192\n",
      "Epoch 10/15  loss=0.0820  val_loss=0.1123  f1=0.690  time=125.49s\n",
      "Epoch 11/15  loss=0.0796  val_loss=0.1057  f1=0.688  time=126.27s\n",
      "Epoch 12/15  loss=0.0777  val_loss=0.1096  f1=0.684  time=125.81s\n",
      "Epoch 13/15  loss=0.0757  val_loss=0.1066  f1=0.686  time=125.59s\n",
      "Epoch 14/15  loss=0.0739  val_loss=0.1160  f1=0.685  time=126.00s\n",
      "Epoch 15/15  loss=0.0722  val_loss=0.1091  f1=0.683  time=126.16s\n",
      "{'threshold': 0.23245945572853088, 'f1': 0.6789619150832686}\n",
      "f1 score: 0.6895503615973168\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1579  val_loss=0.1117  f1=0.647  time=123.33s\n",
      "model_saved at f1: 0.6472036359045574 from 0.0\n",
      "Epoch 2/15  loss=0.1073  val_loss=0.1077  f1=0.669  time=125.75s\n",
      "model_saved at f1: 0.6691616287624348 from 0.6472036359045574\n",
      "Epoch 3/15  loss=0.1017  val_loss=0.1039  f1=0.677  time=126.19s\n",
      "model_saved at f1: 0.6767928758644647 from 0.6691616287624348\n",
      "Epoch 4/15  loss=0.0980  val_loss=0.1003  f1=0.681  time=125.79s\n",
      "model_saved at f1: 0.6808074991201971 from 0.6767928758644647\n",
      "Epoch 5/15  loss=0.0946  val_loss=0.1035  f1=0.681  time=125.74s\n",
      "Epoch 6/15  loss=0.0919  val_loss=0.1042  f1=0.687  time=126.27s\n",
      "model_saved at f1: 0.687035560516664 from 0.6808074991201971\n",
      "Epoch 7/15  loss=0.0890  val_loss=0.1024  f1=0.686  time=125.66s\n",
      "Epoch 8/15  loss=0.0865  val_loss=0.1001  f1=0.686  time=126.06s\n",
      "Epoch 9/15  loss=0.0839  val_loss=0.1052  f1=0.686  time=126.29s\n",
      "Epoch 10/15  loss=0.0815  val_loss=0.1032  f1=0.684  time=125.88s\n",
      "Epoch 11/15  loss=0.0793  val_loss=0.1095  f1=0.685  time=125.85s\n",
      "Epoch 12/15  loss=0.0773  val_loss=0.1132  f1=0.681  time=126.45s\n",
      "Epoch 13/15  loss=0.0752  val_loss=0.1132  f1=0.682  time=126.05s\n",
      "Epoch 14/15  loss=0.0736  val_loss=0.1136  f1=0.678  time=125.65s\n",
      "Epoch 15/15  loss=0.0721  val_loss=0.1172  f1=0.677  time=125.71s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1476  val_loss=0.1069  f1=0.650  time=123.20s\n",
      "model_saved at f1: 0.6503997969285443 from 0.0\n",
      "Epoch 2/15  loss=0.1080  val_loss=0.1049  f1=0.670  time=125.43s\n",
      "model_saved at f1: 0.6698952879581151 from 0.6503997969285443\n",
      "Epoch 3/15  loss=0.1020  val_loss=0.1025  f1=0.676  time=125.67s\n",
      "model_saved at f1: 0.6760733466806496 from 0.6698952879581151\n",
      "Epoch 4/15  loss=0.0981  val_loss=0.1014  f1=0.685  time=125.17s\n",
      "model_saved at f1: 0.6847466424446895 from 0.6760733466806496\n",
      "Epoch 5/15  loss=0.0944  val_loss=0.0990  f1=0.688  time=125.93s\n",
      "model_saved at f1: 0.687502026260334 from 0.6847466424446895\n",
      "Epoch 6/15  loss=0.0916  val_loss=0.1008  f1=0.686  time=125.76s\n",
      "Epoch 7/15  loss=0.0885  val_loss=0.0988  f1=0.689  time=125.54s\n",
      "model_saved at f1: 0.6891567806094715 from 0.687502026260334\n",
      "Epoch 8/15  loss=0.0861  val_loss=0.1016  f1=0.690  time=125.68s\n",
      "model_saved at f1: 0.6897366865848054 from 0.6891567806094715\n",
      "Epoch 9/15  loss=0.0834  val_loss=0.1061  f1=0.687  time=126.07s\n",
      "Epoch 10/15  loss=0.0810  val_loss=0.1097  f1=0.686  time=125.37s\n",
      "Epoch 11/15  loss=0.0789  val_loss=0.1100  f1=0.686  time=125.81s\n",
      "Epoch 12/15  loss=0.0769  val_loss=0.1084  f1=0.685  time=125.90s\n",
      "Epoch 13/15  loss=0.0749  val_loss=0.1086  f1=0.683  time=125.34s\n",
      "Epoch 14/15  loss=0.0729  val_loss=0.1197  f1=0.683  time=125.66s\n",
      "Epoch 15/15  loss=0.0712  val_loss=0.1165  f1=0.680  time=126.08s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1562  val_loss=0.1105  f1=0.650  time=124.22s\n",
      "model_saved at f1: 0.6499492127983748 from 0.0\n",
      "Epoch 2/15  loss=0.1077  val_loss=0.1046  f1=0.666  time=126.09s\n",
      "model_saved at f1: 0.6657530754031487 from 0.6499492127983748\n",
      "Epoch 3/15  loss=0.1018  val_loss=0.1034  f1=0.675  time=125.75s\n",
      "model_saved at f1: 0.6746636910296944 from 0.6657530754031487\n",
      "Epoch 4/15  loss=0.0977  val_loss=0.1013  f1=0.682  time=125.72s\n",
      "model_saved at f1: 0.6816957414058491 from 0.6746636910296944\n",
      "Epoch 5/15  loss=0.0940  val_loss=0.1008  f1=0.683  time=126.51s\n",
      "model_saved at f1: 0.6832193989423448 from 0.6816957414058491\n",
      "Epoch 6/15  loss=0.0908  val_loss=0.1022  f1=0.685  time=126.47s\n",
      "model_saved at f1: 0.6847269763651181 from 0.6832193989423448\n",
      "Epoch 7/15  loss=0.0881  val_loss=0.1061  f1=0.685  time=126.12s\n",
      "model_saved at f1: 0.6852509199856711 from 0.6847269763651181\n",
      "Epoch 8/15  loss=0.0852  val_loss=0.1013  f1=0.688  time=126.34s\n",
      "model_saved at f1: 0.6878450363196126 from 0.6852509199856711\n",
      "Epoch 9/15  loss=0.0829  val_loss=0.1090  f1=0.687  time=125.87s\n",
      "Epoch 10/15  loss=0.0805  val_loss=0.1069  f1=0.684  time=125.55s\n",
      "Epoch 11/15  loss=0.0784  val_loss=0.1070  f1=0.686  time=126.13s\n",
      "Epoch 12/15  loss=0.0763  val_loss=0.1098  f1=0.681  time=126.49s\n",
      "Epoch 13/15  loss=0.0744  val_loss=0.1169  f1=0.683  time=126.05s\n",
      "Epoch 14/15  loss=0.0726  val_loss=0.1184  f1=0.678  time=126.32s\n",
      "Epoch 15/15  loss=0.0706  val_loss=0.1176  f1=0.678  time=126.43s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1480  val_loss=0.1101  f1=0.647  time=123.25s\n",
      "model_saved at f1: 0.6470260568174786 from 0.0\n",
      "Epoch 2/15  loss=0.1077  val_loss=0.1043  f1=0.666  time=126.10s\n",
      "model_saved at f1: 0.6662360312641303 from 0.6470260568174786\n",
      "Epoch 3/15  loss=0.1016  val_loss=0.1015  f1=0.677  time=125.88s\n",
      "model_saved at f1: 0.6771582151415063 from 0.6662360312641303\n",
      "Epoch 4/15  loss=0.0979  val_loss=0.1050  f1=0.683  time=125.36s\n",
      "model_saved at f1: 0.6826315439366726 from 0.6771582151415063\n",
      "Epoch 5/15  loss=0.0944  val_loss=0.1079  f1=0.687  time=126.65s\n",
      "model_saved at f1: 0.6865385884993729 from 0.6826315439366726\n",
      "Epoch 6/15  loss=0.0917  val_loss=0.0995  f1=0.685  time=126.02s\n",
      "Epoch 7/15  loss=0.0890  val_loss=0.1026  f1=0.688  time=125.16s\n",
      "model_saved at f1: 0.6881922404300796 from 0.6865385884993729\n",
      "Epoch 8/15  loss=0.0863  val_loss=0.1055  f1=0.688  time=125.95s\n",
      "Epoch 9/15  loss=0.0837  val_loss=0.1048  f1=0.687  time=126.01s\n",
      "Epoch 10/15  loss=0.0818  val_loss=0.1053  f1=0.684  time=126.03s\n",
      "Epoch 11/15  loss=0.0796  val_loss=0.1104  f1=0.685  time=126.46s\n",
      "Epoch 12/15  loss=0.0777  val_loss=0.1040  f1=0.686  time=126.22s\n",
      "Epoch 13/15  loss=0.0756  val_loss=0.1099  f1=0.683  time=126.01s\n",
      "Epoch 14/15  loss=0.0739  val_loss=0.1119  f1=0.684  time=125.60s\n",
      "Epoch 15/15  loss=0.0723  val_loss=0.1138  f1=0.682  time=125.57s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1539  val_loss=0.1084  f1=0.649  time=123.88s\n",
      "model_saved at f1: 0.6485697050512748 from 0.0\n",
      "Epoch 2/15  loss=0.1079  val_loss=0.1041  f1=0.662  time=126.14s\n",
      "model_saved at f1: 0.6623735050597975 from 0.6485697050512748\n",
      "Epoch 3/15  loss=0.1023  val_loss=0.1012  f1=0.680  time=125.98s\n",
      "model_saved at f1: 0.6796427143047594 from 0.6623735050597975\n",
      "Epoch 4/15  loss=0.0983  val_loss=0.1006  f1=0.684  time=125.82s\n",
      "model_saved at f1: 0.6838069176186081 from 0.6796427143047594\n",
      "Epoch 5/15  loss=0.0949  val_loss=0.0997  f1=0.687  time=125.94s\n",
      "model_saved at f1: 0.6874224047368923 from 0.6838069176186081\n",
      "Epoch 6/15  loss=0.0919  val_loss=0.0992  f1=0.685  time=126.41s\n",
      "Epoch 7/15  loss=0.0892  val_loss=0.0993  f1=0.689  time=125.97s\n",
      "model_saved at f1: 0.6893310355632148 from 0.6874224047368923\n",
      "Epoch 8/15  loss=0.0865  val_loss=0.0989  f1=0.691  time=125.83s\n",
      "model_saved at f1: 0.6905841212278203 from 0.6893310355632148\n",
      "Epoch 9/15  loss=0.0842  val_loss=0.1000  f1=0.689  time=126.19s\n",
      "Epoch 10/15  loss=0.0819  val_loss=0.1006  f1=0.684  time=125.95s\n",
      "Epoch 11/15  loss=0.0798  val_loss=0.1015  f1=0.686  time=126.75s\n",
      "Epoch 12/15  loss=0.0775  val_loss=0.1066  f1=0.681  time=126.20s\n",
      "Epoch 13/15  loss=0.0758  val_loss=0.1070  f1=0.682  time=126.54s\n",
      "Epoch 14/15  loss=0.0740  val_loss=0.1050  f1=0.682  time=126.44s\n",
      "Epoch 15/15  loss=0.0722  val_loss=0.1128  f1=0.680  time=126.50s\n",
      "{'threshold': 0.21896959841251373, 'f1': 0.678691697309851}\n",
      "f1 score: 0.6950278611230176\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1585  val_loss=0.1085  f1=0.647  time=123.42s\n",
      "model_saved at f1: 0.6469420429074608 from 0.0\n",
      "Epoch 2/15  loss=0.1081  val_loss=0.1040  f1=0.666  time=125.71s\n",
      "model_saved at f1: 0.6656043850983141 from 0.6469420429074608\n",
      "Epoch 3/15  loss=0.1022  val_loss=0.1040  f1=0.677  time=124.67s\n",
      "model_saved at f1: 0.6772950016334531 from 0.6656043850983141\n",
      "Epoch 4/15  loss=0.0978  val_loss=0.1035  f1=0.681  time=125.42s\n",
      "model_saved at f1: 0.6814071636259067 from 0.6772950016334531\n",
      "Epoch 5/15  loss=0.0946  val_loss=0.1041  f1=0.684  time=125.88s\n",
      "model_saved at f1: 0.6842207288705092 from 0.6814071636259067\n",
      "Epoch 6/15  loss=0.0913  val_loss=0.1009  f1=0.685  time=125.80s\n",
      "model_saved at f1: 0.6847258485639687 from 0.6842207288705092\n",
      "Epoch 7/15  loss=0.0885  val_loss=0.1003  f1=0.683  time=124.93s\n",
      "Epoch 8/15  loss=0.0858  val_loss=0.1074  f1=0.685  time=125.72s\n",
      "model_saved at f1: 0.6849191231072225 from 0.6847258485639687\n",
      "Epoch 9/15  loss=0.0832  val_loss=0.1103  f1=0.685  time=126.26s\n",
      "Epoch 10/15  loss=0.0808  val_loss=0.1033  f1=0.686  time=125.21s\n",
      "model_saved at f1: 0.6859712695742203 from 0.6849191231072225\n",
      "Epoch 11/15  loss=0.0785  val_loss=0.1078  f1=0.684  time=125.26s\n",
      "Epoch 12/15  loss=0.0763  val_loss=0.1072  f1=0.684  time=125.07s\n",
      "Epoch 13/15  loss=0.0744  val_loss=0.1086  f1=0.680  time=125.08s\n",
      "Epoch 14/15  loss=0.0726  val_loss=0.1114  f1=0.680  time=125.82s\n",
      "Epoch 15/15  loss=0.0706  val_loss=0.1123  f1=0.679  time=125.66s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1519  val_loss=0.1132  f1=0.641  time=124.15s\n",
      "model_saved at f1: 0.6408610516760682 from 0.0\n",
      "Epoch 2/15  loss=0.1078  val_loss=0.1041  f1=0.668  time=126.21s\n",
      "model_saved at f1: 0.6678101032329051 from 0.6408610516760682\n",
      "Epoch 3/15  loss=0.1024  val_loss=0.1011  f1=0.674  time=126.10s\n",
      "model_saved at f1: 0.6742003572008443 from 0.6678101032329051\n",
      "Epoch 4/15  loss=0.0983  val_loss=0.0996  f1=0.680  time=126.36s\n",
      "model_saved at f1: 0.6803353899699414 from 0.6742003572008443\n",
      "Epoch 5/15  loss=0.0948  val_loss=0.1007  f1=0.684  time=126.31s\n",
      "model_saved at f1: 0.684486066400026 from 0.6803353899699414\n",
      "Epoch 6/15  loss=0.0918  val_loss=0.1018  f1=0.688  time=126.01s\n",
      "model_saved at f1: 0.6881014002256287 from 0.684486066400026\n",
      "Epoch 7/15  loss=0.0890  val_loss=0.0993  f1=0.686  time=125.84s\n",
      "Epoch 8/15  loss=0.0862  val_loss=0.0983  f1=0.689  time=126.35s\n",
      "model_saved at f1: 0.6885013378034235 from 0.6881014002256287\n",
      "Epoch 9/15  loss=0.0836  val_loss=0.1024  f1=0.687  time=126.40s\n",
      "Epoch 10/15  loss=0.0812  val_loss=0.1017  f1=0.687  time=126.42s\n",
      "Epoch 11/15  loss=0.0790  val_loss=0.1060  f1=0.688  time=126.46s\n",
      "Epoch 12/15  loss=0.0771  val_loss=0.1059  f1=0.686  time=126.03s\n",
      "Epoch 13/15  loss=0.0748  val_loss=0.1098  f1=0.684  time=125.91s\n",
      "Epoch 14/15  loss=0.0730  val_loss=0.1088  f1=0.683  time=126.55s\n",
      "Epoch 15/15  loss=0.0711  val_loss=0.1085  f1=0.682  time=126.08s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1576  val_loss=0.1096  f1=0.648  time=123.63s\n",
      "model_saved at f1: 0.6479664279178367 from 0.0\n",
      "Epoch 2/15  loss=0.1081  val_loss=0.1050  f1=0.668  time=126.05s\n",
      "model_saved at f1: 0.6676460475729065 from 0.6479664279178367\n",
      "Epoch 3/15  loss=0.1022  val_loss=0.1013  f1=0.677  time=125.81s\n",
      "model_saved at f1: 0.6773017902813299 from 0.6676460475729065\n",
      "Epoch 4/15  loss=0.0981  val_loss=0.0978  f1=0.686  time=124.86s\n",
      "model_saved at f1: 0.6857327516561642 from 0.6773017902813299\n",
      "Epoch 5/15  loss=0.0949  val_loss=0.0988  f1=0.684  time=125.74s\n",
      "Epoch 6/15  loss=0.0920  val_loss=0.1001  f1=0.687  time=125.91s\n",
      "model_saved at f1: 0.6868813102246193 from 0.6857327516561642\n",
      "Epoch 7/15  loss=0.0890  val_loss=0.0995  f1=0.687  time=125.17s\n",
      "model_saved at f1: 0.6871331261575164 from 0.6868813102246193\n",
      "Epoch 8/15  loss=0.0866  val_loss=0.0997  f1=0.688  time=125.76s\n",
      "model_saved at f1: 0.6878696280720199 from 0.6871331261575164\n",
      "Epoch 9/15  loss=0.0844  val_loss=0.0996  f1=0.690  time=125.66s\n",
      "model_saved at f1: 0.6896308638810561 from 0.6878696280720199\n",
      "Epoch 10/15  loss=0.0820  val_loss=0.1015  f1=0.687  time=125.16s\n",
      "Epoch 11/15  loss=0.0801  val_loss=0.1025  f1=0.685  time=126.01s\n",
      "Epoch 12/15  loss=0.0777  val_loss=0.1024  f1=0.685  time=125.96s\n",
      "Epoch 13/15  loss=0.0757  val_loss=0.1051  f1=0.681  time=125.02s\n",
      "Epoch 14/15  loss=0.0738  val_loss=0.1078  f1=0.680  time=125.52s\n",
      "Epoch 15/15  loss=0.0723  val_loss=0.1109  f1=0.679  time=125.70s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1422  val_loss=0.1097  f1=0.646  time=124.36s\n",
      "model_saved at f1: 0.645766633911307 from 0.0\n",
      "Epoch 2/15  loss=0.1069  val_loss=0.1030  f1=0.667  time=126.53s\n",
      "model_saved at f1: 0.6667519508762954 from 0.645766633911307\n",
      "Epoch 3/15  loss=0.1014  val_loss=0.1010  f1=0.675  time=126.35s\n",
      "model_saved at f1: 0.6745998109086166 from 0.6667519508762954\n",
      "Epoch 4/15  loss=0.0977  val_loss=0.0990  f1=0.676  time=126.59s\n",
      "model_saved at f1: 0.6756730800012805 from 0.6745998109086166\n",
      "Epoch 5/15  loss=0.0945  val_loss=0.0998  f1=0.683  time=126.65s\n",
      "model_saved at f1: 0.6827965742941808 from 0.6756730800012805\n",
      "Epoch 6/15  loss=0.0918  val_loss=0.0992  f1=0.682  time=126.25s\n",
      "Epoch 7/15  loss=0.0887  val_loss=0.1002  f1=0.683  time=126.08s\n",
      "model_saved at f1: 0.6829252802387983 from 0.6827965742941808\n",
      "Epoch 8/15  loss=0.0863  val_loss=0.1064  f1=0.682  time=126.37s\n",
      "Epoch 9/15  loss=0.0837  val_loss=0.1062  f1=0.683  time=126.29s\n",
      "Epoch 10/15  loss=0.0814  val_loss=0.1044  f1=0.682  time=126.52s\n",
      "Epoch 11/15  loss=0.0796  val_loss=0.1052  f1=0.682  time=126.26s\n",
      "Epoch 12/15  loss=0.0773  val_loss=0.1038  f1=0.682  time=126.31s\n",
      "Epoch 13/15  loss=0.0757  val_loss=0.1078  f1=0.680  time=126.27s\n",
      "Epoch 14/15  loss=0.0736  val_loss=0.1120  f1=0.679  time=126.93s\n",
      "Epoch 15/15  loss=0.0721  val_loss=0.1129  f1=0.678  time=126.78s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1470  val_loss=0.1078  f1=0.650  time=124.09s\n",
      "model_saved at f1: 0.650181018877683 from 0.0\n",
      "Epoch 2/15  loss=0.1071  val_loss=0.1031  f1=0.672  time=125.75s\n",
      "model_saved at f1: 0.6719756337447254 from 0.650181018877683\n",
      "Epoch 3/15  loss=0.1015  val_loss=0.1038  f1=0.678  time=125.98s\n",
      "model_saved at f1: 0.6776438744602694 from 0.6719756337447254\n",
      "Epoch 4/15  loss=0.0977  val_loss=0.1043  f1=0.685  time=125.18s\n",
      "model_saved at f1: 0.6848514056224899 from 0.6776438744602694\n",
      "Epoch 5/15  loss=0.0943  val_loss=0.0978  f1=0.689  time=125.20s\n",
      "model_saved at f1: 0.6892333311983604 from 0.6848514056224899\n",
      "Epoch 6/15  loss=0.0913  val_loss=0.1045  f1=0.690  time=125.82s\n",
      "model_saved at f1: 0.6895674300254454 from 0.6892333311983604\n",
      "Epoch 7/15  loss=0.0886  val_loss=0.1019  f1=0.688  time=124.88s\n",
      "Epoch 8/15  loss=0.0860  val_loss=0.1020  f1=0.688  time=125.58s\n",
      "Epoch 9/15  loss=0.0835  val_loss=0.1056  f1=0.690  time=125.97s\n",
      "model_saved at f1: 0.6899815767740296 from 0.6895674300254454\n",
      "Epoch 10/15  loss=0.0811  val_loss=0.1060  f1=0.688  time=124.93s\n",
      "Epoch 11/15  loss=0.0790  val_loss=0.1056  f1=0.684  time=125.33s\n",
      "Epoch 12/15  loss=0.0770  val_loss=0.1079  f1=0.682  time=125.26s\n",
      "Epoch 13/15  loss=0.0749  val_loss=0.1117  f1=0.681  time=125.02s\n",
      "Epoch 14/15  loss=0.0732  val_loss=0.1157  f1=0.682  time=125.78s\n",
      "Epoch 15/15  loss=0.0716  val_loss=0.1138  f1=0.677  time=125.60s\n",
      "{'threshold': 0.28102758526802063, 'f1': 0.6779458856779847}\n",
      "f1 score: 0.6978537315150469\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1589  val_loss=0.1076  f1=0.651  time=123.83s\n",
      "model_saved at f1: 0.6507530364372469 from 0.0\n",
      "Epoch 2/15  loss=0.1083  val_loss=0.1034  f1=0.667  time=125.64s\n",
      "model_saved at f1: 0.6672749193522108 from 0.6507530364372469\n",
      "Epoch 3/15  loss=0.1025  val_loss=0.1020  f1=0.675  time=125.71s\n",
      "model_saved at f1: 0.67471712465532 from 0.6672749193522108\n",
      "Epoch 4/15  loss=0.0985  val_loss=0.0997  f1=0.682  time=125.60s\n",
      "model_saved at f1: 0.6824446345404428 from 0.67471712465532\n",
      "Epoch 5/15  loss=0.0950  val_loss=0.0984  f1=0.685  time=125.72s\n",
      "model_saved at f1: 0.6848344978027884 from 0.6824446345404428\n",
      "Epoch 6/15  loss=0.0921  val_loss=0.0997  f1=0.687  time=126.04s\n",
      "model_saved at f1: 0.6866495760205088 from 0.6848344978027884\n",
      "Epoch 7/15  loss=0.0893  val_loss=0.0972  f1=0.688  time=125.55s\n",
      "model_saved at f1: 0.6875427677669523 from 0.6866495760205088\n",
      "Epoch 8/15  loss=0.0867  val_loss=0.0984  f1=0.688  time=125.90s\n",
      "model_saved at f1: 0.6877034011869058 from 0.6875427677669523\n",
      "Epoch 9/15  loss=0.0842  val_loss=0.0988  f1=0.685  time=125.88s\n",
      "Epoch 10/15  loss=0.0821  val_loss=0.0999  f1=0.686  time=125.19s\n",
      "Epoch 11/15  loss=0.0797  val_loss=0.1019  f1=0.684  time=125.77s\n",
      "Epoch 12/15  loss=0.0775  val_loss=0.1051  f1=0.686  time=126.18s\n",
      "Epoch 13/15  loss=0.0752  val_loss=0.1051  f1=0.685  time=125.54s\n",
      "Epoch 14/15  loss=0.0738  val_loss=0.1053  f1=0.683  time=126.18s\n",
      "Epoch 15/15  loss=0.0719  val_loss=0.1085  f1=0.680  time=126.05s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1658  val_loss=0.1094  f1=0.646  time=124.74s\n",
      "model_saved at f1: 0.6456274980015987 from 0.0\n",
      "Epoch 2/15  loss=0.1094  val_loss=0.1037  f1=0.666  time=125.62s\n",
      "model_saved at f1: 0.6656154528432049 from 0.6456274980015987\n",
      "Epoch 3/15  loss=0.1033  val_loss=0.1028  f1=0.672  time=125.68s\n",
      "model_saved at f1: 0.671557418699187 from 0.6656154528432049\n",
      "Epoch 4/15  loss=0.0989  val_loss=0.1030  f1=0.679  time=125.05s\n",
      "model_saved at f1: 0.679208813865086 from 0.671557418699187\n",
      "Epoch 5/15  loss=0.0956  val_loss=0.1003  f1=0.683  time=125.70s\n",
      "model_saved at f1: 0.6833773927445297 from 0.679208813865086\n",
      "Epoch 6/15  loss=0.0925  val_loss=0.0987  f1=0.686  time=125.68s\n",
      "model_saved at f1: 0.6855015748783049 from 0.6833773927445297\n",
      "Epoch 7/15  loss=0.0897  val_loss=0.1018  f1=0.685  time=125.40s\n",
      "Epoch 8/15  loss=0.0870  val_loss=0.1000  f1=0.685  time=125.60s\n",
      "Epoch 9/15  loss=0.0843  val_loss=0.0989  f1=0.686  time=125.88s\n",
      "model_saved at f1: 0.6859053205916674 from 0.6855015748783049\n",
      "Epoch 10/15  loss=0.0820  val_loss=0.1049  f1=0.685  time=125.40s\n",
      "Epoch 11/15  loss=0.0800  val_loss=0.1021  f1=0.684  time=125.76s\n",
      "Epoch 12/15  loss=0.0776  val_loss=0.1023  f1=0.685  time=125.68s\n",
      "Epoch 13/15  loss=0.0757  val_loss=0.1068  f1=0.682  time=125.23s\n",
      "Epoch 14/15  loss=0.0739  val_loss=0.1141  f1=0.681  time=126.00s\n",
      "Epoch 15/15  loss=0.0721  val_loss=0.1095  f1=0.681  time=125.81s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1521  val_loss=0.1087  f1=0.650  time=124.77s\n",
      "model_saved at f1: 0.6503703703703703 from 0.0\n",
      "Epoch 2/15  loss=0.1074  val_loss=0.1033  f1=0.671  time=126.05s\n",
      "model_saved at f1: 0.6705023235229033 from 0.6503703703703703\n",
      "Epoch 3/15  loss=0.1020  val_loss=0.1042  f1=0.676  time=126.10s\n",
      "model_saved at f1: 0.67648 from 0.6705023235229033\n",
      "Epoch 4/15  loss=0.0981  val_loss=0.1014  f1=0.680  time=125.57s\n",
      "model_saved at f1: 0.6798810062730388 from 0.67648\n",
      "Epoch 5/15  loss=0.0948  val_loss=0.0982  f1=0.687  time=126.09s\n",
      "model_saved at f1: 0.6872297864102729 from 0.6798810062730388\n",
      "Epoch 6/15  loss=0.0916  val_loss=0.1049  f1=0.685  time=126.17s\n",
      "Epoch 7/15  loss=0.0891  val_loss=0.0999  f1=0.687  time=125.55s\n",
      "Epoch 8/15  loss=0.0863  val_loss=0.1025  f1=0.686  time=125.99s\n",
      "Epoch 9/15  loss=0.0840  val_loss=0.1028  f1=0.688  time=126.16s\n",
      "model_saved at f1: 0.68837090653997 from 0.6872297864102729\n",
      "Epoch 10/15  loss=0.0816  val_loss=0.1016  f1=0.688  time=125.61s\n",
      "Epoch 11/15  loss=0.0792  val_loss=0.1034  f1=0.684  time=126.23s\n",
      "Epoch 12/15  loss=0.0775  val_loss=0.1053  f1=0.685  time=125.95s\n",
      "Epoch 13/15  loss=0.0753  val_loss=0.1119  f1=0.684  time=125.40s\n",
      "Epoch 14/15  loss=0.0735  val_loss=0.1119  f1=0.684  time=126.19s\n",
      "Epoch 15/15  loss=0.0720  val_loss=0.1098  f1=0.682  time=126.69s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1449  val_loss=0.1080  f1=0.653  time=125.36s\n",
      "model_saved at f1: 0.6526715143850774 from 0.0\n",
      "Epoch 2/15  loss=0.1076  val_loss=0.1038  f1=0.673  time=67.82s\n",
      "model_saved at f1: 0.6729904486821806 from 0.6526715143850774\n",
      "Epoch 3/15  loss=0.1019  val_loss=0.1001  f1=0.682  time=67.48s\n",
      "model_saved at f1: 0.6816990715279351 from 0.6729904486821806\n",
      "Epoch 4/15  loss=0.0977  val_loss=0.0984  f1=0.687  time=67.41s\n",
      "model_saved at f1: 0.687400784008812 from 0.6816990715279351\n",
      "Epoch 5/15  loss=0.0944  val_loss=0.0996  f1=0.690  time=67.93s\n",
      "model_saved at f1: 0.689755529685681 from 0.687400784008812\n",
      "Epoch 6/15  loss=0.0912  val_loss=0.0981  f1=0.692  time=67.56s\n",
      "model_saved at f1: 0.6924906336536896 from 0.689755529685681\n",
      "Epoch 7/15  loss=0.0886  val_loss=0.1019  f1=0.690  time=67.44s\n",
      "Epoch 8/15  loss=0.0859  val_loss=0.1006  f1=0.692  time=68.02s\n",
      "Epoch 9/15  loss=0.0834  val_loss=0.1009  f1=0.692  time=67.63s\n",
      "Epoch 10/15  loss=0.0808  val_loss=0.1045  f1=0.688  time=67.58s\n",
      "Epoch 11/15  loss=0.0786  val_loss=0.1033  f1=0.689  time=68.17s\n",
      "Epoch 12/15  loss=0.0768  val_loss=0.1082  f1=0.685  time=67.92s\n",
      "Epoch 13/15  loss=0.0748  val_loss=0.1098  f1=0.684  time=67.82s\n",
      "Epoch 14/15  loss=0.0729  val_loss=0.1109  f1=0.681  time=68.21s\n",
      "Epoch 15/15  loss=0.0715  val_loss=0.1120  f1=0.683  time=67.98s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1666  val_loss=0.1091  f1=0.642  time=67.99s\n",
      "model_saved at f1: 0.6424173891192361 from 0.0\n",
      "Epoch 2/15  loss=0.1089  val_loss=0.1048  f1=0.660  time=68.67s\n",
      "model_saved at f1: 0.660235458382255 from 0.6424173891192361\n",
      "Epoch 3/15  loss=0.1029  val_loss=0.1025  f1=0.669  time=67.91s\n",
      "model_saved at f1: 0.6692179804887229 from 0.660235458382255\n",
      "Epoch 4/15  loss=0.0988  val_loss=0.1015  f1=0.672  time=68.00s\n",
      "model_saved at f1: 0.6724374143728983 from 0.6692179804887229\n",
      "Epoch 5/15  loss=0.0951  val_loss=0.0994  f1=0.679  time=68.64s\n",
      "model_saved at f1: 0.6792440784013279 from 0.6724374143728983\n",
      "Epoch 6/15  loss=0.0920  val_loss=0.0986  f1=0.682  time=68.07s\n",
      "model_saved at f1: 0.6815821769092797 from 0.6792440784013279\n",
      "Epoch 7/15  loss=0.0894  val_loss=0.1020  f1=0.682  time=67.98s\n",
      "model_saved at f1: 0.6824304866732794 from 0.6815821769092797\n",
      "Epoch 8/15  loss=0.0864  val_loss=0.1023  f1=0.683  time=68.69s\n",
      "model_saved at f1: 0.6828679540709812 from 0.6824304866732794\n",
      "Epoch 9/15  loss=0.0841  val_loss=0.1007  f1=0.686  time=68.20s\n",
      "model_saved at f1: 0.6859087552573281 from 0.6828679540709812\n",
      "Epoch 10/15  loss=0.0817  val_loss=0.1053  f1=0.684  time=68.04s\n",
      "Epoch 11/15  loss=0.0792  val_loss=0.1017  f1=0.684  time=68.77s\n",
      "Epoch 12/15  loss=0.0770  val_loss=0.1056  f1=0.684  time=68.25s\n",
      "Epoch 13/15  loss=0.0751  val_loss=0.1059  f1=0.683  time=68.01s\n",
      "Epoch 14/15  loss=0.0734  val_loss=0.1084  f1=0.680  time=68.73s\n",
      "Epoch 15/15  loss=0.0715  val_loss=0.1127  f1=0.679  time=68.37s\n",
      "{'threshold': 0.2623917758464813, 'f1': 0.6809311044346865}\n",
      "f1 score: 0.7000675523530736\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1530  val_loss=0.1082  f1=0.645  time=67.82s\n",
      "model_saved at f1: 0.6450403877221325 from 0.0\n",
      "Epoch 2/15  loss=0.1079  val_loss=0.1059  f1=0.673  time=68.20s\n",
      "model_saved at f1: 0.6725951424746606 from 0.6450403877221325\n",
      "Epoch 3/15  loss=0.1020  val_loss=0.0994  f1=0.682  time=67.98s\n",
      "model_saved at f1: 0.6818716798676718 from 0.6725951424746606\n",
      "Epoch 4/15  loss=0.0983  val_loss=0.0982  f1=0.688  time=67.84s\n",
      "model_saved at f1: 0.6882559752282038 from 0.6818716798676718\n",
      "Epoch 5/15  loss=0.0948  val_loss=0.0991  f1=0.690  time=68.25s\n",
      "model_saved at f1: 0.6899415963659962 from 0.6882559752282038\n",
      "Epoch 6/15  loss=0.0918  val_loss=0.0981  f1=0.693  time=67.98s\n",
      "model_saved at f1: 0.6933039047031712 from 0.6899415963659962\n",
      "Epoch 7/15  loss=0.0892  val_loss=0.0976  f1=0.693  time=67.87s\n",
      "Epoch 8/15  loss=0.0866  val_loss=0.1022  f1=0.693  time=68.28s\n",
      "Epoch 9/15  loss=0.0840  val_loss=0.1011  f1=0.690  time=67.85s\n",
      "Epoch 10/15  loss=0.0819  val_loss=0.1026  f1=0.689  time=67.75s\n",
      "Epoch 11/15  loss=0.0796  val_loss=0.1031  f1=0.689  time=68.40s\n",
      "Epoch 12/15  loss=0.0774  val_loss=0.1042  f1=0.689  time=68.02s\n",
      "Epoch 13/15  loss=0.0756  val_loss=0.1108  f1=0.688  time=67.76s\n",
      "Epoch 14/15  loss=0.0734  val_loss=0.1076  f1=0.687  time=68.29s\n",
      "Epoch 15/15  loss=0.0721  val_loss=0.1103  f1=0.682  time=68.02s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1505  val_loss=0.1172  f1=0.634  time=68.03s\n",
      "model_saved at f1: 0.6338367257605603 from 0.0\n",
      "Epoch 2/15  loss=0.1078  val_loss=0.1089  f1=0.661  time=68.73s\n",
      "model_saved at f1: 0.661350042722871 from 0.6338367257605603\n",
      "Epoch 3/15  loss=0.1022  val_loss=0.1028  f1=0.670  time=68.42s\n",
      "model_saved at f1: 0.6701692936368944 from 0.661350042722871\n",
      "Epoch 4/15  loss=0.0980  val_loss=0.1122  f1=0.674  time=68.14s\n",
      "model_saved at f1: 0.6743793058624724 from 0.6701692936368944\n",
      "Epoch 5/15  loss=0.0944  val_loss=0.1016  f1=0.680  time=68.82s\n",
      "model_saved at f1: 0.679536925131989 from 0.6743793058624724\n",
      "Epoch 6/15  loss=0.0916  val_loss=0.1030  f1=0.682  time=68.36s\n",
      "model_saved at f1: 0.6818495061201719 from 0.679536925131989\n",
      "Epoch 7/15  loss=0.0888  val_loss=0.1095  f1=0.681  time=68.20s\n",
      "Epoch 8/15  loss=0.0863  val_loss=0.1079  f1=0.684  time=68.84s\n",
      "model_saved at f1: 0.6842613453492115 from 0.6818495061201719\n",
      "Epoch 9/15  loss=0.0836  val_loss=0.1043  f1=0.681  time=68.29s\n",
      "Epoch 10/15  loss=0.0816  val_loss=0.1066  f1=0.681  time=68.23s\n",
      "Epoch 11/15  loss=0.0792  val_loss=0.1087  f1=0.681  time=68.74s\n",
      "Epoch 12/15  loss=0.0771  val_loss=0.1124  f1=0.679  time=68.33s\n",
      "Epoch 13/15  loss=0.0754  val_loss=0.1089  f1=0.678  time=68.25s\n",
      "Epoch 14/15  loss=0.0736  val_loss=0.1204  f1=0.675  time=68.75s\n",
      "Epoch 15/15  loss=0.0720  val_loss=0.1183  f1=0.677  time=68.27s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1700  val_loss=0.1113  f1=0.646  time=67.92s\n",
      "model_saved at f1: 0.6461782429432287 from 0.0\n",
      "Epoch 2/15  loss=0.1089  val_loss=0.1028  f1=0.669  time=68.55s\n",
      "model_saved at f1: 0.6688556253017867 from 0.6461782429432287\n",
      "Epoch 3/15  loss=0.1028  val_loss=0.1005  f1=0.678  time=68.17s\n",
      "model_saved at f1: 0.6779407945988056 from 0.6688556253017867\n",
      "Epoch 4/15  loss=0.0988  val_loss=0.0996  f1=0.684  time=67.93s\n",
      "model_saved at f1: 0.6844161386691179 from 0.6779407945988056\n",
      "Epoch 5/15  loss=0.0954  val_loss=0.1039  f1=0.685  time=68.48s\n",
      "model_saved at f1: 0.6852118305355714 from 0.6844161386691179\n",
      "Epoch 6/15  loss=0.0923  val_loss=0.0994  f1=0.689  time=68.35s\n",
      "model_saved at f1: 0.6885589852814954 from 0.6852118305355714\n",
      "Epoch 7/15  loss=0.0895  val_loss=0.0987  f1=0.692  time=68.15s\n",
      "model_saved at f1: 0.6921100218901685 from 0.6885589852814954\n",
      "Epoch 8/15  loss=0.0871  val_loss=0.1004  f1=0.689  time=68.50s\n",
      "Epoch 9/15  loss=0.0844  val_loss=0.1035  f1=0.689  time=68.34s\n",
      "Epoch 10/15  loss=0.0819  val_loss=0.1018  f1=0.688  time=68.03s\n",
      "Epoch 11/15  loss=0.0793  val_loss=0.1061  f1=0.685  time=68.59s\n",
      "Epoch 12/15  loss=0.0775  val_loss=0.1123  f1=0.684  time=68.29s\n",
      "Epoch 13/15  loss=0.0756  val_loss=0.1093  f1=0.684  time=68.18s\n",
      "Epoch 14/15  loss=0.0735  val_loss=0.1112  f1=0.684  time=68.63s\n",
      "Epoch 15/15  loss=0.0719  val_loss=0.1103  f1=0.684  time=68.16s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1650  val_loss=0.1092  f1=0.645  time=68.41s\n",
      "model_saved at f1: 0.644761436301033 from 0.0\n",
      "Epoch 2/15  loss=0.1092  val_loss=0.1048  f1=0.662  time=68.91s\n",
      "model_saved at f1: 0.6617934995431406 from 0.644761436301033\n",
      "Epoch 3/15  loss=0.1031  val_loss=0.1020  f1=0.675  time=68.45s\n",
      "model_saved at f1: 0.6745752877083265 from 0.6617934995431406\n",
      "Epoch 4/15  loss=0.0989  val_loss=0.1030  f1=0.684  time=68.31s\n",
      "model_saved at f1: 0.6836432936846947 from 0.6745752877083265\n",
      "Epoch 5/15  loss=0.0953  val_loss=0.0997  f1=0.687  time=68.79s\n",
      "model_saved at f1: 0.6865242792835936 from 0.6836432936846947\n",
      "Epoch 6/15  loss=0.0922  val_loss=0.1069  f1=0.686  time=68.35s\n",
      "Epoch 7/15  loss=0.0895  val_loss=0.1026  f1=0.687  time=68.19s\n",
      "model_saved at f1: 0.687368488171906 from 0.6865242792835936\n",
      "Epoch 8/15  loss=0.0870  val_loss=0.0995  f1=0.687  time=68.85s\n",
      "Epoch 9/15  loss=0.0840  val_loss=0.1020  f1=0.687  time=68.41s\n",
      "model_saved at f1: 0.6874593045969528 from 0.687368488171906\n",
      "Epoch 10/15  loss=0.0816  val_loss=0.1016  f1=0.688  time=68.24s\n",
      "model_saved at f1: 0.6883542742118762 from 0.6874593045969528\n",
      "Epoch 11/15  loss=0.0795  val_loss=0.1078  f1=0.686  time=68.64s\n",
      "Epoch 12/15  loss=0.0774  val_loss=0.1116  f1=0.682  time=68.12s\n",
      "Epoch 13/15  loss=0.0754  val_loss=0.1100  f1=0.681  time=68.08s\n",
      "Epoch 14/15  loss=0.0736  val_loss=0.1084  f1=0.679  time=68.64s\n",
      "Epoch 15/15  loss=0.0716  val_loss=0.1146  f1=0.682  time=68.22s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1494  val_loss=0.1086  f1=0.645  time=67.79s\n",
      "model_saved at f1: 0.6452818239392021 from 0.0\n",
      "Epoch 2/15  loss=0.1076  val_loss=0.1024  f1=0.668  time=68.31s\n",
      "model_saved at f1: 0.6681352360884972 from 0.6452818239392021\n",
      "Epoch 3/15  loss=0.1016  val_loss=0.1005  f1=0.671  time=67.87s\n",
      "model_saved at f1: 0.6713415408046707 from 0.6681352360884972\n",
      "Epoch 4/15  loss=0.0979  val_loss=0.1024  f1=0.676  time=67.81s\n",
      "model_saved at f1: 0.6760108513112001 from 0.6713415408046707\n",
      "Epoch 5/15  loss=0.0947  val_loss=0.0996  f1=0.680  time=68.34s\n",
      "model_saved at f1: 0.6804001968181073 from 0.6760108513112001\n",
      "Epoch 6/15  loss=0.0915  val_loss=0.1007  f1=0.682  time=68.01s\n",
      "model_saved at f1: 0.6821870427032062 from 0.6804001968181073\n",
      "Epoch 7/15  loss=0.0888  val_loss=0.1011  f1=0.683  time=67.89s\n",
      "model_saved at f1: 0.6826831889738888 from 0.6821870427032062\n",
      "Epoch 8/15  loss=0.0861  val_loss=0.1016  f1=0.684  time=68.48s\n",
      "model_saved at f1: 0.6840797646289637 from 0.6826831889738888\n",
      "Epoch 9/15  loss=0.0837  val_loss=0.1093  f1=0.683  time=68.13s\n",
      "Epoch 10/15  loss=0.0813  val_loss=0.1058  f1=0.683  time=68.03s\n",
      "Epoch 11/15  loss=0.0794  val_loss=0.1033  f1=0.681  time=68.47s\n",
      "Epoch 12/15  loss=0.0774  val_loss=0.1113  f1=0.677  time=68.29s\n",
      "Epoch 13/15  loss=0.0753  val_loss=0.1119  f1=0.679  time=68.09s\n",
      "Epoch 14/15  loss=0.0736  val_loss=0.1090  f1=0.679  time=68.37s\n",
      "Epoch 15/15  loss=0.0717  val_loss=0.1111  f1=0.678  time=68.08s\n",
      "{'threshold': 0.23290202021598816, 'f1': 0.6796190524597038}\n",
      "f1 score: 0.6980427145485976\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
