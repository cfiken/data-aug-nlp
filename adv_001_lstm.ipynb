{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# use only for tokenizer and padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1019):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# SEED = 1019\n",
    "# seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "class Config:\n",
    "    num_epochs = 15\n",
    "    batch_size = 512\n",
    "    test_batch_size = 512\n",
    "    vocab_size = 120000\n",
    "    max_length = 72\n",
    "    embedding_size = 300\n",
    "    hidden_size = 64\n",
    "    num_layers = 1\n",
    "    embedding_noise_var = 0.1\n",
    "    embedding_dropout = 0.3\n",
    "    layer_dropout = 0.1\n",
    "    dense_size = [hidden_size*2*4, int(hidden_size/4)] # depend on concat num\n",
    "    output_size = 1\n",
    "    num_cv_splits = 5\n",
    "    learning_rate = 0.001\n",
    "    clip_grad = 5.0\n",
    "    embeddings = ['glove', 'paragram', 'fasttext']\n",
    "    datadir = Path('./data/')\n",
    "    # datadir = Path('../input') # for kernel\n",
    "\n",
    "c = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [\n",
    "    ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '¬', '░', '¶', '↑', '±',  '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '₹', '´'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"'re\": \" are\",\n",
    "    \"tryin'\": \"trying\",\n",
    "    \"doesn'\": \"does not\",\n",
    "    'howdo': 'how do',\n",
    "    'whatare': 'what are',\n",
    "    'howcan': 'how can',\n",
    "    'howmuch': 'how much',\n",
    "    'howmany': 'how many',\n",
    "    'whydo': 'why do',\n",
    "    'doI': 'do I',\n",
    "    'theBest': 'the best',\n",
    "    'howdoes': 'how does',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spells = {\n",
    "    'colour': 'color',\n",
    "    'centre': 'center',\n",
    "    'favourite': 'favorite',\n",
    "    'travelling': 'traveling',\n",
    "    'counselling': 'counseling',\n",
    "    'theatre': 'theater',\n",
    "    'cancelled': 'canceled',\n",
    "    'labour': 'labor',\n",
    "    'organisation': 'organization',\n",
    "    'wwii': 'world war 2',\n",
    "    'citicise': 'criticize',\n",
    "    'youtu.be': 'youtube',\n",
    "    'youtu ': 'youtube ',\n",
    "    'qoura': 'quora',\n",
    "    'sallary': 'salary',\n",
    "    'Whta': 'what',\n",
    "    'whta': 'what',\n",
    "    'narcisist': 'narcissist',\n",
    "    'mastrubation': 'masturbation',\n",
    "    'mastrubate': 'masturbate',\n",
    "    \"mastrubating\": 'masturbating',\n",
    "    'pennis': 'penis',\n",
    "    'Etherium': 'ethereum',\n",
    "    'etherium': 'ethereum',\n",
    "    'narcissit': 'narcissist',\n",
    "    'bigdata': 'big data',\n",
    "    '2k17': '2017',\n",
    "    '2k18': '2018',\n",
    "    'qouta': 'quota',\n",
    "    'exboyfriend': 'ex boyfriend',\n",
    "    'exgirlfriend': 'ex girlfriend',\n",
    "    'airhostess': 'air hostess',\n",
    "    \"whst\": 'what',\n",
    "    'watsapp': 'whatsapp',\n",
    "    'demonitisation': 'demonetization',\n",
    "    'demonitization': 'demonetization',\n",
    "    'demonetisation': 'demonetization',\n",
    "    'quorans': 'quora user',\n",
    "    'quoran': 'quora user',\n",
    "    'pokémon': 'pokemon',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datadir):\n",
    "    train_df = pd.read_csv(datadir / 'train_local.csv')\n",
    "    test_df = pd.read_csv(datadir / 'test_local.csv')\n",
    "    print(\"Train shape : \", train_df.shape)\n",
    "    print(\"Test shape : \", test_df.shape)\n",
    "    return train_df, test_df\n",
    "\n",
    "def clean(df):\n",
    "    df = clean_lower(df)\n",
    "    df = clean_unicode(df)\n",
    "    df = clean_abbreviation(df, abbreviations)\n",
    "    df = clean_spells(df, spells)\n",
    "    df = clean_language(df)\n",
    "    df = clean_puncts(df, puncts)\n",
    "    df = clean_space(df)\n",
    "    return df\n",
    "\n",
    "def clean_unicode(df):\n",
    "    codes = ['\\x7f', '\\u200b', '\\xa0', '\\ufeff', '\\u200e', '\\u202a', '\\u202c', '\\u2060', '\\uf0d8', '\\ue019', '\\uf02d', '\\u200f', '\\u2061', '\\ue01b']\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: _clean_unicode(x, codes))\n",
    "    return df\n",
    "\n",
    "def _clean_unicode(x, codes):\n",
    "    for u in codes:\n",
    "        if u in x:\n",
    "            x = x.replace(u, '')\n",
    "    return x\n",
    "\n",
    "def clean_language(df):\n",
    "    langs1 = r'[\\p{Katakana}\\p{Hiragana}\\p{Han}]' # regex\n",
    "    langs2 = r'[ஆய்தஎழுத்துஆயுதஎழுத்துशुषछछशुषدوउसशुष북한내제តើបងប្អូនមានមធ្យបាយអ្វីខ្លះដើម្បីរកឃើញឯកសារអំពីប្រវត្តិស្ត្រនៃប្រាសាទអង្គរវट्टरौरआदસંઘરાજ્યपीतऊनअहএকটিবাড়িএকটিখামারএরঅধীনেপদেরবাছাইপরীক্ষাএরপ্রশ্নওউত্তরসহকোথায়পেতেপারিص、。Емелядуракلكلمقاممقال수능ί서로가를행복하게기乡국고등학교는몇시간업니《》싱관없어나이रचा키کپڤ」मिलगईकलेजेकोठंडकऋॠऌॡर]'\n",
    "    compiled_langs1 = regex.compile(langs1)\n",
    "    compiled_langs2 = re.compile(langs2)\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs1))\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_language(x, compiled_langs2))\n",
    "    return df\n",
    "\n",
    "def _clean_language(x, compiled_re):\n",
    "    return compiled_re.sub(' <lang> ', x)\n",
    "\n",
    "def clean_lower(df):\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "def clean_puncts(df, puncts):\n",
    "    df['question_text'] = df['question_text'].apply(lambda x: _clean_puncts(x, puncts))\n",
    "    return df\n",
    "    \n",
    "def _clean_puncts(x, puncts):\n",
    "    x = str(x)\n",
    "    # added space around puncts after replace\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_spells(df, spells):\n",
    "    compiled_spells = re.compile('(%s)' % '|'.join(spells.keys()))\n",
    "    def replace(match):\n",
    "        return spells[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_spells(x, compiled_spells, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_spells(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_abbreviation(df, abbreviations):\n",
    "    compiled_abbreviation = re.compile('(%s)' % '|'.join(abbreviations.keys()))\n",
    "    def replace(match):\n",
    "        return abbreviations[match.group(0)]\n",
    "    df['question_text'] = df[\"question_text\"].apply(\n",
    "        lambda x: _clean_abreviation(x, compiled_abbreviation, replace)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def _clean_abreviation(x, compiled_re, replace):\n",
    "    return compiled_re.sub(replace, x)\n",
    "\n",
    "def clean_space(df):\n",
    "    compiled_re = re.compile(r\"\\s+\")\n",
    "    df['question_text'] = df[\"question_text\"].apply(lambda x: _clean_space(x, compiled_re))\n",
    "    return df\n",
    "\n",
    "def _clean_space(x, compiled_re):\n",
    "    return compiled_re.sub(\" \", x)\n",
    "        \n",
    "def prepare_tokenizer(texts, max_words):\n",
    "    tokenizer = Tokenizer(num_words=max_words, filters='', oov_token='<unk>')\n",
    "    tokenizer.fit_on_texts(list(texts))\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_and_padding(texts, tokenizer, max_length):\n",
    "    texts = tokenizer.texts_to_sequences(texts)\n",
    "    texts = pad_sequences(texts, maxlen=max_length)\n",
    "    return texts\n",
    "\n",
    "def get_all_vocabs(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config, tokenizer, all_vocabs, embedding_weights = None):\n",
    "        super(Embeddings, self).__init__()\n",
    "        \n",
    "        self.embedding_map = {\n",
    "            'fasttext': self._load_fasttext,\n",
    "            'glove': self._load_glove,\n",
    "            'paragram': self._load_paragram\n",
    "        }\n",
    "        self.c = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.all_vocabs = all_vocabs\n",
    "        \n",
    "        if embedding_weights is None:\n",
    "            embedding_weights = self._load_embeddings(self.c.embeddings)\n",
    "            \n",
    "        self.original_embedding_weights = embedding_weights\n",
    "        self.embeddings = nn.Embedding(self.c.vocab_size + 1, self.c.embedding_size, padding_idx=0)\n",
    "        self.embeddings.weight = nn.Parameter(embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.embedding_dropout = nn.Dropout2d(self.c.embedding_dropout)\n",
    "        \n",
    "    def forward(self, x, perturb):\n",
    "        embedding = self.embeddings(x)\n",
    "        embedding.requires_grad = True\n",
    "        if perturb is not None:\n",
    "            return embedding + perturb\n",
    "        return embedding\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        self.embeddings.weight = nn.Parameter(self.original_embedding_weights)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "    \n",
    "    def _load_embeddings(self, embedding_list: list):\n",
    "        embedding_weights = np.zeros((self.c.vocab_size, self.c.embedding_size))\n",
    "        pool = Pool(num_cores)\n",
    "        embedding_weights = np.mean(pool.map(self._load_an_embedding, embedding_list), 0)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return torch.tensor(embedding_weights, dtype=torch.float32)\n",
    "\n",
    "    def _load_an_embedding(self, emb):\n",
    "        return self.embedding_map[emb](self.tokenizer.word_index)\n",
    "        \n",
    "    def _get_embeddings_pair(self, word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "        \n",
    "    def _make_embeddings(self, embeddings_index, word_index, emb_mean, emb_std):\n",
    "        nb_words = min(self.c.vocab_size, len(word_index))\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, self.c.embedding_size))\n",
    "        embedding_matrix[0] = np.zeros(self.c.embedding_size)\n",
    "        for word, i in word_index.items():\n",
    "            if i >= self.c.vocab_size:\n",
    "                continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return embedding_matrix\n",
    "    \n",
    "    def _load_glove(self, word_index):\n",
    "        print('loading glove')\n",
    "        filepath = self.c.datadir / 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "    \n",
    "    def _load_fasttext(self, word_index):    \n",
    "        print('loading fasttext')\n",
    "        filepath = self.c.datadir / 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath)\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0033469985, 0.109855495\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)\n",
    "\n",
    "    def _load_paragram(self, word_index):\n",
    "        print('loading paragram')\n",
    "        filepath = self.c.datadir / 'embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "        embeddings_index = dict(\n",
    "            self._get_embeddings_pair(*o.split(\" \"))\n",
    "            for o in open(filepath, encoding=\"utf8\", errors='ignore')\n",
    "            if len(o) > 100 and o.split(\" \")[0] in word_index\n",
    "        )\n",
    "        emb_mean, emb_std = -0.0053247833, 0.49346462\n",
    "        return self._make_embeddings(embeddings_index, word_index, emb_mean, emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 2\n",
    "def df_parallelize_run(df, func, num_cores=2):\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1175509, 3)\n",
      "Test shape :  (130613, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_data(c.datadir)\n",
    "train_df = df_parallelize_run(train_df, clean)\n",
    "test_df = df_parallelize_run(test_df, clean)\n",
    "train_x, train_y = train_df['question_text'].values, train_df['target'].values\n",
    "test_x = test_df['question_text'].values\n",
    "tokenizer = prepare_tokenizer(train_x, c.vocab_size)\n",
    "train_x = tokenize_and_padding(train_x, tokenizer, c.max_length)\n",
    "test_x = tokenize_and_padding(test_x, tokenizer, c.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_vocabs:  184279\n",
      "loading glove\n",
      "loading paragram\n",
      "loading fasttext\n",
      "50.16184663772583\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_vocabs = get_all_vocabs(train_df['question_text'])\n",
    "print('all_vocabs: ', len(all_vocabs))\n",
    "embeddings = Embeddings(c, tokenizer, all_vocabs)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(GRULayer, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          bias=False,\n",
    "                          bidirectional=True,\n",
    "                          batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_outputs, gru_state = self.gru(x)\n",
    "        return self.dropout(gru_outputs), gru_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bias=False,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_outputs, (lstm_states, _) = self.lstm(x)\n",
    "        return self.dropout(lstm_outputs), lstm_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, config: Config, embeddings):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.c = config\n",
    "        \n",
    "        self.embedding = embeddings\n",
    "        self.lstm1 = LSTMLayer(input_size=self.c.embedding_size,\n",
    "                              hidden_size=self.c.hidden_size,\n",
    "                              num_layers=self.c.num_layers,\n",
    "                              dropout_rate=self.c.layer_dropout)\n",
    "        self.lstm2 = LSTMLayer(input_size=self.c.hidden_size*2,\n",
    "                            hidden_size=self.c.hidden_size,\n",
    "                            num_layers=self.c.num_layers,\n",
    "                            dropout_rate=self.c.layer_dropout)\n",
    "        \n",
    "        self.cell_dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.linear = nn.Linear(self.c.dense_size[0], self.c.dense_size[1])\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(self.c.dense_size[1])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(self.c.layer_dropout)\n",
    "        self.out = nn.Linear(self.c.dense_size[1], self.c.output_size)\n",
    "        \n",
    "    def forward(self, x, perturb=None):\n",
    "        h_embedding = self.embedding(x, perturb)\n",
    "        o_lstm1, h_lstm1 = self.lstm1(h_embedding)\n",
    "        o_lstm2, h_lstm2 = self.lstm2(o_lstm1)\n",
    "        \n",
    "        avg_pool = torch.mean(o_lstm2, 1)\n",
    "        max_pool, _ = torch.max(o_lstm2, 1)\n",
    "        \n",
    "        h_lstm1 = self.cell_dropout(torch.cat(h_lstm1.split(1, 0), -1).squeeze(0))\n",
    "        h_lstm2 = self.cell_dropout(torch.cat(h_lstm2.split(1, 0), -1).squeeze(0))\n",
    "\n",
    "        concat = torch.cat([h_lstm1, h_lstm2, avg_pool, max_pool], 1)\n",
    "        concat = self.linear(concat)\n",
    "        concat = self.batch_norm(concat)\n",
    "        concat = self.relu(concat)\n",
    "        concat = self.dropout(concat)\n",
    "        out = self.out(concat)\n",
    "        \n",
    "        return out, h_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_length(data, mask):\n",
    "    max_length = data.shape[1]\n",
    "    transposed = torch.transpose(data, 1, 0)\n",
    "    res = (transposed == mask).all(1)\n",
    "    for i, r in enumerate(res):\n",
    "        if r == 0:\n",
    "            break\n",
    "    data = data[:, -(max_length - i):]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_x, train_y, test_x, c, embeddings, trial=0):\n",
    "    splits = list(StratifiedKFold(n_splits=c.num_cv_splits, shuffle=True).split(train_x, train_y))\n",
    "    x_test_cuda = torch.tensor(test_x, dtype=torch.long).cuda(cuda_idx)\n",
    "    test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=c.test_batch_size, shuffle=False)\n",
    "    train_preds = np.zeros((len(train_x)))\n",
    "    test_preds = np.zeros((len(test_x)))\n",
    "\n",
    "    mask = torch.zeros((c.max_length, 1), dtype=torch.long).cuda(cuda_idx)\n",
    "    \n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        x_train_fold = torch.tensor(train_x[train_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "        x_val_fold = torch.tensor(train_x[valid_idx], dtype=torch.long).cuda(cuda_idx)\n",
    "        y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda(cuda_idx)\n",
    "\n",
    "        model = SimpleRNN(c, embeddings)\n",
    "        model.cuda(cuda_idx)\n",
    "\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=c.learning_rate)\n",
    "\n",
    "        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "        valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=c.batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid, batch_size=c.test_batch_size, shuffle=False)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "        print(f'Fold {i + 1}')\n",
    "\n",
    "        for epoch in range(c.num_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred, embedding = model(x_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                perturb = embedding.grad\n",
    "                perturb = 0.01 * perturb / torch.norm(perturb, p=2)\n",
    "                perturb = perturb.detach()\n",
    "\n",
    "                y_pred, embedding = model(x_batch, perturb)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), c.clip_grad)\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "            avg_val_loss = 0.\n",
    "\n",
    "            # validation prediction\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                x_batch = cut_length(x_batch, mask)\n",
    "                y_pred, _ = model(x_batch)\n",
    "                y_pred = y_pred.detach()\n",
    "                avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "                valid_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            search_result = threshold_search(y_val_fold.cpu().numpy(), valid_preds_fold)\n",
    "            valid_pred_targets = valid_preds_fold > search_result['threshold']\n",
    "            val_f1 = f1_score(y_val_fold.cpu().numpy(), valid_pred_targets)\n",
    "\n",
    "            elapsed_time = time.time() - start_time \n",
    "            print('Epoch {}/{}  loss={:.4f}  val_loss={:.4f}  f1={:.3f}  time={:.2f}s'.format(\n",
    "                epoch + 1, c.num_epochs, avg_loss, avg_val_loss, val_f1, elapsed_time))\n",
    "            if best_f1 < val_f1:\n",
    "                print(f'model_saved at f1: {val_f1} from {best_f1}')\n",
    "                ckpt_path = Path(f'./ckpt/adv/{trial}/')\n",
    "                if not ckpt_path.exists():\n",
    "                    ckpt_path.mkdir(parents=True)\n",
    "                torch.save(model.state_dict(),ckpt_path / f'{i}_model.pt')\n",
    "                best_f1 = val_f1\n",
    "                best_epoch = epoch\n",
    "\n",
    "        # test prediction\n",
    "        model.load_state_dict(torch.load(f'./ckpt/adv/{trial}/{i}_model.pt'))  # load best model\n",
    "        test_preds_fold = np.zeros(len(test_x))\n",
    "        for i, (x_batch, ) in enumerate(test_loader):\n",
    "            x_batch = cut_length(x_batch, mask)\n",
    "            y_pred, _ = model(x_batch)\n",
    "            y_pred = y_pred.detach()\n",
    "            test_preds_fold[i * c.test_batch_size:(i+1) * c.test_batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "        train_preds[valid_idx] = valid_preds_fold\n",
    "        test_preds += test_preds_fold / len(splits)\n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1738  val_loss=0.1106  f1=0.651  time=167.56s\n",
      "model_saved at f1: 0.6509729659135431 from 0.0\n",
      "Epoch 2/15  loss=0.1068  val_loss=0.1033  f1=0.671  time=198.45s\n",
      "model_saved at f1: 0.6705996131528047 from 0.6509729659135431\n",
      "Epoch 3/15  loss=0.1003  val_loss=0.0994  f1=0.677  time=201.26s\n",
      "model_saved at f1: 0.6773767747335888 from 0.6705996131528047\n",
      "Epoch 4/15  loss=0.0953  val_loss=0.0981  f1=0.684  time=200.91s\n",
      "model_saved at f1: 0.6836340411943159 from 0.6773767747335888\n",
      "Epoch 5/15  loss=0.0908  val_loss=0.0966  f1=0.686  time=200.69s\n",
      "model_saved at f1: 0.6864748155070736 from 0.6836340411943159\n",
      "Epoch 6/15  loss=0.0865  val_loss=0.0990  f1=0.684  time=198.67s\n",
      "Epoch 7/15  loss=0.0822  val_loss=0.0997  f1=0.686  time=202.05s\n",
      "Epoch 8/15  loss=0.0780  val_loss=0.1014  f1=0.680  time=199.96s\n",
      "Epoch 9/15  loss=0.0736  val_loss=0.1025  f1=0.680  time=199.05s\n",
      "Epoch 10/15  loss=0.0693  val_loss=0.1094  f1=0.676  time=201.47s\n",
      "Epoch 11/15  loss=0.0652  val_loss=0.1125  f1=0.675  time=201.47s\n",
      "Epoch 12/15  loss=0.0612  val_loss=0.1175  f1=0.671  time=201.34s\n",
      "Epoch 13/15  loss=0.0577  val_loss=0.1203  f1=0.666  time=195.78s\n",
      "Epoch 14/15  loss=0.0540  val_loss=0.1326  f1=0.661  time=202.01s\n",
      "Epoch 15/15  loss=0.0507  val_loss=0.1332  f1=0.662  time=199.94s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1939  val_loss=0.1130  f1=0.628  time=199.46s\n",
      "model_saved at f1: 0.6281939424155554 from 0.0\n",
      "Epoch 2/15  loss=0.1088  val_loss=0.1040  f1=0.663  time=199.78s\n",
      "model_saved at f1: 0.662935639786314 from 0.6281939424155554\n",
      "Epoch 3/15  loss=0.1018  val_loss=0.1001  f1=0.674  time=200.72s\n",
      "model_saved at f1: 0.673547813383301 from 0.662935639786314\n",
      "Epoch 4/15  loss=0.0970  val_loss=0.1012  f1=0.679  time=201.97s\n",
      "model_saved at f1: 0.6792272074085901 from 0.673547813383301\n",
      "Epoch 5/15  loss=0.0926  val_loss=0.0993  f1=0.682  time=198.40s\n",
      "model_saved at f1: 0.6818181818181819 from 0.6792272074085901\n",
      "Epoch 6/15  loss=0.0886  val_loss=0.0979  f1=0.686  time=201.93s\n",
      "model_saved at f1: 0.6864149030541122 from 0.6818181818181819\n",
      "Epoch 7/15  loss=0.0843  val_loss=0.0995  f1=0.685  time=202.35s\n",
      "Epoch 8/15  loss=0.0801  val_loss=0.1037  f1=0.682  time=200.05s\n",
      "Epoch 9/15  loss=0.0758  val_loss=0.1039  f1=0.679  time=198.56s\n",
      "Epoch 10/15  loss=0.0715  val_loss=0.1089  f1=0.678  time=200.84s\n",
      "Epoch 11/15  loss=0.0674  val_loss=0.1160  f1=0.669  time=201.86s\n",
      "Epoch 12/15  loss=0.0635  val_loss=0.1207  f1=0.661  time=197.14s\n",
      "Epoch 13/15  loss=0.0596  val_loss=0.1269  f1=0.662  time=201.58s\n",
      "Epoch 14/15  loss=0.0559  val_loss=0.1284  f1=0.659  time=201.39s\n",
      "Epoch 15/15  loss=0.0525  val_loss=0.1383  f1=0.662  time=201.40s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1414  val_loss=0.1082  f1=0.648  time=197.28s\n",
      "model_saved at f1: 0.647587375691491 from 0.0\n",
      "Epoch 2/15  loss=0.1047  val_loss=0.1028  f1=0.668  time=201.58s\n",
      "model_saved at f1: 0.6681364366476646 from 0.647587375691491\n",
      "Epoch 3/15  loss=0.0987  val_loss=0.1003  f1=0.677  time=200.14s\n",
      "model_saved at f1: 0.6774584657731756 from 0.6681364366476646\n",
      "Epoch 4/15  loss=0.0938  val_loss=0.0982  f1=0.683  time=196.56s\n",
      "model_saved at f1: 0.6828176936844798 from 0.6774584657731756\n",
      "Epoch 5/15  loss=0.0896  val_loss=0.0981  f1=0.680  time=202.16s\n",
      "Epoch 6/15  loss=0.0853  val_loss=0.1013  f1=0.677  time=199.22s\n",
      "Epoch 7/15  loss=0.0810  val_loss=0.1001  f1=0.683  time=200.08s\n",
      "Epoch 8/15  loss=0.0766  val_loss=0.1061  f1=0.678  time=195.28s\n",
      "Epoch 9/15  loss=0.0726  val_loss=0.1101  f1=0.676  time=201.54s\n",
      "Epoch 10/15  loss=0.0684  val_loss=0.1114  f1=0.673  time=199.39s\n",
      "Epoch 11/15  loss=0.0642  val_loss=0.1142  f1=0.669  time=201.29s\n",
      "Epoch 12/15  loss=0.0603  val_loss=0.1229  f1=0.669  time=197.70s\n",
      "Epoch 13/15  loss=0.0566  val_loss=0.1253  f1=0.666  time=200.52s\n",
      "Epoch 14/15  loss=0.0532  val_loss=0.1341  f1=0.664  time=202.03s\n",
      "Epoch 15/15  loss=0.0501  val_loss=0.1434  f1=0.656  time=197.72s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1555  val_loss=0.1096  f1=0.650  time=201.93s\n",
      "model_saved at f1: 0.6497985901309165 from 0.0\n",
      "Epoch 2/15  loss=0.1056  val_loss=0.1033  f1=0.671  time=201.58s\n",
      "model_saved at f1: 0.6709714726291441 from 0.6497985901309165\n",
      "Epoch 3/15  loss=0.0991  val_loss=0.1020  f1=0.674  time=199.20s\n",
      "model_saved at f1: 0.6743875278396436 from 0.6709714726291441\n",
      "Epoch 4/15  loss=0.0944  val_loss=0.0987  f1=0.680  time=198.07s\n",
      "model_saved at f1: 0.6798747363711894 from 0.6743875278396436\n",
      "Epoch 5/15  loss=0.0899  val_loss=0.1001  f1=0.679  time=201.42s\n",
      "Epoch 6/15  loss=0.0857  val_loss=0.0992  f1=0.682  time=201.80s\n",
      "model_saved at f1: 0.6823250148358684 from 0.6798747363711894\n",
      "Epoch 7/15  loss=0.0814  val_loss=0.1008  f1=0.677  time=200.83s\n",
      "Epoch 8/15  loss=0.0770  val_loss=0.1040  f1=0.678  time=196.56s\n",
      "Epoch 9/15  loss=0.0729  val_loss=0.1055  f1=0.678  time=202.60s\n",
      "Epoch 10/15  loss=0.0685  val_loss=0.1086  f1=0.673  time=202.28s\n",
      "Epoch 11/15  loss=0.0646  val_loss=0.1163  f1=0.668  time=200.19s\n",
      "Epoch 12/15  loss=0.0608  val_loss=0.1176  f1=0.665  time=201.80s\n",
      "Epoch 13/15  loss=0.0570  val_loss=0.1258  f1=0.662  time=202.28s\n",
      "Epoch 14/15  loss=0.0536  val_loss=0.1356  f1=0.657  time=200.45s\n",
      "Epoch 15/15  loss=0.0509  val_loss=0.1393  f1=0.660  time=195.57s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1593  val_loss=0.1099  f1=0.643  time=202.34s\n",
      "model_saved at f1: 0.6432010727128664 from 0.0\n",
      "Epoch 2/15  loss=0.1081  val_loss=0.1042  f1=0.665  time=201.16s\n",
      "model_saved at f1: 0.6648485438784455 from 0.6432010727128664\n",
      "Epoch 3/15  loss=0.1016  val_loss=0.1001  f1=0.679  time=200.57s\n",
      "model_saved at f1: 0.679369166398455 from 0.6648485438784455\n",
      "Epoch 4/15  loss=0.0965  val_loss=0.0987  f1=0.682  time=200.82s\n",
      "model_saved at f1: 0.6824378766880459 from 0.679369166398455\n",
      "Epoch 5/15  loss=0.0920  val_loss=0.0974  f1=0.686  time=202.43s\n",
      "model_saved at f1: 0.6863507669332006 from 0.6824378766880459\n",
      "Epoch 6/15  loss=0.0878  val_loss=0.0971  f1=0.687  time=200.22s\n",
      "model_saved at f1: 0.6866527961666982 from 0.6863507669332006\n",
      "Epoch 7/15  loss=0.0833  val_loss=0.0995  f1=0.688  time=198.11s\n",
      "model_saved at f1: 0.6879553259090472 from 0.6866527961666982\n",
      "Epoch 8/15  loss=0.0790  val_loss=0.1017  f1=0.684  time=202.75s\n",
      "Epoch 9/15  loss=0.0744  val_loss=0.1041  f1=0.684  time=200.90s\n",
      "Epoch 10/15  loss=0.0701  val_loss=0.1084  f1=0.679  time=200.29s\n",
      "Epoch 11/15  loss=0.0655  val_loss=0.1156  f1=0.674  time=198.25s\n",
      "Epoch 12/15  loss=0.0614  val_loss=0.1199  f1=0.670  time=202.34s\n",
      "Epoch 13/15  loss=0.0576  val_loss=0.1206  f1=0.666  time=200.30s\n",
      "Epoch 14/15  loss=0.0538  val_loss=0.1366  f1=0.663  time=198.13s\n",
      "Epoch 15/15  loss=0.0504  val_loss=0.1448  f1=0.660  time=200.87s\n",
      "{'threshold': 0.2481481283903122, 'f1': 0.6593984768961613}\n",
      "f1 score: 0.6880865277556862\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1484  val_loss=0.1084  f1=0.650  time=200.16s\n",
      "model_saved at f1: 0.6497287860839206 from 0.0\n",
      "Epoch 2/15  loss=0.1053  val_loss=0.1019  f1=0.672  time=201.44s\n",
      "model_saved at f1: 0.6715672050810846 from 0.6497287860839206\n",
      "Epoch 3/15  loss=0.0992  val_loss=0.1004  f1=0.677  time=197.65s\n",
      "model_saved at f1: 0.676933052358567 from 0.6715672050810846\n",
      "Epoch 4/15  loss=0.0943  val_loss=0.0981  f1=0.684  time=202.25s\n",
      "model_saved at f1: 0.6840380413828556 from 0.676933052358567\n",
      "Epoch 5/15  loss=0.0897  val_loss=0.0990  f1=0.684  time=199.57s\n",
      "model_saved at f1: 0.6843267108167771 from 0.6840380413828556\n",
      "Epoch 6/15  loss=0.0854  val_loss=0.0983  f1=0.685  time=196.93s\n",
      "model_saved at f1: 0.6850803565619105 from 0.6843267108167771\n",
      "Epoch 7/15  loss=0.0811  val_loss=0.1041  f1=0.681  time=201.61s\n",
      "Epoch 8/15  loss=0.0765  val_loss=0.1050  f1=0.676  time=199.54s\n",
      "Epoch 9/15  loss=0.0719  val_loss=0.1084  f1=0.678  time=201.87s\n",
      "Epoch 10/15  loss=0.0675  val_loss=0.1110  f1=0.676  time=196.69s\n",
      "Epoch 11/15  loss=0.0632  val_loss=0.1198  f1=0.670  time=203.01s\n",
      "Epoch 12/15  loss=0.0593  val_loss=0.1269  f1=0.665  time=201.48s\n",
      "Epoch 13/15  loss=0.0556  val_loss=0.1312  f1=0.665  time=201.18s\n",
      "Epoch 14/15  loss=0.0520  val_loss=0.1384  f1=0.663  time=198.17s\n",
      "Epoch 15/15  loss=0.0486  val_loss=0.1446  f1=0.659  time=201.89s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1643  val_loss=0.1085  f1=0.648  time=202.90s\n",
      "model_saved at f1: 0.6482718559395876 from 0.0\n",
      "Epoch 2/15  loss=0.1081  val_loss=0.1037  f1=0.664  time=197.51s\n",
      "model_saved at f1: 0.663733341774556 from 0.6482718559395876\n",
      "Epoch 3/15  loss=0.1014  val_loss=0.0991  f1=0.676  time=203.17s\n",
      "model_saved at f1: 0.6763670353637639 from 0.663733341774556\n",
      "Epoch 4/15  loss=0.0964  val_loss=0.0978  f1=0.681  time=202.51s\n",
      "model_saved at f1: 0.6814893683650143 from 0.6763670353637639\n",
      "Epoch 5/15  loss=0.0922  val_loss=0.0965  f1=0.688  time=199.26s\n",
      "model_saved at f1: 0.6884868421052632 from 0.6814893683650143\n",
      "Epoch 6/15  loss=0.0882  val_loss=0.0967  f1=0.685  time=196.19s\n",
      "Epoch 7/15  loss=0.0838  val_loss=0.0991  f1=0.688  time=201.31s\n",
      "Epoch 8/15  loss=0.0797  val_loss=0.1013  f1=0.681  time=202.94s\n",
      "Epoch 9/15  loss=0.0755  val_loss=0.1035  f1=0.683  time=199.85s\n",
      "Epoch 10/15  loss=0.0714  val_loss=0.1083  f1=0.676  time=201.21s\n",
      "Epoch 11/15  loss=0.0670  val_loss=0.1109  f1=0.666  time=202.33s\n",
      "Epoch 12/15  loss=0.0633  val_loss=0.1136  f1=0.665  time=201.32s\n",
      "Epoch 13/15  loss=0.0593  val_loss=0.1219  f1=0.666  time=196.24s\n",
      "Epoch 14/15  loss=0.0560  val_loss=0.1264  f1=0.662  time=201.26s\n",
      "Epoch 15/15  loss=0.0528  val_loss=0.1310  f1=0.657  time=203.40s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1687  val_loss=0.1114  f1=0.639  time=197.76s\n",
      "model_saved at f1: 0.6385057471264368 from 0.0\n",
      "Epoch 2/15  loss=0.1073  val_loss=0.1055  f1=0.661  time=199.65s\n",
      "model_saved at f1: 0.6605102007400858 from 0.6385057471264368\n",
      "Epoch 3/15  loss=0.1012  val_loss=0.1011  f1=0.670  time=202.66s\n",
      "model_saved at f1: 0.6704307146313921 from 0.6605102007400858\n",
      "Epoch 4/15  loss=0.0966  val_loss=0.0996  f1=0.677  time=201.42s\n",
      "model_saved at f1: 0.6770679042822817 from 0.6704307146313921\n",
      "Epoch 5/15  loss=0.0923  val_loss=0.0982  f1=0.681  time=198.54s\n",
      "model_saved at f1: 0.6810845499759346 from 0.6770679042822817\n",
      "Epoch 6/15  loss=0.0882  val_loss=0.0993  f1=0.682  time=201.74s\n",
      "model_saved at f1: 0.6822227256545519 from 0.6810845499759346\n",
      "Epoch 7/15  loss=0.0843  val_loss=0.0997  f1=0.681  time=202.32s\n",
      "Epoch 8/15  loss=0.0798  val_loss=0.1078  f1=0.680  time=200.72s\n",
      "Epoch 9/15  loss=0.0771  val_loss=0.1076  f1=0.679  time=198.09s\n",
      "Epoch 10/15  loss=0.0726  val_loss=0.1107  f1=0.675  time=201.51s\n",
      "Epoch 11/15  loss=0.0687  val_loss=0.1161  f1=0.676  time=201.60s\n",
      "Epoch 12/15  loss=0.0651  val_loss=0.1176  f1=0.675  time=198.64s\n",
      "Epoch 13/15  loss=0.0613  val_loss=0.1231  f1=0.673  time=199.73s\n",
      "Epoch 14/15  loss=0.0575  val_loss=0.1338  f1=0.664  time=201.26s\n",
      "Epoch 15/15  loss=0.0540  val_loss=0.1407  f1=0.664  time=202.27s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1437  val_loss=0.1090  f1=0.652  time=196.49s\n",
      "model_saved at f1: 0.6517467959143842 from 0.0\n",
      "Epoch 2/15  loss=0.1056  val_loss=0.1023  f1=0.674  time=201.73s\n",
      "model_saved at f1: 0.6741186027619822 from 0.6517467959143842\n",
      "Epoch 3/15  loss=0.0994  val_loss=0.0980  f1=0.685  time=201.01s\n",
      "model_saved at f1: 0.6854478211931179 from 0.6741186027619822\n",
      "Epoch 4/15  loss=0.0943  val_loss=0.0972  f1=0.685  time=198.50s\n",
      "model_saved at f1: 0.6854514458490142 from 0.6854478211931179\n",
      "Epoch 5/15  loss=0.0900  val_loss=0.0972  f1=0.689  time=199.96s\n",
      "model_saved at f1: 0.6886558999739516 from 0.6854514458490142\n",
      "Epoch 6/15  loss=0.0857  val_loss=0.0977  f1=0.689  time=201.39s\n",
      "model_saved at f1: 0.6894395926653991 from 0.6886558999739516\n",
      "Epoch 7/15  loss=0.0812  val_loss=0.1014  f1=0.686  time=201.00s\n",
      "Epoch 8/15  loss=0.0768  val_loss=0.1026  f1=0.683  time=197.66s\n",
      "Epoch 9/15  loss=0.0726  val_loss=0.1119  f1=0.681  time=201.57s\n",
      "Epoch 10/15  loss=0.0683  val_loss=0.1066  f1=0.679  time=199.57s\n",
      "Epoch 11/15  loss=0.0642  val_loss=0.1131  f1=0.673  time=201.03s\n",
      "Epoch 12/15  loss=0.0604  val_loss=0.1194  f1=0.667  time=198.42s\n",
      "Epoch 13/15  loss=0.0567  val_loss=0.1233  f1=0.668  time=201.07s\n",
      "Epoch 14/15  loss=0.0535  val_loss=0.1310  f1=0.668  time=202.28s\n",
      "Epoch 15/15  loss=0.0507  val_loss=0.1424  f1=0.659  time=198.22s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1635  val_loss=0.1110  f1=0.637  time=201.82s\n",
      "model_saved at f1: 0.6369909612207212 from 0.0\n",
      "Epoch 2/15  loss=0.1058  val_loss=0.1046  f1=0.662  time=202.85s\n",
      "model_saved at f1: 0.6619417475728154 from 0.6369909612207212\n",
      "Epoch 3/15  loss=0.0991  val_loss=0.1042  f1=0.672  time=202.10s\n",
      "model_saved at f1: 0.6722737082645678 from 0.6619417475728154\n",
      "Epoch 4/15  loss=0.0943  val_loss=0.1002  f1=0.671  time=196.61s\n",
      "Epoch 5/15  loss=0.0898  val_loss=0.1010  f1=0.680  time=200.58s\n",
      "model_saved at f1: 0.6798720392929848 from 0.6722737082645678\n",
      "Epoch 6/15  loss=0.0855  val_loss=0.0999  f1=0.677  time=201.00s\n",
      "Epoch 7/15  loss=0.0810  val_loss=0.1019  f1=0.678  time=199.12s\n",
      "Epoch 8/15  loss=0.0764  val_loss=0.1060  f1=0.674  time=197.41s\n",
      "Epoch 9/15  loss=0.0720  val_loss=0.1095  f1=0.670  time=200.83s\n",
      "Epoch 10/15  loss=0.0677  val_loss=0.1119  f1=0.670  time=200.94s\n",
      "Epoch 11/15  loss=0.0635  val_loss=0.1163  f1=0.663  time=198.10s\n",
      "Epoch 12/15  loss=0.0594  val_loss=0.1286  f1=0.664  time=201.18s\n",
      "Epoch 13/15  loss=0.0557  val_loss=0.1302  f1=0.658  time=201.01s\n",
      "Epoch 14/15  loss=0.0520  val_loss=0.1418  f1=0.660  time=201.28s\n",
      "Epoch 15/15  loss=0.0487  val_loss=0.1439  f1=0.653  time=197.09s\n",
      "{'threshold': 0.27929794788360596, 'f1': 0.657332826012653}\n",
      "f1 score: 0.695686230627203\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1482  val_loss=0.1084  f1=0.651  time=200.56s\n",
      "model_saved at f1: 0.6512512899896801 from 0.0\n",
      "Epoch 2/15  loss=0.1052  val_loss=0.1030  f1=0.669  time=202.22s\n",
      "model_saved at f1: 0.6689555591208086 from 0.6512512899896801\n",
      "Epoch 3/15  loss=0.0986  val_loss=0.0999  f1=0.679  time=196.50s\n",
      "model_saved at f1: 0.6787637855860477 from 0.6689555591208086\n",
      "Epoch 4/15  loss=0.0942  val_loss=0.0986  f1=0.683  time=201.09s\n",
      "model_saved at f1: 0.6832728892282243 from 0.6787637855860477\n",
      "Epoch 5/15  loss=0.0897  val_loss=0.0985  f1=0.684  time=201.31s\n",
      "model_saved at f1: 0.6839417678962225 from 0.6832728892282243\n",
      "Epoch 6/15  loss=0.0855  val_loss=0.1000  f1=0.683  time=202.13s\n",
      "Epoch 7/15  loss=0.0813  val_loss=0.1022  f1=0.681  time=197.76s\n",
      "Epoch 8/15  loss=0.0767  val_loss=0.1048  f1=0.681  time=200.38s\n",
      "Epoch 9/15  loss=0.0725  val_loss=0.1096  f1=0.680  time=201.46s\n",
      "Epoch 10/15  loss=0.0683  val_loss=0.1176  f1=0.675  time=200.27s\n",
      "Epoch 11/15  loss=0.0641  val_loss=0.1206  f1=0.674  time=200.23s\n",
      "Epoch 12/15  loss=0.0604  val_loss=0.1285  f1=0.671  time=201.38s\n",
      "Epoch 13/15  loss=0.0568  val_loss=0.1320  f1=0.667  time=201.64s\n",
      "Epoch 14/15  loss=0.0535  val_loss=0.1446  f1=0.663  time=197.84s\n",
      "Epoch 15/15  loss=0.0506  val_loss=0.1543  f1=0.662  time=198.01s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1565  val_loss=0.1083  f1=0.651  time=201.39s\n",
      "model_saved at f1: 0.6507971037011717 from 0.0\n",
      "Epoch 2/15  loss=0.1066  val_loss=0.1038  f1=0.670  time=199.56s\n",
      "model_saved at f1: 0.6696601628117328 from 0.6507971037011717\n",
      "Epoch 3/15  loss=0.0998  val_loss=0.0993  f1=0.676  time=196.56s\n",
      "model_saved at f1: 0.6762502009969449 from 0.6696601628117328\n",
      "Epoch 4/15  loss=0.0949  val_loss=0.0984  f1=0.681  time=202.05s\n",
      "model_saved at f1: 0.68141075604053 from 0.6762502009969449\n",
      "Epoch 5/15  loss=0.0905  val_loss=0.0980  f1=0.683  time=201.55s\n",
      "model_saved at f1: 0.6831867664846841 from 0.68141075604053\n",
      "Epoch 6/15  loss=0.0862  val_loss=0.1013  f1=0.684  time=197.31s\n",
      "model_saved at f1: 0.6839147286821704 from 0.6831867664846841\n",
      "Epoch 7/15  loss=0.0819  val_loss=0.1007  f1=0.681  time=202.51s\n",
      "Epoch 8/15  loss=0.0776  val_loss=0.1054  f1=0.681  time=201.57s\n",
      "Epoch 9/15  loss=0.0731  val_loss=0.1082  f1=0.675  time=199.80s\n",
      "Epoch 10/15  loss=0.0690  val_loss=0.1084  f1=0.674  time=196.52s\n",
      "Epoch 11/15  loss=0.0649  val_loss=0.1126  f1=0.669  time=202.44s\n",
      "Epoch 12/15  loss=0.0606  val_loss=0.1240  f1=0.665  time=198.42s\n",
      "Epoch 13/15  loss=0.0576  val_loss=0.1226  f1=0.663  time=201.05s\n",
      "Epoch 14/15  loss=0.0539  val_loss=0.1324  f1=0.663  time=198.94s\n",
      "Epoch 15/15  loss=0.0506  val_loss=0.1418  f1=0.653  time=201.47s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1498  val_loss=0.1065  f1=0.655  time=199.66s\n",
      "model_saved at f1: 0.6546596166556511 from 0.0\n",
      "Epoch 2/15  loss=0.1050  val_loss=0.1043  f1=0.672  time=197.07s\n",
      "model_saved at f1: 0.6722016018753663 from 0.6546596166556511\n",
      "Epoch 3/15  loss=0.0989  val_loss=0.0983  f1=0.683  time=201.79s\n",
      "model_saved at f1: 0.6825381463918191 from 0.6722016018753663\n",
      "Epoch 4/15  loss=0.0940  val_loss=0.0973  f1=0.684  time=201.64s\n",
      "model_saved at f1: 0.6837139862139069 from 0.6825381463918191\n",
      "Epoch 5/15  loss=0.0896  val_loss=0.0977  f1=0.689  time=200.99s\n",
      "model_saved at f1: 0.6886472884472692 from 0.6837139862139069\n",
      "Epoch 6/15  loss=0.0852  val_loss=0.0976  f1=0.686  time=197.57s\n",
      "Epoch 7/15  loss=0.0807  val_loss=0.0989  f1=0.684  time=200.25s\n",
      "Epoch 8/15  loss=0.0762  val_loss=0.1047  f1=0.682  time=200.61s\n",
      "Epoch 9/15  loss=0.0720  val_loss=0.1066  f1=0.682  time=197.91s\n",
      "Epoch 10/15  loss=0.0678  val_loss=0.1096  f1=0.675  time=198.81s\n",
      "Epoch 11/15  loss=0.0638  val_loss=0.1158  f1=0.673  time=201.04s\n",
      "Epoch 12/15  loss=0.0598  val_loss=0.1170  f1=0.665  time=201.94s\n",
      "Epoch 13/15  loss=0.0563  val_loss=0.1247  f1=0.667  time=197.52s\n",
      "Epoch 14/15  loss=0.0528  val_loss=0.1400  f1=0.666  time=200.37s\n",
      "Epoch 15/15  loss=0.0498  val_loss=0.1442  f1=0.663  time=199.49s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1626  val_loss=0.1094  f1=0.653  time=199.36s\n",
      "model_saved at f1: 0.6527958387516255 from 0.0\n",
      "Epoch 2/15  loss=0.1065  val_loss=0.1033  f1=0.668  time=198.75s\n",
      "model_saved at f1: 0.6676482135846094 from 0.6527958387516255\n",
      "Epoch 3/15  loss=0.0998  val_loss=0.0995  f1=0.680  time=199.90s\n",
      "model_saved at f1: 0.6799948572897917 from 0.6676482135846094\n",
      "Epoch 4/15  loss=0.0951  val_loss=0.0988  f1=0.686  time=202.37s\n",
      "model_saved at f1: 0.6858991113434395 from 0.6799948572897917\n",
      "Epoch 5/15  loss=0.0906  val_loss=0.0989  f1=0.680  time=195.56s\n",
      "Epoch 6/15  loss=0.0864  val_loss=0.0991  f1=0.685  time=200.29s\n",
      "Epoch 7/15  loss=0.0820  val_loss=0.1002  f1=0.681  time=201.26s\n",
      "Epoch 8/15  loss=0.0775  val_loss=0.1023  f1=0.680  time=201.47s\n",
      "Epoch 9/15  loss=0.0735  val_loss=0.1039  f1=0.681  time=196.15s\n",
      "Epoch 10/15  loss=0.0690  val_loss=0.1087  f1=0.669  time=201.55s\n",
      "Epoch 11/15  loss=0.0651  val_loss=0.1151  f1=0.669  time=201.58s\n",
      "Epoch 12/15  loss=0.0611  val_loss=0.1171  f1=0.668  time=200.03s\n",
      "Epoch 13/15  loss=0.0575  val_loss=0.1260  f1=0.661  time=196.68s\n",
      "Epoch 14/15  loss=0.0545  val_loss=0.1293  f1=0.660  time=201.78s\n",
      "Epoch 15/15  loss=0.0511  val_loss=0.1371  f1=0.661  time=200.52s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1472  val_loss=0.1078  f1=0.652  time=196.14s\n",
      "model_saved at f1: 0.6519305568731619 from 0.0\n",
      "Epoch 2/15  loss=0.1046  val_loss=0.1057  f1=0.668  time=198.77s\n",
      "model_saved at f1: 0.6676339717546914 from 0.6519305568731619\n",
      "Epoch 3/15  loss=0.0988  val_loss=0.0995  f1=0.677  time=202.19s\n",
      "model_saved at f1: 0.6768193812990811 from 0.6676339717546914\n",
      "Epoch 4/15  loss=0.0941  val_loss=0.0982  f1=0.680  time=202.11s\n",
      "model_saved at f1: 0.6796496518977189 from 0.6768193812990811\n",
      "Epoch 5/15  loss=0.0897  val_loss=0.0979  f1=0.682  time=196.88s\n",
      "model_saved at f1: 0.682275115117712 from 0.6796496518977189\n",
      "Epoch 6/15  loss=0.0853  val_loss=0.0989  f1=0.680  time=202.46s\n",
      "Epoch 7/15  loss=0.0811  val_loss=0.0998  f1=0.680  time=201.87s\n",
      "Epoch 8/15  loss=0.0768  val_loss=0.1026  f1=0.676  time=198.42s\n",
      "Epoch 9/15  loss=0.0723  val_loss=0.1053  f1=0.674  time=200.03s\n",
      "Epoch 10/15  loss=0.0681  val_loss=0.1088  f1=0.671  time=201.50s\n",
      "Epoch 11/15  loss=0.0644  val_loss=0.1162  f1=0.670  time=202.94s\n",
      "Epoch 12/15  loss=0.0605  val_loss=0.1206  f1=0.663  time=197.14s\n",
      "Epoch 13/15  loss=0.0569  val_loss=0.1283  f1=0.660  time=202.03s\n",
      "Epoch 14/15  loss=0.0540  val_loss=0.1313  f1=0.661  time=199.27s\n",
      "Epoch 15/15  loss=0.0508  val_loss=0.1368  f1=0.657  time=199.97s\n",
      "{'threshold': 0.27035778760910034, 'f1': 0.6569435637285986}\n",
      "f1 score: 0.6943375174337517\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1467  val_loss=0.1087  f1=0.649  time=197.10s\n",
      "model_saved at f1: 0.6488473958499856 from 0.0\n",
      "Epoch 2/15  loss=0.1055  val_loss=0.1037  f1=0.667  time=199.39s\n",
      "model_saved at f1: 0.667449760309851 from 0.6488473958499856\n",
      "Epoch 3/15  loss=0.0989  val_loss=0.0990  f1=0.681  time=200.93s\n",
      "model_saved at f1: 0.6812564366632338 from 0.667449760309851\n",
      "Epoch 4/15  loss=0.0941  val_loss=0.0996  f1=0.679  time=198.03s\n",
      "Epoch 5/15  loss=0.0897  val_loss=0.0984  f1=0.685  time=201.20s\n",
      "model_saved at f1: 0.6846467737821984 from 0.6812564366632338\n",
      "Epoch 6/15  loss=0.0853  val_loss=0.0991  f1=0.684  time=201.02s\n",
      "Epoch 7/15  loss=0.0809  val_loss=0.0999  f1=0.682  time=201.97s\n",
      "Epoch 8/15  loss=0.0766  val_loss=0.1062  f1=0.680  time=197.87s\n",
      "Epoch 9/15  loss=0.0723  val_loss=0.1075  f1=0.676  time=198.81s\n",
      "Epoch 10/15  loss=0.0681  val_loss=0.1093  f1=0.676  time=200.36s\n",
      "Epoch 11/15  loss=0.0638  val_loss=0.1183  f1=0.673  time=198.99s\n",
      "Epoch 12/15  loss=0.0600  val_loss=0.1237  f1=0.665  time=196.86s\n",
      "Epoch 13/15  loss=0.0563  val_loss=0.1285  f1=0.662  time=200.56s\n",
      "Epoch 14/15  loss=0.0530  val_loss=0.1344  f1=0.659  time=202.40s\n",
      "Epoch 15/15  loss=0.0500  val_loss=0.1454  f1=0.658  time=199.36s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1583  val_loss=0.1088  f1=0.653  time=198.97s\n",
      "model_saved at f1: 0.6527944511137788 from 0.0\n",
      "Epoch 2/15  loss=0.1059  val_loss=0.1030  f1=0.666  time=201.66s\n",
      "model_saved at f1: 0.6659776500226083 from 0.6527944511137788\n",
      "Epoch 3/15  loss=0.0996  val_loss=0.0997  f1=0.679  time=201.73s\n",
      "model_saved at f1: 0.6794359351517284 from 0.6659776500226083\n",
      "Epoch 4/15  loss=0.0947  val_loss=0.0985  f1=0.683  time=196.60s\n",
      "model_saved at f1: 0.683059236787482 from 0.6794359351517284\n",
      "Epoch 5/15  loss=0.0905  val_loss=0.0991  f1=0.683  time=201.59s\n",
      "Epoch 6/15  loss=0.0862  val_loss=0.0995  f1=0.683  time=201.61s\n",
      "model_saved at f1: 0.6832793747342948 from 0.683059236787482\n",
      "Epoch 7/15  loss=0.0820  val_loss=0.1014  f1=0.681  time=200.34s\n",
      "Epoch 8/15  loss=0.0778  val_loss=0.1044  f1=0.684  time=194.09s\n",
      "model_saved at f1: 0.684047580250937 from 0.6832793747342948\n",
      "Epoch 9/15  loss=0.0732  val_loss=0.1058  f1=0.678  time=200.35s\n",
      "Epoch 10/15  loss=0.0686  val_loss=0.1104  f1=0.675  time=199.81s\n",
      "Epoch 11/15  loss=0.0647  val_loss=0.1171  f1=0.670  time=199.49s\n",
      "Epoch 12/15  loss=0.0607  val_loss=0.1192  f1=0.666  time=198.97s\n",
      "Epoch 13/15  loss=0.0571  val_loss=0.1338  f1=0.664  time=201.01s\n",
      "Epoch 14/15  loss=0.0536  val_loss=0.1360  f1=0.660  time=201.72s\n",
      "Epoch 15/15  loss=0.0506  val_loss=0.1442  f1=0.658  time=195.62s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1541  val_loss=0.1078  f1=0.655  time=201.44s\n",
      "model_saved at f1: 0.6550504725776378 from 0.0\n",
      "Epoch 2/15  loss=0.1047  val_loss=0.1029  f1=0.672  time=200.39s\n",
      "model_saved at f1: 0.6716875326030256 from 0.6550504725776378\n",
      "Epoch 3/15  loss=0.0987  val_loss=0.0991  f1=0.680  time=199.73s\n",
      "model_saved at f1: 0.6803944315545245 from 0.6716875326030256\n",
      "Epoch 4/15  loss=0.0937  val_loss=0.1000  f1=0.681  time=196.91s\n",
      "model_saved at f1: 0.6813642599623521 from 0.6803944315545245\n",
      "Epoch 5/15  loss=0.0893  val_loss=0.0976  f1=0.685  time=203.18s\n",
      "model_saved at f1: 0.6854327031466392 from 0.6813642599623521\n",
      "Epoch 6/15  loss=0.0851  val_loss=0.1002  f1=0.680  time=201.53s\n",
      "Epoch 7/15  loss=0.0807  val_loss=0.1011  f1=0.685  time=198.54s\n",
      "Epoch 8/15  loss=0.0763  val_loss=0.1033  f1=0.682  time=200.27s\n",
      "Epoch 9/15  loss=0.0721  val_loss=0.1077  f1=0.677  time=201.48s\n",
      "Epoch 10/15  loss=0.0678  val_loss=0.1136  f1=0.675  time=201.47s\n",
      "Epoch 11/15  loss=0.0639  val_loss=0.1215  f1=0.673  time=197.34s\n",
      "Epoch 12/15  loss=0.0600  val_loss=0.1175  f1=0.670  time=202.26s\n",
      "Epoch 13/15  loss=0.0563  val_loss=0.1272  f1=0.667  time=202.39s\n",
      "Epoch 14/15  loss=0.0530  val_loss=0.1322  f1=0.663  time=199.94s\n",
      "Epoch 15/15  loss=0.0498  val_loss=0.1393  f1=0.661  time=198.97s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1747  val_loss=0.1115  f1=0.645  time=197.95s\n",
      "model_saved at f1: 0.6452358036573628 from 0.0\n",
      "Epoch 2/15  loss=0.1075  val_loss=0.1040  f1=0.663  time=199.52s\n",
      "model_saved at f1: 0.6632486447652014 from 0.6452358036573628\n",
      "Epoch 3/15  loss=0.1008  val_loss=0.1011  f1=0.674  time=196.99s\n",
      "model_saved at f1: 0.6742057081313947 from 0.6632486447652014\n",
      "Epoch 4/15  loss=0.0960  val_loss=0.0990  f1=0.678  time=202.47s\n",
      "model_saved at f1: 0.6777148128624143 from 0.6742057081313947\n",
      "Epoch 5/15  loss=0.0914  val_loss=0.0982  f1=0.679  time=200.77s\n",
      "model_saved at f1: 0.6790962237320272 from 0.6777148128624143\n",
      "Epoch 6/15  loss=0.0872  val_loss=0.0985  f1=0.679  time=200.12s\n",
      "model_saved at f1: 0.6793321445939375 from 0.6790962237320272\n",
      "Epoch 7/15  loss=0.0829  val_loss=0.0985  f1=0.682  time=197.45s\n",
      "model_saved at f1: 0.6818473380372033 from 0.6793321445939375\n",
      "Epoch 8/15  loss=0.0788  val_loss=0.1012  f1=0.680  time=201.84s\n",
      "Epoch 9/15  loss=0.0746  val_loss=0.1058  f1=0.678  time=199.13s\n",
      "Epoch 10/15  loss=0.0702  val_loss=0.1082  f1=0.672  time=199.52s\n",
      "Epoch 11/15  loss=0.0662  val_loss=0.1128  f1=0.668  time=199.15s\n",
      "Epoch 12/15  loss=0.0624  val_loss=0.1147  f1=0.667  time=201.23s\n",
      "Epoch 13/15  loss=0.0589  val_loss=0.1216  f1=0.663  time=197.23s\n",
      "Epoch 14/15  loss=0.0553  val_loss=0.1308  f1=0.656  time=199.57s\n",
      "Epoch 15/15  loss=0.0520  val_loss=0.1374  f1=0.656  time=199.33s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1437  val_loss=0.1061  f1=0.655  time=200.71s\n",
      "model_saved at f1: 0.6553014553014553 from 0.0\n",
      "Epoch 2/15  loss=0.1045  val_loss=0.1026  f1=0.672  time=201.14s\n",
      "model_saved at f1: 0.6717562275823489 from 0.6553014553014553\n",
      "Epoch 3/15  loss=0.0984  val_loss=0.0982  f1=0.680  time=197.51s\n",
      "model_saved at f1: 0.6803143617695525 from 0.6717562275823489\n",
      "Epoch 4/15  loss=0.0937  val_loss=0.0993  f1=0.681  time=200.63s\n",
      "model_saved at f1: 0.6813179699637494 from 0.6803143617695525\n",
      "Epoch 5/15  loss=0.0894  val_loss=0.0973  f1=0.689  time=200.04s\n",
      "model_saved at f1: 0.6889903602232369 from 0.6813179699637494\n",
      "Epoch 6/15  loss=0.0852  val_loss=0.0979  f1=0.687  time=199.37s\n",
      "Epoch 7/15  loss=0.0808  val_loss=0.1025  f1=0.682  time=200.80s\n",
      "Epoch 8/15  loss=0.0766  val_loss=0.1024  f1=0.685  time=200.13s\n",
      "Epoch 9/15  loss=0.0722  val_loss=0.1079  f1=0.677  time=202.31s\n",
      "Epoch 10/15  loss=0.0679  val_loss=0.1142  f1=0.677  time=133.71s\n",
      "Epoch 11/15  loss=0.0640  val_loss=0.1167  f1=0.672  time=121.62s\n",
      "Epoch 12/15  loss=0.0603  val_loss=0.1223  f1=0.668  time=120.25s\n",
      "Epoch 13/15  loss=0.0567  val_loss=0.1260  f1=0.663  time=122.25s\n",
      "Epoch 14/15  loss=0.0538  val_loss=0.1333  f1=0.668  time=121.59s\n",
      "Epoch 15/15  loss=0.0506  val_loss=0.1406  f1=0.663  time=121.06s\n",
      "{'threshold': 0.27809372544288635, 'f1': 0.6589098789399342}\n",
      "f1 score: 0.6976245556621341\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/data_aug-A9JyLOeQ/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  loss=0.1556  val_loss=0.1077  f1=0.650  time=121.35s\n",
      "model_saved at f1: 0.6502152817942292 from 0.0\n",
      "Epoch 2/15  loss=0.1069  val_loss=0.1026  f1=0.669  time=120.63s\n",
      "model_saved at f1: 0.6688107030295234 from 0.6502152817942292\n",
      "Epoch 3/15  loss=0.1005  val_loss=0.0998  f1=0.676  time=119.87s\n",
      "model_saved at f1: 0.6764845006464635 from 0.6688107030295234\n",
      "Epoch 4/15  loss=0.0959  val_loss=0.0977  f1=0.686  time=122.21s\n",
      "model_saved at f1: 0.6862552639077985 from 0.6764845006464635\n",
      "Epoch 5/15  loss=0.0913  val_loss=0.0996  f1=0.686  time=122.01s\n",
      "Epoch 6/15  loss=0.0874  val_loss=0.0987  f1=0.683  time=121.29s\n",
      "Epoch 7/15  loss=0.0833  val_loss=0.0991  f1=0.685  time=121.06s\n",
      "Epoch 8/15  loss=0.0791  val_loss=0.1004  f1=0.681  time=121.88s\n",
      "Epoch 9/15  loss=0.0747  val_loss=0.1046  f1=0.681  time=120.93s\n",
      "Epoch 10/15  loss=0.0705  val_loss=0.1074  f1=0.671  time=120.20s\n",
      "Epoch 11/15  loss=0.0665  val_loss=0.1095  f1=0.672  time=122.28s\n",
      "Epoch 12/15  loss=0.0623  val_loss=0.1137  f1=0.672  time=122.43s\n",
      "Epoch 13/15  loss=0.0585  val_loss=0.1251  f1=0.667  time=121.66s\n",
      "Epoch 14/15  loss=0.0552  val_loss=0.1259  f1=0.663  time=120.97s\n",
      "Epoch 15/15  loss=0.0520  val_loss=0.1319  f1=0.660  time=121.80s\n",
      "Fold 2\n",
      "Epoch 1/15  loss=0.1425  val_loss=0.1068  f1=0.653  time=121.04s\n",
      "model_saved at f1: 0.653248 from 0.0\n",
      "Epoch 2/15  loss=0.1052  val_loss=0.1011  f1=0.672  time=122.25s\n",
      "model_saved at f1: 0.6720856962822936 from 0.653248\n",
      "Epoch 3/15  loss=0.0991  val_loss=0.1014  f1=0.673  time=122.12s\n",
      "model_saved at f1: 0.6733655558382091 from 0.6720856962822936\n",
      "Epoch 4/15  loss=0.0942  val_loss=0.0980  f1=0.682  time=122.66s\n",
      "model_saved at f1: 0.6823649207662444 from 0.6733655558382091\n",
      "Epoch 5/15  loss=0.0898  val_loss=0.0977  f1=0.683  time=123.18s\n",
      "model_saved at f1: 0.6832465021789705 from 0.6823649207662444\n",
      "Epoch 6/15  loss=0.0856  val_loss=0.0982  f1=0.687  time=121.91s\n",
      "model_saved at f1: 0.6865575667942 from 0.6832465021789705\n",
      "Epoch 7/15  loss=0.0813  val_loss=0.0988  f1=0.683  time=122.18s\n",
      "Epoch 8/15  loss=0.0768  val_loss=0.1015  f1=0.681  time=121.74s\n",
      "Epoch 9/15  loss=0.0727  val_loss=0.1046  f1=0.682  time=122.29s\n",
      "Epoch 10/15  loss=0.0683  val_loss=0.1088  f1=0.677  time=122.42s\n",
      "Epoch 11/15  loss=0.0644  val_loss=0.1154  f1=0.669  time=122.47s\n",
      "Epoch 12/15  loss=0.0608  val_loss=0.1182  f1=0.671  time=123.03s\n",
      "Epoch 13/15  loss=0.0573  val_loss=0.1231  f1=0.664  time=122.35s\n",
      "Epoch 14/15  loss=0.0541  val_loss=0.1337  f1=0.664  time=122.31s\n",
      "Epoch 15/15  loss=0.0510  val_loss=0.1346  f1=0.661  time=121.71s\n",
      "Fold 3\n",
      "Epoch 1/15  loss=0.1628  val_loss=0.1083  f1=0.648  time=121.92s\n",
      "model_saved at f1: 0.6477329659318637 from 0.0\n",
      "Epoch 2/15  loss=0.1057  val_loss=0.1029  f1=0.668  time=120.80s\n",
      "model_saved at f1: 0.6682471945629841 from 0.6477329659318637\n",
      "Epoch 3/15  loss=0.0992  val_loss=0.1003  f1=0.676  time=122.35s\n",
      "model_saved at f1: 0.6757178825254166 from 0.6682471945629841\n",
      "Epoch 4/15  loss=0.0945  val_loss=0.0988  f1=0.680  time=122.70s\n",
      "model_saved at f1: 0.6795772336374976 from 0.6757178825254166\n",
      "Epoch 5/15  loss=0.0900  val_loss=0.1003  f1=0.681  time=121.87s\n",
      "model_saved at f1: 0.68114171131372 from 0.6795772336374976\n",
      "Epoch 6/15  loss=0.0856  val_loss=0.1001  f1=0.679  time=121.64s\n",
      "Epoch 7/15  loss=0.0814  val_loss=0.1014  f1=0.678  time=123.61s\n",
      "Epoch 8/15  loss=0.0770  val_loss=0.1031  f1=0.678  time=121.89s\n",
      "Epoch 9/15  loss=0.0727  val_loss=0.1049  f1=0.676  time=120.91s\n",
      "Epoch 10/15  loss=0.0686  val_loss=0.1144  f1=0.671  time=122.09s\n",
      "Epoch 11/15  loss=0.0645  val_loss=0.1180  f1=0.671  time=123.06s\n",
      "Epoch 12/15  loss=0.0609  val_loss=0.1239  f1=0.666  time=122.66s\n",
      "Epoch 13/15  loss=0.0572  val_loss=0.1275  f1=0.659  time=122.45s\n",
      "Epoch 14/15  loss=0.0539  val_loss=0.1374  f1=0.664  time=122.97s\n",
      "Epoch 15/15  loss=0.0506  val_loss=0.1403  f1=0.660  time=122.16s\n",
      "Fold 4\n",
      "Epoch 1/15  loss=0.1399  val_loss=0.1067  f1=0.652  time=122.38s\n",
      "model_saved at f1: 0.6516656925774401 from 0.0\n",
      "Epoch 2/15  loss=0.1047  val_loss=0.1011  f1=0.671  time=122.55s\n",
      "model_saved at f1: 0.6711111111111112 from 0.6516656925774401\n",
      "Epoch 3/15  loss=0.0986  val_loss=0.0989  f1=0.683  time=121.31s\n",
      "model_saved at f1: 0.6825832714114948 from 0.6711111111111112\n",
      "Epoch 4/15  loss=0.0938  val_loss=0.0989  f1=0.682  time=122.24s\n",
      "Epoch 5/15  loss=0.0896  val_loss=0.0976  f1=0.684  time=121.30s\n",
      "model_saved at f1: 0.6843367411334091 from 0.6825832714114948\n",
      "Epoch 6/15  loss=0.0851  val_loss=0.0984  f1=0.685  time=123.15s\n",
      "model_saved at f1: 0.6850424055512722 from 0.6843367411334091\n",
      "Epoch 7/15  loss=0.0808  val_loss=0.1031  f1=0.683  time=123.49s\n",
      "Epoch 8/15  loss=0.0765  val_loss=0.1050  f1=0.675  time=122.51s\n",
      "Epoch 9/15  loss=0.0722  val_loss=0.1070  f1=0.676  time=122.35s\n",
      "Epoch 10/15  loss=0.0678  val_loss=0.1113  f1=0.675  time=121.69s\n",
      "Epoch 11/15  loss=0.0640  val_loss=0.1156  f1=0.676  time=122.12s\n",
      "Epoch 12/15  loss=0.0602  val_loss=0.1230  f1=0.667  time=121.33s\n",
      "Epoch 13/15  loss=0.0569  val_loss=0.1297  f1=0.664  time=123.06s\n",
      "Epoch 14/15  loss=0.0535  val_loss=0.1339  f1=0.665  time=123.44s\n",
      "Epoch 15/15  loss=0.0507  val_loss=0.1437  f1=0.657  time=122.23s\n",
      "Fold 5\n",
      "Epoch 1/15  loss=0.1506  val_loss=0.1117  f1=0.643  time=122.02s\n",
      "model_saved at f1: 0.6433984073978937 from 0.0\n",
      "Epoch 2/15  loss=0.1059  val_loss=0.1022  f1=0.673  time=123.36s\n",
      "model_saved at f1: 0.6727936547903816 from 0.6433984073978937\n",
      "Epoch 3/15  loss=0.0995  val_loss=0.0991  f1=0.682  time=120.36s\n",
      "model_saved at f1: 0.6822119875036232 from 0.6727936547903816\n",
      "Epoch 4/15  loss=0.0945  val_loss=0.0985  f1=0.684  time=122.70s\n",
      "model_saved at f1: 0.6841282501196364 from 0.6822119875036232\n",
      "Epoch 5/15  loss=0.0901  val_loss=0.0983  f1=0.686  time=122.26s\n",
      "model_saved at f1: 0.6857198405754837 from 0.6841282501196364\n",
      "Epoch 6/15  loss=0.0859  val_loss=0.0987  f1=0.687  time=121.85s\n",
      "model_saved at f1: 0.6870113014023869 from 0.6857198405754837\n",
      "Epoch 7/15  loss=0.0815  val_loss=0.1004  f1=0.683  time=121.42s\n",
      "Epoch 8/15  loss=0.0773  val_loss=0.1030  f1=0.678  time=122.79s\n",
      "Epoch 9/15  loss=0.0727  val_loss=0.1048  f1=0.676  time=122.39s\n",
      "Epoch 10/15  loss=0.0685  val_loss=0.1125  f1=0.671  time=120.93s\n",
      "Epoch 11/15  loss=0.0645  val_loss=0.1159  f1=0.666  time=121.88s\n",
      "Epoch 12/15  loss=0.0607  val_loss=0.1225  f1=0.666  time=123.09s\n",
      "Epoch 13/15  loss=0.0569  val_loss=0.1266  f1=0.665  time=121.49s\n",
      "Epoch 14/15  loss=0.0537  val_loss=0.1294  f1=0.660  time=121.92s\n",
      "Epoch 15/15  loss=0.0505  val_loss=0.1405  f1=0.659  time=122.49s\n",
      "{'threshold': 0.3103404641151428, 'f1': 0.6582718307099471}\n",
      "f1 score: 0.6939148304600982\n"
     ]
    }
   ],
   "source": [
    "train_preds, test_preds = training(train_x, train_y, test_x, c, embeddings)\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "print(search_result)\n",
    "test_pred_targets = test_preds > search_result['threshold']\n",
    "f1 = f1_score(test_df['target'], test_pred_targets)\n",
    "print('f1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
